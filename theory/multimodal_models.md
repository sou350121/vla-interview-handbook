# å¤šæ¨¡æ€æ¨¡å‹åŸºç¡€ (Multimodal Models)

> **æ ¸å¿ƒæ¦‚å¿µ**: å¤šæ¨¡æ€æ¨¡å‹ (Multimodal Models) æ˜¯æŒ‡èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§æ•°æ®æ¨¡æ€ï¼ˆå¦‚è§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘ã€è§¦è§‰ç­‰ï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚åœ¨ VLA é¢†åŸŸï¼Œå¤šæ¨¡æ€èƒ½åŠ›æ˜¯è¿æ¥"çœ‹"ã€"è¯´"ã€"åš"çš„å…³é”®ã€‚

## 1. ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€? (Why Multimodal?)

### 1.1 æœºå™¨äººçš„æ„ŸçŸ¥éœ€æ±‚

æœºå™¨äººåœ¨çœŸå®ä¸–ç•Œä¸­éœ€è¦åŒæ—¶å¤„ç†å¤šç§ä¿¡æ¯ï¼š

| æ¨¡æ€ | æ¥æº | ä½œç”¨ |
| :--- | :--- | :--- |
| **è§†è§‰ (Vision)** | RGB ç›¸æœºã€æ·±åº¦ç›¸æœº | ç†è§£åœºæ™¯ã€è¯†åˆ«ç‰©ä½“ |
| **è¯­è¨€ (Language)** | è¯­éŸ³æŒ‡ä»¤ã€æ–‡æœ¬ | ç†è§£ä»»åŠ¡æ„å›¾ |
| **æœ¬ä½“æ„ŸçŸ¥ (Proprioception)** | å…³èŠ‚ç¼–ç å™¨ã€IMU | æ„ŸçŸ¥è‡ªèº«çŠ¶æ€ |
| **è§¦è§‰ (Tactile)** | è§¦è§‰ä¼ æ„Ÿå™¨ | æ„ŸçŸ¥æ¥è§¦åŠ›ã€çº¹ç† |
| **éŸ³é¢‘ (Audio)** | éº¦å…‹é£ | ç¯å¢ƒå£°éŸ³ã€è¯­éŸ³äº¤äº’ |

### 1.2 å•æ¨¡æ€çš„å±€é™æ€§

- **ä»…è§†è§‰**: æ— æ³•ç†è§£æŠ½è±¡æŒ‡ä»¤ï¼ˆ"æŠŠé‚£ä¸ªå±é™©çš„ä¸œè¥¿æ‹¿èµ°"ï¼‰
- **ä»…è¯­è¨€**: æ— æ³•å®šä½å…·ä½“ç‰©ä½“ï¼ˆ"æ¡Œä¸Šçš„çº¢è‰²æ¯å­"åœ¨å“ªï¼Ÿï¼‰
- **ç¼ºä¹æœ¬ä½“æ„ŸçŸ¥**: ä¸çŸ¥é“æœºæ¢°è‡‚å½“å‰å§¿æ€ï¼Œæ— æ³•é—­ç¯æ§åˆ¶

### 1.3 å¤šæ¨¡æ€çš„ä¼˜åŠ¿


$$
\text{å¤šæ¨¡æ€ç†è§£} > \sum \text{å•æ¨¡æ€ç†è§£}
$$


- **è¯­ä¹‰æ¥åœ° (Grounding)**: å°†è¯­è¨€æ¦‚å¿µä¸è§†è§‰å®ä½“ç»‘å®š
- **è·¨æ¨¡æ€æ¨ç†**: "çº¢è‰²çš„ä¸œè¥¿"ï¼ˆè¯­è¨€ï¼‰â†’ é”å®šçº¢è‰²ç‰©ä½“ï¼ˆè§†è§‰ï¼‰â†’ æŠ“å–åŠ¨ä½œ
- **é²æ£’æ€§**: ä¸€ä¸ªæ¨¡æ€å¤±æ•ˆæ—¶ï¼Œå…¶ä»–æ¨¡æ€å¯ä»¥è¡¥å¿

## 2. å¤šæ¨¡æ€æ¶æ„æ¼”è¿› (Architecture Evolution)

### 2.1 æ—©æœŸï¼šåŒå¡”æ¨¡å‹ (Dual-Encoder)

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
å›¾åƒ â”€â”€â”€â”€â–¶â”‚  Image      â”‚      â”‚   Text      â”‚â—€â”€â”€â”€â”€ æ–‡æœ¬
          â”‚  Encoder    â”‚      â”‚   Encoder   â”‚
          â”‚  (ResNet)   â”‚      â”‚   (BERT)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
                 â–¼                    â–¼
              img_emb              text_emb
                 â”‚                    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    Cosine Similarity
```

**ä»£è¡¨**: CLIP, ALIGN
**ç‰¹ç‚¹**: å›¾åƒå’Œæ–‡æœ¬ç‹¬ç«‹ç¼–ç ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹é½åˆ°åŒä¸€ç©ºé—´
**å±€é™**: æ— æ³•è¿›è¡Œæ·±åº¦çš„è·¨æ¨¡æ€äº¤äº’

### 2.2 ä¸­æœŸï¼šèåˆç¼–ç å™¨ (Fusion Encoder)

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
å›¾åƒ â”€â”€â”€â”€â–¶â”‚  Image      â”‚      â”‚   Text      â”‚â—€â”€â”€â”€â”€ æ–‡æœ¬
          â”‚  Encoder    â”‚      â”‚   Encoder   â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Fusion Module  â”‚
                 â”‚  (Cross-Attn)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                   Fused Features
```

**ä»£è¡¨**: ViLBERT, LXMERT, UNITER
**ç‰¹ç‚¹**: é€šè¿‡ Cross-Attention å®ç°æ·±åº¦äº¤äº’
**æ”¹è¿›**: æ”¯æŒæ›´å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†

### 2.3 ç°ä»£ï¼šç»Ÿä¸€è§£ç å™¨ (Unified Decoder)

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
å›¾åƒ â”€â”€â”€â”€â–¶â”‚  Vision     â”‚â”€â”€â”
          â”‚  Encoder    â”‚  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                           â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”œâ”€â”€â–¶â”‚     LLM Decoder     â”‚â”€â”€â–¶ è¾“å‡º
                           â”‚   â”‚  (Unified Token)    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ–‡æœ¬ â”€â”€â”€â”€â–¶â”‚  Tokenizer  â”‚â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£è¡¨**: Flamingo, LLaVA, GPT-4V, Gemini
**ç‰¹ç‚¹**: å°†è§†è§‰ç‰¹å¾ä½œä¸º"è™šæ‹Ÿ Token"è¾“å…¥åˆ° LLM
**ä¼˜åŠ¿**: åˆ©ç”¨ LLM çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒä»»æ„è¾“å…¥è¾“å‡ºç»„åˆ

## 3. VLA ä¸­çš„å¤šæ¨¡æ€èåˆç­–ç•¥ (Fusion Strategies in VLA)

### 3.1 æ—©æœŸèåˆ (Early Fusion)

åœ¨ç‰¹å¾æå–é˜¶æ®µå°±è¿›è¡Œèåˆã€‚

```python
class EarlyFusion(nn.Module):
    def __init__(self):
        self.vision_proj = nn.Linear(vision_dim, hidden_dim)
        self.language_proj = nn.Linear(language_dim, hidden_dim)
        self.proprio_proj = nn.Linear(proprio_dim, hidden_dim)
        
    def forward(self, image_feat, text_feat, proprio):
        # ç›´æ¥æ‹¼æ¥
        fused = torch.cat([
            self.vision_proj(image_feat),
            self.language_proj(text_feat),
            self.proprio_proj(proprio)
        ], dim=1)  # [B, L_v + L_t + 1, D]
        return fused
```

**ä¼˜ç‚¹**: ç®€å•é«˜æ•ˆ
**ç¼ºç‚¹**: ä¸åŒæ¨¡æ€çš„ç‰¹å¾å°ºåº¦å¯èƒ½ä¸åŒ¹é…

### 3.2 ä¸­æœŸèåˆ (Mid Fusion / Cross-Attention)

é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€èåˆã€‚

```python
class CrossModalAttention(nn.Module):
    def __init__(self, hidden_dim, num_heads=8):
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        
    def forward(self, query_feat, context_feat):
        """
        query_feat: éœ€è¦è¢«å¢å¼ºçš„ç‰¹å¾ (e.g., åŠ¨ä½œ query)
        context_feat: æä¾›ä¸Šä¸‹æ–‡çš„ç‰¹å¾ (e.g., å›¾åƒ + è¯­è¨€)
        """
        # Query attends to Context
        attended, attn_weights = self.cross_attn(
            query=query_feat,
            key=context_feat,
            value=context_feat
        )
        return attended, attn_weights
```

**ä»£è¡¨**: RT-1 (TokenLearner)ï¼ŒOcto
**ä¼˜ç‚¹**: åŠ¨æ€å­¦ä¹ æ¨¡æ€é—´å…³ç³»
**ç¼ºç‚¹**: è®¡ç®—å¼€é”€å¤§

### 3.3 æ™šæœŸèåˆ (Late Fusion)

å„æ¨¡æ€ç‹¬ç«‹å¤„ç†åå†åˆå¹¶å†³ç­–ã€‚

```python
class LateFusion(nn.Module):
    def __init__(self):
        self.vision_policy = VisionPolicy()
        self.language_policy = LanguagePolicy()
        self.fusion_head = nn.Linear(hidden_dim * 2, action_dim)
        
    def forward(self, image, text):
        vision_out = self.vision_policy(image)
        language_out = self.language_policy(text)
        
        # å†³ç­–å±‚èåˆ
        fused = torch.cat([vision_out, language_out], dim=-1)
        action = self.fusion_head(fused)
        return action
```

**ä¼˜ç‚¹**: å„æ¨¡æ€å¯ä»¥ç‹¬ç«‹ä¼˜åŒ–
**ç¼ºç‚¹**: æ— æ³•å­¦ä¹ å¤æ‚çš„è·¨æ¨¡æ€äº¤äº’

### 3.4 VLA ä¸­çš„ä¸»æµæ–¹æ¡ˆï¼šFiLM è°ƒåˆ¶

**FiLM (Feature-wise Linear Modulation)** æ˜¯ VLA ä¸­æœ€å¸¸ç”¨çš„æ¡ä»¶æ³¨å…¥æ–¹å¼ã€‚

```python
class FiLM(nn.Module):
    """Feature-wise Linear Modulation"""
    def __init__(self, cond_dim, feature_dim):
        self.gamma = nn.Linear(cond_dim, feature_dim)  # Scale
        self.beta = nn.Linear(cond_dim, feature_dim)   # Shift
        
    def forward(self, feature, condition):
        """
        feature: è¦è°ƒåˆ¶çš„ç‰¹å¾ [B, L, D]
        condition: æ¡ä»¶ä¿¡æ¯ [B, C]
        """
        gamma = self.gamma(condition).unsqueeze(1)  # [B, 1, D]
        beta = self.beta(condition).unsqueeze(1)
        
        # è°ƒåˆ¶: Î³ * feature + Î²
        return gamma * feature + beta
```

**åº”ç”¨åœºæ™¯**:
- **RT-1**: è¯­è¨€ç‰¹å¾é€šè¿‡ FiLM è°ƒåˆ¶è§†è§‰ç‰¹å¾
- **Diffusion Policy**: æ—¶é—´æ­¥ $t$ é€šè¿‡ FiLM æ³¨å…¥åˆ° U-Net

## 4. æ ¸å¿ƒè§†è§‰ç¼–ç å™¨ (Vision Encoders)

### 4.1 ViT (Vision Transformer)

```
å›¾åƒ [H, W, 3] 
    â”‚
    â–¼ Patch Embedding (16x16)
[N_patches, D] where N = (H/16) * (W/16)
    â”‚
    â–¼ + Position Embedding
    â”‚
    â–¼ Transformer Encoder (L layers)
    â”‚
    â–¼
[CLS] token æˆ– å…¨å±€å¹³å‡æ± åŒ–
```

**ç‰¹ç‚¹**:
- å°†å›¾åƒåˆ‡åˆ†ä¸º Patch (å¦‚ 16x16)
- æ¯ä¸ª Patch ä½œä¸ºä¸€ä¸ª Token
- é€šè¿‡ Self-Attention å»ºæ¨¡å…¨å±€å…³ç³»

### 4.2 SigLIP (Sigmoid Loss for Language-Image Pre-training)

**æ”¹è¿› CLIP**:
- ä½¿ç”¨ Sigmoid æ›¿ä»£ Softmax (æ›´å¥½çš„æ‰¹é‡å¯¹æ¯”å­¦ä¹ )
- æ”¯æŒæ›´å¤§çš„ batch size
- VLA é¦–é€‰çš„è§†è§‰ç¼–ç å™¨ (OpenVLA, RDT)

### 4.3 DINOv2 (Self-supervised Vision Transformer)

**ç‰¹ç‚¹**:
- è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæ— éœ€æ ‡ç­¾
- å¼ºå¤§çš„ä½å±‚è§†è§‰ç‰¹å¾ (è¾¹ç¼˜ã€çº¹ç†)
- é€‚åˆéœ€è¦ç²¾ç¡®ç©ºé—´ä¿¡æ¯çš„ä»»åŠ¡

### 4.4 å¯¹æ¯”ä¸é€‰æ‹©

| ç¼–ç å™¨ | é¢„è®­ç»ƒæ–¹å¼ | ç‰¹ç‚¹ | VLA åº”ç”¨ |
| :--- | :--- | :--- | :--- |
| **ResNet** | ç›‘ç£å­¦ä¹  | é«˜æ•ˆï¼Œé€‚åˆ CNN ç­–ç•¥ | RT-1, Diffusion Policy |
| **ViT** | ç›‘ç£/è‡ªç›‘ç£ | å…¨å±€å»ºæ¨¡å¼º | é€šç”¨ |
| **CLIP/SigLIP** | å¯¹æ¯”å­¦ä¹  | è¯­ä¹‰å¯¹é½å¥½ | OpenVLA, RDT |
| **DINOv2** | è‡ªç›‘ç£ | ç©ºé—´ç‰¹å¾å¼º | ç²¾ç»†æ“ä½œ |

## 5. è¯­è¨€ç¼–ç å™¨ (Language Encoders)

### 5.1 BERT-style (Encoder-only)

```python
from transformers import BertModel

text = "pick up the red cup"
inputs = tokenizer(text, return_tensors="pt")
outputs = bert_model(**inputs)

# ä½¿ç”¨ [CLS] token æˆ–å¹³å‡æ± åŒ–
text_embedding = outputs.last_hidden_state[:, 0, :]  # [B, D]
```

**é€‚ç”¨**: ç†è§£å‹ä»»åŠ¡ï¼ŒæŒ‡ä»¤åµŒå…¥

### 5.2 T5-style (Encoder-Decoder)

**é€‚ç”¨**: éœ€è¦ç”Ÿæˆæ–‡æœ¬çš„ä»»åŠ¡ (å¦‚ CoT æ¨ç†)

### 5.3 LLM-style (Decoder-only)

**ä»£è¡¨**: Llama, Gemma, Qwen
**é€‚ç”¨**: ç°ä»£ VLA çš„æ ‡å‡†é€‰æ‹©ï¼Œåˆ©ç”¨å¼ºå¤§çš„ In-context Learning

---

## 5.5 PaliGemma è¯¦è§£ (VLA å¸¸ç”¨ Backbone)

> **è®ºæ–‡**: [PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/abs/2407.07726) (Google, 2024)
> **å®˜æ–¹**: [HuggingFace](https://huggingface.co/google/paligemma-3b-pt-224)

PaliGemma æ˜¯ Google æ¨å‡ºçš„è½»é‡çº§ VLMï¼Œå·²æˆä¸º **Ï€0ã€OpenVLA** ç­‰ VLA çš„é¦–é€‰ backboneã€‚

### ä¸ºä»€ä¹ˆ VLA å¸¸ç”¨ PaliGemma?

| ä¼˜åŠ¿ | è¯´æ˜ |
| :--- | :--- |
| **è½»é‡é«˜æ•ˆ** | 3B å‚æ•°ï¼Œå¯åœ¨å•å¡ (24GB) å¾®è°ƒ |
| **é¢„è®­ç»ƒå……åˆ†** | åœ¨å¤§é‡å›¾æ–‡æ•°æ®ä¸Šè®­ç»ƒï¼Œè§†è§‰ç†è§£å¼º |
| **å¼€æºå‹å¥½** | Apache 2.0 è®¸å¯ï¼Œå¯å•†ç”¨ |
| **æ¨¡å—åŒ–è®¾è®¡** | Vision Encoder å’Œ LLM è§£è€¦ï¼Œæ˜“äºé€‚é… |
| **å¤šåˆ†è¾¨ç‡** | æ”¯æŒ 224/448/896 è¾“å…¥å°ºå¯¸ |

### PaliGemma æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PaliGemma 3B                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Image Input                    Text Input                  â”‚
â”‚   [224Ã—224Ã—3]                    "Pick up the cup"           â”‚
â”‚        â”‚                              â”‚                      â”‚
â”‚        â–¼                              â–¼                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚   SigLIP     â”‚              â”‚   Gemma      â”‚            â”‚
â”‚   â”‚  ViT-So400m  â”‚              â”‚  Tokenizer   â”‚            â”‚
â”‚   â”‚  (400M)      â”‚              â”‚              â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚          â”‚                             â”‚                     â”‚
â”‚   [256 patches]                  [L tokens]                  â”‚
â”‚   [256, 1152]                    [L, 2048]                   â”‚
â”‚          â”‚                             â”‚                     â”‚
â”‚          â–¼                             â”‚                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚                     â”‚
â”‚   â”‚  Linear Proj â”‚ (1152 â†’ 2048)       â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚                     â”‚
â”‚          â”‚                             â”‚                     â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                     â–¼                                        â”‚
â”‚            [Vision] + [Text Tokens]                          â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚                 Gemma 2B LLM                            â”‚â”‚
â”‚   â”‚          (18 Transformer Layers)                        â”‚â”‚
â”‚   â”‚                                                         â”‚â”‚
â”‚   â”‚    Self-Attention (Vision + Text ä¸€èµ·å¤„ç†)               â”‚â”‚
â”‚   â”‚                     â†“                                   â”‚â”‚
â”‚   â”‚              Hidden States                              â”‚â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚              [B, L, 2048]                                    â”‚
â”‚           (é€ç»™ Action Head)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

#### 1. SigLIP Vision Encoder

```python
# SigLIP vs CLIP
# SigLIP ä½¿ç”¨ Sigmoid Loss è€Œé Softmaxï¼Œæ›´é€‚åˆç»†ç²’åº¦ç†è§£

# é…ç½®
vision_config = {
    "model": "ViT-So400m",      # 400M å‚æ•°
    "image_size": 224,          # æˆ– 448, 896
    "patch_size": 14,           # 16Ã—16 patches
    "hidden_size": 1152,
    "num_layers": 27,
    "num_heads": 16
}

# è¾“å‡º: [B, 256, 1152] (256 = (224/14)Â² patches)
```

#### 2. Gemma 2B LLM

```python
# Gemma æ˜¯ Google çš„è½»é‡çº§ LLM
llm_config = {
    "hidden_size": 2048,
    "num_layers": 18,
    "num_heads": 8,
    "vocab_size": 256000,
    "max_position": 8192,
    "intermediate_size": 16384  # FFN
}
```

#### 3. æŠ•å½±å±‚ (Linear Projection)

```python
# å°† SigLIP ç‰¹å¾æŠ•å°„åˆ° Gemma ç©ºé—´
self.vision_proj = nn.Linear(1152, 2048)

# æŠ•å°„åï¼Œè§†è§‰ Token å’Œæ–‡æœ¬ Token åœ¨åŒä¸€ç©ºé—´
vision_tokens = self.vision_proj(siglip_output)  # [B, 256, 2048]
```

### VLA ä¸­çš„ä½¿ç”¨æ–¹å¼

```python
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration

# åŠ è½½æ¨¡å‹
model = PaliGemmaForConditionalGeneration.from_pretrained(
    "google/paligemma-3b-pt-224",
    torch_dtype=torch.bfloat16
)
processor = AutoProcessor.from_pretrained("google/paligemma-3b-pt-224")

# æ–¹å¼ 1: è·å– Hidden States (ç”¨äº Action Head)
def get_vlm_features(images, text):
    inputs = processor(images=images, text=text, return_tensors="pt")
    outputs = model(
        **inputs,
        output_hidden_states=True
    )
    # æœ€åä¸€å±‚ hidden states
    hidden = outputs.hidden_states[-1]  # [B, L, 2048]
    return hidden

# æ–¹å¼ 2: ç›´æ¥ç”Ÿæˆæ–‡æœ¬ (ç”¨äº CoT)
def generate_text(images, text):
    inputs = processor(images=images, text=text, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100)
    return processor.decode(outputs[0])
```

### PaliGemma ç‰ˆæœ¬å¯¹æ¯”

| ç‰ˆæœ¬ | å‚æ•°é‡ | è¾“å…¥åˆ†è¾¨ç‡ | é€‚ç”¨åœºæ™¯ |
| :--- | :--- | :--- | :--- |
| **paligemma-3b-pt-224** | 3B | 224Ã—224 | VLA é¦–é€‰ï¼Œå¹³è¡¡æ•ˆç‡ |
| paligemma-3b-pt-448 | 3B | 448Ã—448 | éœ€è¦æ›´å¤šç»†èŠ‚ |
| paligemma-3b-pt-896 | 3B | 896Ã—896 | é«˜åˆ†è¾¨ç‡ä»»åŠ¡ |
| paligemma-3b-mix-224 | 3B | 224Ã—224 | æ··åˆä»»åŠ¡å¾®è°ƒç‰ˆ |

### PaliGemma vs å…¶ä»– VLM

| æ¨¡å‹ | å‚æ•°é‡ | å¼€æº | VLA é€‚ç”¨æ€§ |
| :--- | :--- | :--- | :--- |
| **PaliGemma** | **3B** | âœ… Apache 2.0 | â­â­â­â­â­ æœ€å¸¸ç”¨ |
| LLaVA 1.5 | 7B/13B | âœ… | â­â­â­â­ è¾ƒå¤§ä½†æˆç†Ÿ |
| Qwen-VL | 7B | âœ… | â­â­â­â­ ä¸­æ–‡æ”¯æŒå¥½ |
| GPT-4V | ~1T | âŒ | â­â­ API å»¶è¿Ÿé«˜ |
| PaLI-X | 55B | âŒ | â­ å¤ªå¤§æ— æ³•éƒ¨ç½² |

### é¢è¯•å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆ Ï€0 é€‰æ‹© PaliGemma è€Œä¸æ˜¯æ›´å¤§çš„ LLaVA?**

A: ä¸‰ä¸ªåŸå› :
1. **æ•ˆç‡**: 3B å‚æ•°å¯åœ¨å•å¡è®­ç»ƒ/æ¨ç†ï¼Œæ»¡è¶³æœºå™¨äººå®æ—¶æ€§è¦æ±‚
2. **SigLIP**: æ¯” CLIP æ›´å¥½çš„ç»†ç²’åº¦è§†è§‰ç†è§£
3. **æ¨¡å—åŒ–**: Vision/Language è§£è€¦ï¼Œæ–¹ä¾¿æ¥ Action Head

---

**Q: PaliGemma çš„ 256 ä¸ª vision tokens å¤Ÿç”¨å—?**

A: å¯¹äºå¤§å¤šæ•°æœºå™¨äººä»»åŠ¡è¶³å¤Ÿ:
- æ¡Œé¢æ“ä½œ: 224Ã—224 åˆ†è¾¨ç‡ + 256 tokens èƒ½è¦†ç›–å…³é”®ç‰©ä½“
- éœ€è¦ç²¾ç»†æ“ä½œæ—¶: å¯ç”¨ 448/896 ç‰ˆæœ¬ (1024/4096 tokens)
- Trade-off: æ›´å¤š tokens = æ›´æ…¢æ¨ç†

---

## 5.6 ä¸»æµ VLM å¯¹æ¯”è¡¨ï¼ˆVLA è®­ç»ƒå‚è€ƒï¼‰

> **ç›®æ ‡**: ä¸º VLA å¼€å‘è€…æä¾›å½“å‰å¸‚åœºä¸Šä¸»æµ Vision Language Model çš„å¯¹æ¯”ï¼Œé‡ç‚¹å…³æ³¨**å·²åœ¨ VLA é¡¹ç›®ä¸­å®é™…ä½¿ç”¨**çš„æ¨¡å‹ã€‚
> 
> **æœ€åæ›´æ–°**: 2025å¹´12æœˆ5æ—¥

---

### 5.6.1 âœ… å·²åœ¨ VLA ä¸­å®é™…ä½¿ç”¨ï¼ˆä¼˜å…ˆæ¨èï¼‰

| æ¨¡å‹ | æœºæ„ | å‘å¸ƒæ—¶é—´ | Vision Encoder | LLM Backbone | å‚æ•°é‡ | è¾“å…¥åˆ†è¾¨ç‡ | å¼€æº | è®¸å¯è¯ | VLA åº”ç”¨æ¡ˆä¾‹ | HuggingFace |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **PaliGemma 3B** | Google | 2024.07 | SigLIP ViT-So400m | Gemma 2B | 3B | 224/448/896 | âœ… | Apache 2.0 | **Ï€0 (Pi-Zero)**, OpenVLA å˜ä½“ | [google/paligemma-3b-pt-224](https://huggingface.co/google/paligemma-3b-pt-224) |
| **SigLIP** | Google | 2023.09 | ViT (Sigmoid Loss) | - | 400M-2.6B | 224-384 | âœ… | Apache 2.0 | **OpenVLA**, **RDT** (Vision Encoder) | [google/siglip-*](https://huggingface.co/models?search=siglip) |
| **LLaVA 1.5/1.6** | - | 2023.10/2024.01 | CLIP/ViT | Llama 2/Vicuna | 7B/13B | 336/672 | âœ… | Apache 2.0 | **OpenVLA** (Llama 2 + SigLIP ç»„åˆ) | [llava-hf/llava-1.5-*](https://huggingface.co/models?search=llava) |
| **LLaVA-NeXT** | - | 2024.12 | CLIP/ViT | Llama 3/Vicuna | 7B/13B/34B | 672/1344 | âœ… | Apache 2.0 | æœ€æ–°ç‰ˆæœ¬ï¼Œæ€§èƒ½æå‡ | [llava-hf/llava-next-*](https://huggingface.co/models?search=llava-next) |
| **PaLI-X** | Google | 2023.12 | ViT-22B | PaLM-E | 55B | 224-1024 | âŒ | - | **RT-2** | - |

**é€‰æ‹©å»ºè®®**:
- **PaliGemma 3B**: VLA è®­ç»ƒé¦–é€‰ï¼Œè½»é‡é«˜æ•ˆï¼ˆå•å¡ 24GB å¯è®­ç»ƒï¼‰ï¼Œé¢„è®­ç»ƒå……åˆ†ï¼Œæ¨¡å—åŒ–è®¾è®¡
- **SigLIP**: VLA é¦–é€‰è§†è§‰ç¼–ç å™¨ï¼Œæ¯” CLIP æ›´å¼ºçš„ç»†ç²’åº¦ç†è§£ï¼Œæ”¯æŒå¤§ batch è®­ç»ƒ
- **LLaVA**: æˆç†Ÿç¨³å®šï¼Œç¤¾åŒºæ”¯æŒå¥½ï¼Œé€‚åˆéœ€è¦æ›´å¤§æ¨¡å‹çš„åœºæ™¯

### 5.6.2 ğŸ”„ é€‚åˆ VLA è®­ç»ƒçš„å¼€æº VLMï¼ˆæ¨èå°è¯•ï¼‰

#### ğŸ†• 2025å¹´æœ€æ–°å‘å¸ƒ

| æ¨¡å‹ | æœºæ„ | å‘å¸ƒæ—¶é—´ | Vision Encoder | LLM Backbone | å‚æ•°é‡ | è¾“å…¥åˆ†è¾¨ç‡ | å¼€æº | è®¸å¯è¯ | ä¼˜åŠ¿ | HuggingFace |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Qwen2.5-VL** | é˜¿é‡Œå·´å·´ | 2025.03 | Window Attn ViT + MRoPE | Qwen2.5 LLM | 3B/7B/32B/72B | ä»»æ„åˆ†è¾¨ç‡ | âœ… | Apache 2.0 | **2025 SOTA**ï¼Œæ•°å­¦æ¨ç†å¼ºï¼Œé•¿è§†é¢‘æ”¯æŒ | [Qwen/Qwen2.5-VL-*](https://huggingface.co/models?search=Qwen2.5-VL) |
| **Eagle 2.5** | NVIDIA | 2025.04 | é•¿ä¸Šä¸‹æ–‡ ViT | - | 8B | é•¿è§†é¢‘ | âœ… | Apache 2.0 | é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ï¼ŒVideo-MME 72.4% | [nvidia/Eagle-*](https://huggingface.co/models?search=Eagle) |
| **Seed 1.5-VL** | å­—èŠ‚è·³åŠ¨ | 2025.05 | - | - | 20B (æ¿€æ´») | - | âœ… | - | åª²ç¾ Gemini 2.5 Proï¼ŒGUI äº¤äº’å¼º | [ByteDance/Seed-*](https://huggingface.co/models?search=Seed) |
| **PLM** | Meta | 2025.05 | - | - | - | - | âœ… | MIT | å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¤æ‚è§†è§‰ä»»åŠ¡ | [meta-llama/PLM](https://github.com/facebookresearch/PLM) |
| **GLM-4.5V** | æ™ºè°±AI | 2025 | 3D-RoPE ViT | GLM-4.5-Air | 106B (12B æ¿€æ´») | - | âœ… | Apache 2.0 | MoE æ¶æ„ï¼Œ3D ç©ºé—´æ¨ç† | [THUDM/GLM-4.5V](https://huggingface.co/models?search=GLM-4) |
| **Llama 4 Scout/Maverick** | Meta | 2025.04 | ViT Patch | MoE Transformer | 16-128 ä¸“å®¶ | - | âœ… | Meta Llama | 10M token ä¸Šä¸‹æ–‡ï¼Œå¤šæ¨¡æ€ | [meta-llama/Llama-4](https://huggingface.co/models?search=llama-4) |

#### 2024å¹´å‘å¸ƒï¼ˆä»æ¨èï¼‰

| æ¨¡å‹ | æœºæ„ | å‘å¸ƒæ—¶é—´ | Vision Encoder | LLM Backbone | å‚æ•°é‡ | è¾“å…¥åˆ†è¾¨ç‡ | å¼€æº | è®¸å¯è¯ | ä¼˜åŠ¿ | HuggingFace |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Qwen2-VL** | é˜¿é‡Œå·´å·´ | 2024.08 | InternViT | Qwen2 LLM | 2B/7B/72B | 448-1344 | âœ… | Apache 2.0 | æ€§èƒ½å¤§å¹…æå‡ | [Qwen/Qwen2-VL-*](https://huggingface.co/models?search=Qwen2-VL) |
| **InternVL2** | å•†æ±¤ | 2024.07 | InternViT-6B | InternLM2 | 2B/4B/8B/26B | 448-1344 | âœ… | Apache 2.0 | å¤šæ¨¡æ€èƒ½åŠ›å¢å¼º | [OpenGVLab/InternVL2-*](https://huggingface.co/models?search=InternVL2) |
| **MiniCPM-V 2.6** | é¢å£æ™ºèƒ½ | 2024.08 | ViT | MiniCPM | 8B | 336-1344 | âœ… | Apache 2.0 | è¶…è½»é‡çº§ï¼Œè¾¹ç¼˜éƒ¨ç½² | [openbmb/MiniCPM-V-*](https://huggingface.co/models?search=MiniCPM-V) |
| **LLaVA-NeXT** | - | 2024.06 | CLIP/ViT | Llama 3/Vicuna | 7B/13B/34B | 672/1344 | âœ… | Apache 2.0 | æœ€æ–° LLaVA ç‰ˆæœ¬ | [llava-hf/llava-next-*](https://huggingface.co/models?search=llava-next) |
| **SmolVLA** | Hugging Face | 2024.12 | ViT-Small | TinyLlama | 450M | 224 | âœ… | Apache 2.0 | è¶…è½»é‡çº§ï¼ŒVLA ç ”ç©¶å…¥é—¨ | [huggingface/smolvla](https://huggingface.co/models?search=smolvla) |

#### ç»å…¸æ¨¡å‹ï¼ˆä»å¯ç”¨ï¼‰

| æ¨¡å‹ | æœºæ„ | å‘å¸ƒæ—¶é—´ | Vision Encoder | LLM Backbone | å‚æ•°é‡ | è¾“å…¥åˆ†è¾¨ç‡ | å¼€æº | è®¸å¯è¯ | ä¼˜åŠ¿ | HuggingFace |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Qwen-VL** | é˜¿é‡Œå·´å·´ | 2023.11 | CLIP-ViT | Qwen LLM | 7B/72B | 448-1024 | âœ… | Apache 2.0 | ä¸­æ–‡æ”¯æŒå¥½ | [Qwen/Qwen-VL](https://huggingface.co/Qwen/Qwen-VL) |
| **CogVLM** | æ™ºè°±AI | 2023.10 | EVA2-ViT | GLM | 17B | 490 | âœ… | Apache 2.0 | è§†è§‰ç†è§£å¼ºï¼Œä¸­æ–‡æ”¯æŒ | [THUDM/cogvlm-*](https://huggingface.co/models?search=cogvlm) |
| **InternVL** | å•†æ±¤ | 2024.01 | InternViT | InternLM | 2B-26B | 448-1024 | âœ… | Apache 2.0 | å¤šåˆ†è¾¨ç‡æ”¯æŒ | [OpenGVLab/InternVL-*](https://huggingface.co/models?search=InternVL) |

**é€‚ç”¨åœºæ™¯**:
- **Qwen2.5-VL** (ğŸ†• 2025): ä¸­æ–‡æŒ‡ä»¤ VLA é¦–é€‰ï¼Œæ•°å­¦æ¨ç†å¼ºï¼Œæ”¯æŒä»»æ„åˆ†è¾¨ç‡å’Œé•¿è§†é¢‘
- **Eagle 2.5** (ğŸ†• 2025): é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»»åŠ¡ï¼Œè§†é¢‘ç†è§£
- **Seed 1.5-VL** (ğŸ†• 2025): GUI äº¤äº’ã€å¤æ‚è§†è§‰æ¨ç†
- **GLM-4.5V** (ğŸ†• 2025): 3D ç©ºé—´æ¨ç†ä»»åŠ¡
- **Llama 4** (ğŸ†• 2025): è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ10M tokenï¼‰ï¼Œæ–‡æ¡£åˆ†æ
- **Qwen2-VL**: ä¸­æ–‡æ”¯æŒå¥½ï¼ˆ2024 ç‰ˆæœ¬ï¼‰
- **MiniCPM-V**: è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼Œèµ„æºå—é™åœºæ™¯
- **SmolVLA**: è¶…è½»é‡çº§ç ”ç©¶ï¼Œå¿«é€ŸåŸå‹éªŒè¯

### 5.6.3 âŒ é—­æº APIï¼ˆå‚è€ƒï¼Œä¸é€‚åˆç›´æ¥è®­ç»ƒï¼‰

| æ¨¡å‹ | æœºæ„ | å‘å¸ƒæ—¶é—´ | å‚æ•°é‡ | ç‰¹ç‚¹ | VLA é€‚ç”¨æ€§ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Gemini 2.5 Pro** ğŸ†• | Google | 2025.03 | æœªå…¬å¼€ | **2025 SOTA**ï¼Œ1M token ä¸Šä¸‹æ–‡ï¼Œå†…ç½®æ€è€ƒåŠŸèƒ½ | â­â­ API è°ƒç”¨ï¼Œæˆæœ¬é«˜ |
| **Claude 3.7 Vision** ğŸ†• | Anthropic | 2025.02 | æœªå…¬å¼€ | é«˜ç²¾åº¦ OCRï¼Œå›¾è¡¨è§£æ | â­â­ API è°ƒç”¨ï¼Œå»¶è¿Ÿé—®é¢˜ |
| **GPT-4o** | OpenAI | 2024.05 | ~1T | å¤šæ¨¡æ€ç†è§£å¼ºï¼Œç»Ÿä¸€ Transformer æ¶æ„ | â­â­ API å»¶è¿Ÿé«˜ï¼Œä¸é€‚åˆå®æ—¶æ§åˆ¶ |
| **GPT-4o-mini** | OpenAI | 2024.07 | æœªå…¬å¼€ | è½»é‡ç‰ˆ GPT-4oï¼Œæˆæœ¬æ›´ä½ | â­â­ API è°ƒç”¨ï¼Œå»¶è¿Ÿä»è¾ƒé«˜ |
| **Gemini 1.5 Pro** | Google | 2024.02 | æœªå…¬å¼€ | 1M token ä¸Šä¸‹æ–‡ | â­â­ API è°ƒç”¨ï¼Œæˆæœ¬é«˜ |
| **Claude 3.5 Sonnet** | Anthropic | 2024.06 | æœªå…¬å¼€ | è§†è§‰ç†è§£å¼ºï¼Œæ€§èƒ½æå‡ | â­â­ API è°ƒç”¨ï¼Œå»¶è¿Ÿé—®é¢˜ |

**è¯´æ˜**: é—­æº API æ¨¡å‹è™½ç„¶èƒ½åŠ›å¼ºï¼Œä½†å­˜åœ¨å»¶è¿Ÿé«˜ã€æˆæœ¬é«˜ã€æ— æ³•æœ¬åœ°éƒ¨ç½²ç­‰é—®é¢˜ï¼Œä¸é€‚åˆç›´æ¥ç”¨äº VLA è®­ç»ƒã€‚å¯ä½œä¸ºå‚è€ƒæˆ–ç”¨äºæ•°æ®æ ‡æ³¨ã€CoT æ¨ç†ç­‰è¾…åŠ©ä»»åŠ¡ã€‚

**2025 å¹´é—­æºæ¨¡å‹è¶‹åŠ¿**:
- **Gemini 2.5 Pro**: ç›®å‰æ’è¡Œæ¦œç¬¬ä¸€ï¼Œå†…ç½®æ¨ç†æ€è€ƒåŠŸèƒ½
- **Claude 3.7**: OCR å’Œå›¾è¡¨è§£æèƒ½åŠ›å¤§å¹…æå‡

### 5.6.4 ç»å…¸æ¨¡å‹ï¼ˆå†å²å‚è€ƒï¼‰

| æ¨¡å‹ | æœºæ„ | å‘å¸ƒæ—¶é—´ | ç‰¹ç‚¹ | VLA å½±å“ |
| :--- | :--- | :--- | :--- | :--- |
| **BLIP-2** | Salesforce | 2023.01 | Q-Former æ¶æ„åˆ›æ–° | â­ æ—©æœŸ VLMï¼Œè¾ƒå°‘ç›´æ¥ç”¨äº VLA |
| **Flamingo** | DeepMind | 2022.04 | Perceiver Resampler, Gated Cross-Attention | â­â­ æ¶æ„åˆ›æ–°å½±å“æ·±è¿œï¼Œä½†æœªç›´æ¥ç”¨äº VLA |

### 5.6.5 VLA è®­ç»ƒé€‰æ‹©æŒ‡å—

#### å¿«é€Ÿé€‰æ‹©

```
éœ€è¦è½»é‡çº§ã€å•å¡è®­ç»ƒï¼Ÿ
  â”œâ”€ æ˜¯ â†’ PaliGemma 3B (é¦–é€‰)
  â””â”€ å¦ â†’ LLaVA 7B/13B

åªéœ€è¦ Vision Encoderï¼Ÿ
  â””â”€ SigLIP (VLA é¦–é€‰)

éœ€è¦ä¸­æ–‡æ”¯æŒï¼Ÿ
  â””â”€ Qwen-VL 7B

éœ€è¦è¾¹ç¼˜éƒ¨ç½²ï¼Ÿ
  â””â”€ MiniCPM-V 2.4B

éœ€è¦é«˜åˆ†è¾¨ç‡è¾“å…¥ï¼Ÿ
  â””â”€ InternVL æˆ– PaliGemma 896px ç‰ˆæœ¬
```

#### æŠ€æœ¯å¯¹æ¯”

| ç‰¹æ€§ | PaliGemma 3B | LLaVA 7B | Qwen-VL 7B | SigLIP (Vision) |
| :--- | :--- | :--- | :--- | :--- |
| **è®­ç»ƒæ•ˆç‡** | â­â­â­â­â­ å•å¡å¯è®­ç»ƒ | â­â­â­ éœ€è¦å¤šå¡ | â­â­â­ éœ€è¦å¤šå¡ | â­â­â­â­â­ ä»… Vision |
| **æ¨ç†é€Ÿåº¦** | â­â­â­â­ å¿« | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­â­â­ æå¿« |
| **è§†è§‰ç†è§£** | â­â­â­â­ å¼º | â­â­â­â­ å¼º | â­â­â­â­ å¼º | â­â­â­â­â­ æœ€å¼º |
| **ä¸­æ–‡æ”¯æŒ** | â­â­ ä¸€èˆ¬ | â­â­ ä¸€èˆ¬ | â­â­â­â­â­ ä¼˜ç§€ | - |
| **VLA ç”Ÿæ€** | â­â­â­â­â­ æœ€å¸¸ç”¨ | â­â­â­â­ æˆç†Ÿ | â­â­â­ è¾ƒå°‘ | â­â­â­â­â­ æœ€å¸¸ç”¨ |

#### å®é™…åº”ç”¨æ¡ˆä¾‹

1. **Ï€0 (Pi-Zero)**: ä½¿ç”¨ PaliGemma 3B ä½œä¸º VLM backboneï¼Œç»“åˆ Flow Matching å®ç°é«˜é¢‘æ§åˆ¶
2. **OpenVLA**: ä½¿ç”¨ Llama 2 7B + SigLIP ç»„åˆï¼Œé€šè¿‡ LoRA é«˜æ•ˆå¾®è°ƒ
3. **RT-2**: ä½¿ç”¨ PaLI-X 55Bï¼ˆé—­æºï¼‰ï¼Œè¯æ˜äº† VLM è¯­ä¹‰èƒ½åŠ›å¯è¿ç§»åˆ°æœºå™¨äººæ§åˆ¶
4. **RDT**: ä½¿ç”¨ SigLIP ä½œä¸º Vision Encoderï¼Œä¸“æ³¨äºè§†è§‰ç‰¹å¾æå–

### 5.6.6 é›†æˆå»ºè®®

#### ä½¿ç”¨ PaliGemma 3B è®­ç»ƒ VLA

```python
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
import torch

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = PaliGemmaForConditionalGeneration.from_pretrained(
    "google/paligemma-3b-pt-224",
    torch_dtype=torch.bfloat16
)
processor = AutoProcessor.from_pretrained("google/paligemma-3b-pt-224")

# 2. è·å–å¤šæ¨¡æ€ç‰¹å¾ï¼ˆç”¨äº Action Headï¼‰
def get_vlm_features(images, text_instructions):
    inputs = processor(images=images, text=text_instructions, return_tensors="pt")
    outputs = model(**inputs, output_hidden_states=True)
    hidden = outputs.hidden_states[-1]  # [B, L, 2048]
    return hidden

# 3. æ¥ Action Head
action_head = nn.Linear(2048, action_dim * chunk_size)
actions = action_head(hidden[:, -1, :])  # ä½¿ç”¨æœ€åä¸€ä¸ª token
```

#### ä½¿ç”¨ SigLIP ä½œä¸º Vision Encoder

```python
from transformers import AutoProcessor, AutoModel
import torch

# åŠ è½½ SigLIP Vision Encoder
vision_encoder = AutoModel.from_pretrained("google/siglip-base-patch16-224")
processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

# æå–è§†è§‰ç‰¹å¾
def extract_vision_features(images):
    inputs = processor(images=images, return_tensors="pt")
    outputs = vision_encoder(**inputs)
    return outputs.last_hidden_state  # [B, N_patches, D]
```

### 5.6.7 Pre-training vs Fine-tuning vs Post-training

> **é‡è¦æ¦‚å¿µ**: åœ¨ VLA è®­ç»ƒä¸­ï¼Œè¿™ä¸‰ä¸ªæœ¯è¯­æœ‰æ˜ç¡®çš„åŒºåˆ«å’Œé¡ºåºã€‚

#### è®­ç»ƒé˜¶æ®µå¯¹æ¯”

| é˜¶æ®µ | è‹±æ–‡ | ä¸­æ–‡ | æ•°æ®æ¥æº | è®­ç»ƒç›®æ ‡ | å…¸å‹æ–¹æ³• | VLA åº”ç”¨ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Pre-training** | Pre-training | é¢„è®­ç»ƒ | å¤§è§„æ¨¡é€šç”¨æ•°æ® (ImageNet, CLIP, äº’è”ç½‘å›¾æ–‡) | å­¦ä¹ é€šç”¨è§†è§‰/è¯­è¨€ç‰¹å¾ | è‡ªç›‘ç£å­¦ä¹ ã€å¯¹æ¯”å­¦ä¹  | VLM backbone (PaliGemma, SigLIP) |
| **Fine-tuning** | Fine-tuning | å¾®è°ƒ | ç›®æ ‡ä»»åŠ¡æ•°æ® (æœºå™¨äººç¤ºæ•™æ•°æ®) | é€‚é…ç‰¹å®šä»»åŠ¡ | ç›‘ç£å­¦ä¹  (BC), LoRA | OpenVLA, Ï€0 åœ¨æœºå™¨äººæ•°æ®ä¸Šå¾®è°ƒ |
| **Post-training** | Post-training | åè®­ç»ƒ | äº¤äº’æ”¶é›†çš„æ•°æ® (æˆåŠŸ+å¤±è´¥è½¨è¿¹) | è‡ªæˆ‘æ”¹è¿›ï¼Œè¶…è¶Šç¤ºæ•™ | Offline RL (Recap) | Ï€*0.6 çš„ Recap ç®—æ³• |

#### è¯¦ç»†è¯´æ˜

**1. Pre-training (é¢„è®­ç»ƒ)**

```
å¤§è§„æ¨¡æ•°æ® (ImageNet/CLIP/äº’è”ç½‘å›¾æ–‡)
        â”‚
        â–¼
  å­¦ä¹ é€šç”¨ç‰¹å¾
        â”‚
        â–¼
  é¢„è®­ç»ƒæ¨¡å‹ (å¦‚ PaliGemma 3B)
```

- **ç›®æ ‡**: åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šå­¦ä¹ é€šç”¨çš„è§†è§‰å’Œè¯­è¨€ç†è§£èƒ½åŠ›
- **æ•°æ®**: é€šå¸¸ä¸éœ€è¦æ ‡æ³¨ï¼Œä½¿ç”¨è‡ªç›‘ç£æˆ–å¯¹æ¯”å­¦ä¹ 
- **ç»“æœ**: å¾—åˆ°ä¸€ä¸ªå…·å¤‡åŸºç¡€èƒ½åŠ›çš„æ¨¡å‹
- **VLA åº”ç”¨**: 
  - PaliGemma 3B åœ¨äº’è”ç½‘å›¾æ–‡æ•°æ®ä¸Šé¢„è®­ç»ƒ
  - SigLIP åœ¨å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ

**2. Fine-tuning (å¾®è°ƒ)**

```
é¢„è®­ç»ƒæ¨¡å‹ (PaliGemma 3B)
        â”‚
        â–¼
  ç›®æ ‡ä»»åŠ¡æ•°æ® (æœºå™¨äººç¤ºæ•™)
        â”‚
        â–¼
  å¾®è°ƒåæ¨¡å‹ (é€‚é…æœºå™¨äººæ§åˆ¶)
```

- **ç›®æ ‡**: åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œç”¨ç›®æ ‡ä»»åŠ¡æ•°æ®å¾®è°ƒï¼Œä½¿å…¶é€‚é…ç‰¹å®šä»»åŠ¡
- **æ•°æ®**: éœ€è¦æ ‡æ³¨çš„ç¤ºæ•™æ•°æ® (observation-action pairs)
- **æ–¹æ³•**: 
  - **Full Fine-tuning**: æ›´æ–°æ‰€æœ‰å‚æ•°ï¼ˆæ˜¾å­˜éœ€æ±‚å¤§ï¼‰
  - **LoRA/QLoRA**: åªè®­ç»ƒå°‘é‡å‚æ•°ï¼ˆæ¨èï¼‰
- **VLA åº”ç”¨**:
  - OpenVLA: åœ¨æœºå™¨äººæ•°æ®ä¸Š LoRA å¾®è°ƒ
  - Ï€0: åœ¨æœºå™¨äººæ•°æ®ä¸Šå¾®è°ƒ PaliGemma

**3. Post-training (åè®­ç»ƒ)**

```
å¾®è°ƒåæ¨¡å‹ (Ï€0.6)
        â”‚
        â–¼
  æœºå™¨äººäº¤äº’æ”¶é›†æ•°æ® (æˆåŠŸ+å¤±è´¥)
        â”‚
        â–¼
  Offline RL (Recap ç®—æ³•)
        â”‚
        â–¼
  æ”¹è¿›åæ¨¡å‹ (Ï€*0.6, è¶…è¶Šç¤ºæ•™)
```

- **ç›®æ ‡**: é€šè¿‡åˆ†ææˆåŠŸå’Œå¤±è´¥è½¨è¿¹ï¼Œè‡ªæˆ‘æ”¹è¿›ï¼Œè¶…è¶Šäººç±»ç¤ºæ•™æ°´å¹³
- **æ•°æ®**: æœºå™¨äººå®é™…è¿è¡Œæ”¶é›†çš„æ•°æ®ï¼ˆåŒ…å«æˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹ï¼‰
- **æ–¹æ³•**: Offline RL (å¦‚ Recap ç®—æ³•)
- **ç‰¹ç‚¹**: 
  - ä¸ä»…å­¦ä¹ "æ€ä¹ˆåš"ï¼Œè¿˜å­¦ä¹ "æ€ä¹ˆåšå¾—æ›´å¥½"
  - å¯ä»¥è¶…è¶Šäººç±»ç¤ºæ•™è€…çš„æ°´å¹³
- **VLA åº”ç”¨**: Ï€*0.6 çš„ Recap ç®—æ³•

#### å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹ (Ï€0.6 â†’ Ï€*0.6)

```python
# Phase 1: Pre-training (é€šå¸¸ç”±æ¨¡å‹æä¾›æ–¹å®Œæˆ)
# ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®è®­ç»ƒ PaliGemma 3B
pretrained_vlm = load_pretrained("google/paligemma-3b-pt-224")

# Phase 2: Fine-tuning (åœ¨æœºå™¨äººæ•°æ®ä¸Šå¾®è°ƒ)
# ä½¿ç”¨ç¤ºæ•™æ•°æ®å¾®è°ƒ
robot_demos = load_robot_demonstrations()  # äººç±»ç¤ºæ•™æ•°æ®
finetuned_model = fine_tune(pretrained_vlm, robot_demos, method="LoRA")
# å¾—åˆ° Ï€0.6

# Phase 3: Post-training (Recap, è‡ªæˆ‘æ”¹è¿›)
# æœºå™¨äººäº¤äº’æ”¶é›†æ•°æ®
interaction_data = robot.collect_data()  # åŒ…å«æˆåŠŸå’Œå¤±è´¥è½¨è¿¹

# ä½¿ç”¨ Offline RL æ”¹è¿›
improved_model = recap_algorithm(finetuned_model, interaction_data)
# å¾—åˆ° Ï€*0.6
```

#### å…³é”®åŒºåˆ«æ€»ç»“

| ç‰¹æ€§ | Pre-training | Fine-tuning | Post-training |
| :--- | :--- | :--- | :--- |
| **æ•°æ®æ¥æº** | é€šç”¨å¤§è§„æ¨¡æ•°æ® | ç›®æ ‡ä»»åŠ¡ç¤ºæ•™æ•°æ® | äº¤äº’æ”¶é›†çš„æˆåŠŸ+å¤±è´¥æ•°æ® |
| **è®­ç»ƒç›®æ ‡** | å­¦ä¹ é€šç”¨ç‰¹å¾ | é€‚é…ç‰¹å®šä»»åŠ¡ | è‡ªæˆ‘æ”¹è¿›ï¼Œè¶…è¶Šç¤ºæ•™ |
| **å­¦ä¹ æ–¹å¼** | è‡ªç›‘ç£/å¯¹æ¯”å­¦ä¹  | ç›‘ç£å­¦ä¹  (BC) | Offline RL |
| **æ˜¯å¦å¿…éœ€** | âœ… æ˜¯ (æ¨¡å‹åŸºç¡€) | âœ… æ˜¯ (ä»»åŠ¡é€‚é…) | âš ï¸ å¯é€‰ (æ€§èƒ½æå‡) |
| **å…¸å‹æ—¶é—´** | æ•°å‘¨/æœˆ (å¤§è§„æ¨¡) | æ•°å°æ—¶/å¤© | æ•°å¤©/å‘¨ (æŒç»­æ”¹è¿›) |

#### é¢è¯•å¸¸è§é—®é¢˜

**Q: Pre-training å’Œ Fine-tuning çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

A:
- **Pre-training**: åœ¨å¤§è§„æ¨¡é€šç”¨æ•°æ®ä¸Šå­¦ä¹ åŸºç¡€èƒ½åŠ›ï¼ˆå¦‚è§†è§‰ç†è§£ã€è¯­è¨€ç†è§£ï¼‰
- **Fine-tuning**: åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œç”¨ç›®æ ‡ä»»åŠ¡æ•°æ®å¾®è°ƒï¼Œä½¿å…¶é€‚é…ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚æœºå™¨äººæ§åˆ¶ï¼‰

**Q: Post-training å’Œ Fine-tuning çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

A:
- **Fine-tuning**: ä½¿ç”¨äººç±»ç¤ºæ•™æ•°æ®ï¼Œå­¦ä¹ "æ€ä¹ˆåš"ï¼ˆæ¨¡ä»¿å­¦ä¹ ï¼‰
- **Post-training**: ä½¿ç”¨äº¤äº’æ”¶é›†çš„æˆåŠŸ+å¤±è´¥æ•°æ®ï¼Œå­¦ä¹ "æ€ä¹ˆåšå¾—æ›´å¥½"ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œå¯ä»¥è¶…è¶Šäººç±»ç¤ºæ•™æ°´å¹³

**Q: ä¸ºä»€ä¹ˆéœ€è¦ Pre-trainingï¼Ÿ**

A: 
- æœºå™¨äººæ•°æ®ç¨€ç¼ºä¸”æ˜‚è´µï¼Œä»å¤´è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®
- Pre-training è®©æ¨¡å‹å…·å¤‡é€šç”¨èƒ½åŠ›ï¼Œåªéœ€å°‘é‡æœºå™¨äººæ•°æ®å³å¯é€‚é…
- ç±»ä¼¼äººç±»å…ˆå­¦åŸºç¡€çŸ¥è¯†ï¼Œå†å­¦ä¸“ä¸šæŠ€èƒ½

### 5.6.8 å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆ VLA é¦–é€‰ PaliGemma 3B è€Œä¸æ˜¯æ›´å¤§çš„ LLaVA?**

A: ä¸‰ä¸ªåŸå› :
1. **æ•ˆç‡**: 3B å‚æ•°å¯åœ¨å•å¡ (24GB) è®­ç»ƒ/æ¨ç†ï¼Œæ»¡è¶³æœºå™¨äººå®æ—¶æ€§è¦æ±‚
2. **SigLIP**: æ¯” CLIP æ›´å¥½çš„ç»†ç²’åº¦è§†è§‰ç†è§£
3. **æ¨¡å—åŒ–**: Vision/Language è§£è€¦ï¼Œæ–¹ä¾¿æ¥ Action Head

**Q: SigLIP å’Œ CLIP çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

A: 
- **æŸå¤±å‡½æ•°**: CLIP ä½¿ç”¨ Softmax + Cross-Entropy (InfoNCE)ï¼ŒSigLIP ä½¿ç”¨ Sigmoid + Binary CE
- **Batch ä¾èµ–**: CLIP çš„ Softmax éœ€è¦å¯¹æ¯” batch å†…æ‰€æœ‰æ ·æœ¬ï¼ŒSigLIP çš„ Sigmoid æ¯å¯¹ç‹¬ç«‹è®¡ç®—
- **æ‰©å±•æ€§**: SigLIP æ›´é€‚åˆå¤§ batch è®­ç»ƒï¼Œè´Ÿæ ·æœ¬åˆ©ç”¨æ›´é«˜æ•ˆ

**Q: å¦‚ä½•é€‰æ‹© Vision Encoder å’Œ LLM çš„ç»„åˆï¼Ÿ**

A:
- **è½»é‡çº§**: PaliGemma 3B (SigLIP + Gemma 2B)
- **å¹³è¡¡**: LLaVA (CLIP/ViT + Llama 2 7B)
- **è‡ªå®šä¹‰**: SigLIP (Vision) + ä»»æ„ LLM (Language)

**Q: ä¸­æ–‡ VLA ä»»åŠ¡åº”è¯¥é€‰æ‹©å“ªä¸ª VLMï¼Ÿ**

A: æ¨è **Qwen2.5-VL 7B**ï¼ˆğŸ†• 2025.03ï¼‰ï¼Œä¸­æ–‡æ”¯æŒæœ€å¥½ï¼Œæ•°å­¦æ¨ç†èƒ½åŠ›å¼ºï¼Œæ”¯æŒä»»æ„åˆ†è¾¨ç‡å’Œé•¿è§†é¢‘ã€‚å¦‚æœèµ„æºå—é™ï¼Œå¯é€‰æ‹© **Qwen2.5-VL 3B** ç‰ˆæœ¬ã€‚

**Q: æœ‰å“ªäº› 2025 å¹´æœ€æ–°çš„ VLM æ›´æ–°å€¼å¾—å…³æ³¨ï¼Ÿ**

A: 
- **Qwen2.5-VL** (2025.03): é˜¿é‡Œå·´å·´æœ€æ–°ç‰ˆæœ¬ï¼Œ**2025 SOTA**ï¼Œæ•°å­¦æ¨ç†å¼ºï¼Œæ”¯æŒä»»æ„åˆ†è¾¨ç‡
- **Eagle 2.5** (2025.04): NVIDIA å‘å¸ƒï¼Œé•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ï¼ŒVideo-MME 72.4%
- **Seed 1.5-VL** (2025.05): å­—èŠ‚è·³åŠ¨å‘å¸ƒï¼Œåª²ç¾ Gemini 2.5 Proï¼ŒGUI äº¤äº’å¼º
- **GLM-4.5V** (2025): æ™ºè°±AIï¼ŒMoE æ¶æ„ï¼Œ3D ç©ºé—´æ¨ç†
- **Llama 4** (2025.04): Meta å‘å¸ƒï¼Œ10M token ä¸Šä¸‹æ–‡ï¼Œå¤šæ¨¡æ€ MoE æ¶æ„
- **PLM** (2025.05): Meta å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹

**Q: 2025å¹´é—­æº API æ¨¡å‹æœ‰å“ªäº›æ›´æ–°ï¼Ÿ**

A:
- **Gemini 2.5 Pro** (2025.03): Google å‘å¸ƒï¼Œæ’è¡Œæ¦œç¬¬ä¸€ï¼Œå†…ç½®æ€è€ƒåŠŸèƒ½
- **Claude 3.7 Vision** (2025.02): Anthropic å‘å¸ƒï¼Œé«˜ç²¾åº¦ OCR å’Œå›¾è¡¨è§£æ

---

## 6. æŠ•å½±å±‚è®¾è®¡ (Projector Design)

å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç©ºé—´æ˜¯ VLA çš„å…³é”®ã€‚

### 6.1 ç®€å• MLP

```python
class MLPProjector(nn.Module):
    def __init__(self, vision_dim, language_dim):
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, language_dim),
            nn.GELU(),
            nn.Linear(language_dim, language_dim)
        )
    
    def forward(self, vision_feat):
        return self.proj(vision_feat)
```

### 6.2 Perceiver Resampler (Flamingo)

```python
class PerceiverResampler(nn.Module):
    """å°†å¯å˜æ•°é‡çš„è§†è§‰ Token å‹ç¼©ä¸ºå›ºå®šæ•°é‡"""
    def __init__(self, num_latents=64):
        self.latents = nn.Parameter(torch.randn(num_latents, hidden_dim))
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        
    def forward(self, vision_tokens):
        # vision_tokens: [B, N_patches, D] (N_patches å¯å˜)
        # è¾“å‡º: [B, num_latents, D] (å›ºå®š)
        
        latents = self.latents.unsqueeze(0).expand(B, -1, -1)
        output, _ = self.cross_attn(
            query=latents,
            key=vision_tokens,
            value=vision_tokens
        )
        return output  # [B, 64, D]
```

**ä¼˜åŠ¿**: æ§åˆ¶è§†è§‰ Token æ•°é‡ï¼Œå‡å°‘ LLM çš„è®¡ç®—è´Ÿæ‹…

### 6.3 Q-Former (BLIP-2)

ä½¿ç”¨å¯å­¦ä¹ çš„ Query ä»è§†è§‰ç¼–ç å™¨ä¸­æå–ä¸ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ã€‚

## 7. å®æˆ˜ï¼šæ„å»ºç®€å•çš„å¤šæ¨¡æ€ VLA

```python
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer

class SimpleMultimodalVLA(nn.Module):
    def __init__(
        self,
        vision_encoder_name="google/siglip-base-patch16-224",
        language_model_name="meta-llama/Llama-2-7b-hf",
        action_dim=7,
        chunk_size=16
    ):
        super().__init__()
        
        # è§†è§‰ç¼–ç å™¨ (å†»ç»“)
        self.vision_encoder = AutoModel.from_pretrained(vision_encoder_name)
        for param in self.vision_encoder.parameters():
            param.requires_grad = False
        
        # æŠ•å½±å±‚ (å¯è®­ç»ƒ)
        vision_dim = self.vision_encoder.config.hidden_size
        language_dim = 4096  # Llama 2 hidden dim
        self.vision_projector = nn.Sequential(
            nn.Linear(vision_dim, language_dim),
            nn.GELU(),
            nn.Linear(language_dim, language_dim)
        )
        
        # è¯­è¨€æ¨¡å‹ (LoRA å¾®è°ƒ)
        self.language_model = AutoModel.from_pretrained(
            language_model_name,
            load_in_4bit=True  # QLoRA
        )
        
        # åŠ¨ä½œå¤´ (å¯è®­ç»ƒ)
        self.action_head = nn.Sequential(
            nn.Linear(language_dim, language_dim // 2),
            nn.ReLU(),
            nn.Linear(language_dim // 2, action_dim * chunk_size)
        )
        
        self.chunk_size = chunk_size
        self.action_dim = action_dim
    
    def forward(self, images, input_ids, attention_mask):
        """
        images: [B, C, H, W]
        input_ids: [B, L]
        attention_mask: [B, L]
        """
        batch_size = images.shape[0]
        
        # 1. è§†è§‰ç¼–ç 
        with torch.no_grad():
            vision_outputs = self.vision_encoder(images)
            vision_features = vision_outputs.last_hidden_state  # [B, N_patches, D_v]
        
        # 2. æŠ•å½±åˆ°è¯­è¨€ç©ºé—´
        vision_tokens = self.vision_projector(vision_features)  # [B, N_patches, D_l]
        
        # 3. è·å–è¯­è¨€åµŒå…¥
        text_embeds = self.language_model.get_input_embeddings()(input_ids)
        
        # 4. æ‹¼æ¥ [Vision Tokens | Text Tokens]
        inputs_embeds = torch.cat([vision_tokens, text_embeds], dim=1)
        
        # 5. é€šè¿‡è¯­è¨€æ¨¡å‹
        outputs = self.language_model(
            inputs_embeds=inputs_embeds,
            output_hidden_states=True
        )
        
        # 6. å–æœ€åä¸€ä¸ª hidden state ä½œä¸ºåŠ¨ä½œæ¡ä»¶
        last_hidden = outputs.hidden_states[-1][:, -1, :]  # [B, D_l]
        
        # 7. é¢„æµ‹åŠ¨ä½œ
        actions = self.action_head(last_hidden)  # [B, action_dim * chunk_size]
        actions = actions.view(batch_size, self.chunk_size, self.action_dim)
        
        return actions
```

## 8. é¢è¯•é«˜é¢‘é—®é¢˜ (Q&A)

**Q1: CLIP å’Œ SigLIP çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

A:
- **æŸå¤±å‡½æ•°**: CLIP ä½¿ç”¨ Softmax + Cross-Entropy (InfoNCE)ï¼ŒSigLIP ä½¿ç”¨ Sigmoid + Binary CE
- **batch ä¾èµ–**: CLIP çš„ Softmax éœ€è¦å¯¹æ¯” batch å†…æ‰€æœ‰æ ·æœ¬ï¼ŒSigLIP çš„ Sigmoid æ¯å¯¹ç‹¬ç«‹è®¡ç®—
- **æ‰©å±•æ€§**: SigLIP æ›´é€‚åˆå¤§ batch è®­ç»ƒï¼Œè´Ÿæ ·æœ¬åˆ©ç”¨æ›´é«˜æ•ˆ

**Q2: ä¸ºä»€ä¹ˆ VLA æ™®éé€‰æ‹© Decoder-only LLM è€Œä¸æ˜¯ BERTï¼Ÿ**

A:
- **ç”Ÿæˆèƒ½åŠ›**: Decoder-only å¤©ç„¶æ”¯æŒè‡ªå›å½’ç”Ÿæˆï¼ˆåŒ…æ‹¬åŠ¨ä½œ Tokenï¼‰
- **In-context Learning**: å¯ä»¥é€šè¿‡ Prompt å¼•å¯¼æ¨¡å‹ç†è§£æ–°ä»»åŠ¡
- **è§„æ¨¡æ•ˆåº”**: å¤§è§„æ¨¡ LLM (7B+) ä¸»è¦æ˜¯ Decoder-only æ¶æ„ï¼Œå¯ä»¥ç›´æ¥å¤ç”¨

**Q3: å¤šæ¨¡æ€èåˆä¸­ Early / Mid / Late Fusion å¦‚ä½•é€‰æ‹©ï¼Ÿ**

A:
- **Early Fusion**: æ•°æ®æ¨¡æ€ç›¸ä¼¼åº¦é«˜ï¼ˆå¦‚å¤šç›¸æœºå›¾åƒï¼‰
- **Mid Fusion (Cross-Attention)**: éœ€è¦åŠ¨æ€å»ºæ¨¡æ¨¡æ€é—´å…³ç³»ï¼ˆVLA é¦–é€‰ï¼‰
- **Late Fusion**: å„æ¨¡æ€ä»»åŠ¡ç‹¬ç«‹æ€§å¼ºï¼Œæˆ–éœ€è¦æ¨¡å—åŒ–è§£é‡Šæ€§

**Q4: è§†è§‰ Token æ•°é‡å¦‚ä½•é€‰æ‹©ï¼Ÿ**

A:
- **å¤šäº†**: LLM è®¡ç®—å¼€é”€å¤§ï¼Œé•¿åºåˆ— Attention å˜æ…¢
- **å°‘äº†**: ä¸¢å¤±ç©ºé—´ç»†èŠ‚ï¼Œå½±å“ç²¾ç»†æ“ä½œ
- **å¸¸è§é€‰æ‹©**: 256 tokens (16x16 patches @ 224px)ï¼Œæˆ–ä½¿ç”¨ Perceiver Resampler å‹ç¼©åˆ° 64

**Q5: ä¸ºä»€ä¹ˆè¦å†»ç»“è§†è§‰ç¼–ç å™¨ï¼Ÿ**

A:
- **é˜²æ­¢ç¾éš¾æ€§é—å¿˜**: è§†è§‰ç¼–ç å™¨çš„é¢„è®­ç»ƒç‰¹å¾å¾ˆé‡è¦
- **è®¡ç®—æ•ˆç‡**: å‡å°‘å¯è®­ç»ƒå‚æ•°
- **æ•°æ®æ•ˆç‡**: æœºå™¨äººæ•°æ®å°‘ï¼Œå…¨é‡è®­ç»ƒå®¹æ˜“è¿‡æ‹Ÿåˆ
- **ä¾‹å¤–**: å¦‚æœè§†è§‰ä»»åŠ¡å·®å¼‚å¤§ï¼ˆå¦‚ä» ImageNet è¿ç§»åˆ°å†…çª¥é•œï¼‰ï¼Œå¯èƒ½éœ€è¦å¾®è°ƒ

**Q6: å¦‚æœè§†è§‰æ¨¡å—è¯¯åˆ¤ï¼Œå¦‚ä½•é€šè¿‡è¯­è¨€çº é”™ï¼Ÿ**

A: è¿™æ˜¯å¤šæ¨¡æ€ VLA çš„æ ¸å¿ƒä¼˜åŠ¿ä¹‹ä¸€ï¼Œæœ‰ä»¥ä¸‹å‡ ç§æœºåˆ¶ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è§†è§‰è¯¯åˆ¤ â†’ è¯­è¨€çº é”™æœºåˆ¶                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   åœºæ™¯: è§†è§‰æ¨¡å—è¯¯åˆ¤ "çº¢è‰²æ¯å­" ä¸º "æ©™è‰²æ¯å­"                    â”‚
â”‚                                                                 â”‚
â”‚   æ–¹æ¡ˆ 1: é—­ç¯è¯­è¨€åé¦ˆ (Human-in-the-Loop)                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  ç”¨æˆ·: "ä¸å¯¹ï¼Œæ˜¯çº¢è‰²çš„é‚£ä¸ª"                               â”‚   â”‚
â”‚   â”‚  VLA: é‡æ–°å®šä½ â†’ ä¿®æ­£ç›®æ ‡                                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   æ–¹æ¡ˆ 2: Chain-of-Thought è‡ªæ£€                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  VLA è¾“å‡º: "æˆ‘çœ‹åˆ°ä¸€ä¸ªæ©™è‰²ç‰©ä½“..."                        â”‚   â”‚
â”‚   â”‚  ç”¨æˆ·æŒ‡ä»¤: "æŠ“çº¢è‰²æ¯å­"                                   â”‚   â”‚
â”‚   â”‚  CoT æ¨ç†: "æŒ‡ä»¤è¯´çº¢è‰²ï¼Œä½†æˆ‘è¯†åˆ«ä¸ºæ©™è‰²ï¼Œå¯èƒ½æœ‰è¯¯"          â”‚   â”‚
â”‚   â”‚  åŠ¨ä½œ: è¯·æ±‚ç¡®è®¤ æˆ– é‡æ–°æ„ŸçŸ¥                              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   æ–¹æ¡ˆ 3: å¤šæ¨¡æ€ä¸€è‡´æ€§æ£€æŸ¥                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  è®¡ç®—: sim(è¯­è¨€æè¿° Embedding, è§†è§‰ç‰¹å¾ Embedding)        â”‚   â”‚
â”‚   â”‚  å¦‚æœ sim < threshold: è§¦å‘é‡æ–°æ„ŸçŸ¥/è¯¢é—®                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   æ–¹æ¡ˆ 4: ä¸»åŠ¨è¯¢é—® (Uncertainty-aware)                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  VLA: "ä½ æ˜¯æŒ‡è¿™ä¸ªå—ï¼Ÿ" (æ˜¾ç¤ºå€™é€‰ç‰©ä½“)                     â”‚   â”‚
â”‚   â”‚  ç”¨æˆ·: "æ˜¯çš„" / "ä¸æ˜¯ï¼Œæ˜¯å·¦è¾¹é‚£ä¸ª"                        â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®ç°è¦ç‚¹**:
1. **è¯­ä¹‰æ¥åœ° (Grounding)**: è¯­è¨€æŒ‡ä»¤å¿…é¡»ä¸è§†è§‰æ£€æµ‹ç»“æœç»‘å®šï¼Œè€Œéç‹¬ç«‹å¤„ç†
2. **ç½®ä¿¡åº¦è¾“å‡º**: è§†è§‰æ¨¡å—è¾“å‡ºæ£€æµ‹ç½®ä¿¡åº¦ï¼Œä½ç½®ä¿¡åº¦æ—¶è§¦å‘çº é”™æœºåˆ¶
3. **å¤šè½®å¯¹è¯**: VLA éœ€è¦æ”¯æŒå¤šè½®äº¤äº’ï¼Œè€Œéå•æ¬¡æŒ‡ä»¤æ‰§è¡Œ
4. **CoT æ¨ç†**: æ˜¾å¼è¾“å‡ºæ¨ç†è¿‡ç¨‹ï¼Œä¾¿äºå‘ç°çŸ›ç›¾ (å‚è§ [chain_of_thought.md](./chain_of_thought.md))

## 9. å‚è€ƒèµ„æº (References)

- **CLIP**: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- **LLaVA**: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)
- **Flamingo**: [A Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)
- **SigLIP**: [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343)

---
[â† Back to Theory](./README.md)

