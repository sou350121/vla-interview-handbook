# Physics of AI：不赌规模，把神经网络当作“物理系统”来研究（刘子鸣专访笔记）

这篇笔记整理自《专访 MIT 刘子鸣：另辟蹊径，不赌规模：Physics of AI 是通往 AGI 的“科学路径”》的核心观点，并把它翻译成更可执行的工程抓手：**如何观察、定义现象、做 toy experiments、建立可测量的“中间知识节点”**，从而把“结构主义”落到具体方法上。

---

## 1) 什么是 Physics of AI（他在讲的“科学路径”）

核心不是“用物理公式替代神经网络”，而是把神经网络视为一个可控的复杂系统，用更像物理学的方式推进：

- **现象（Phenomena）**：训练过程中出现哪些可复现行为（如顿悟、表征几何演化、信息瓶颈等）。
- **观测量（Observables）**：能不能定义一些中间量，稳定地刻画现象的出现与消失（而不只盯最终指标）。
- **规律/理论（Laws/Theory）**：把现象之间的关系串成“知识网络”，最终反过来指导算法/结构设计。

他强调这条路的价值在于“效率”：用更少数据/算力/能量达到同样甚至更好的效果；同时让黑箱更透明、更可控。

---

## 2) 三条 AGI 路径：Scaling / Agent / Physics of AI

专访里给出了一个很清晰的分法：

- **Scaling**：短期有效，但可能会撞上能量/数据/算力瓶颈。
- **Agent 路线**：接受黑箱不可完全理解的现实，在外部叠加记忆、工具、长期学习等系统结构，争取工程可用性。
- **Physics of AI**：把“结构主义”变成可执行路径——通过观察与归纳，为 AI 建立可解释、可推演的理论体系。

他本人更看重第三条（长期），但承认短期里 Scaling 仍会带来红利。

---

## 3) 代表性成果与“为什么重要”

### 3.1 KAN：用 Kolmogorov–Arnold 表示定理重构网络基元

他说 KAN 的核心动机是两点：

- **符号公式/科学计算**：希望网络能高效表示并提取符号规律，而不是停留在不可解释的拟合器。
- **替代 MLP 的基元依赖**：不走“万能逼近定理 + 黑箱 MLP”的默认路线，而是换一个底层数学表述（KA 表示定理）来构建网络。

他强调 KAN 的优势集中在：

- **可解释性**：对含符号结构的任务更友好。
- **高精度需求**：可学习的基础函数更灵活，精度扩展不必总是“重新训更大的 MLP”。

### 3.2 “顿悟现象”（Grokking）：从表征几何演化与表达能力压缩理解

专访里给了两种互补的解释角度：

- **表征几何演化**：嵌入从随机初始化逐步演化出可描述的几何形态（例如环状结构），并与泛化跃迁相关。
- **表达能力压缩**：正则化等机制让模型从“记忆训练集”被迫转向“压缩出更简单的算法”，一旦找到该算法，就出现性能突变。

这类“训练中相变/跃迁”现象，是 Physics of AI 想系统化刻画与预测的对象。

### 3.3 生成模型：为什么一定是 diffusion？能否用电场/波动等物理过程

他提到一条探索线：把生成过程类比为物理过程（如电场吸引），并指出某些物理方程在合适正则化下能成为合格生成模型。

同时也给出务实判断：随着 Flow Matching / EDM 等数学统一框架推进，这条“物理比喻”路线未必能直接带来颠覆性算法提升，但仍能提供洞见与边界理解。

---

## 4) 方法论：Physics of AI 的“工程抓手”

这部分是最适合落到团队实践的：

### 4.1 toy model + 控变量实验：把“大象”拆成可观测侧面

把复杂系统当“大象”，不同角度的观察都是必要的；关键是：

- 选择可控、可复现实验设置（toy 数据、toy 架构、toy 任务）
- 定义你要测的观测量（而不是只看最终 loss/acc）
- 追踪训练全过程中的结构变化（表征、稀疏性、对称性、曲率、信息量等）

### 4.2 建立“中间知识节点”，而不是只关心最后一层表现

他认为现在很多研究只盯最终指标，导致中间的知识节点缺失；Physics of AI 的难点与核心，就是把这些节点补齐并串联起来。

### 4.3 用博客/开放研究积累“小洞察”

他直言当前发表文化不鼓励“细碎但重要”的观察，导致很多小洞察被浪费；因此他选择用博客每日记录，并倡导类似 LessWrong 的开放论坛来汇聚社区观察。

---

## 5) 对具身/VLA 的映射：怎么把 Physics of AI 用到“看-说-动”

VLA 系统比纯语言/纯视觉更像“真实复杂系统”，更需要一套可复用的观察与诊断法。

### 5.1 典型问题：泛化失败到底是“推理”坏了还是“动作传递”坏了？

你可以把 VLA 的失败拆成可诊断的中间节点（类似 ERIQ 的思路）：

- **感知是否学到了关键属性？**
- **语言先验是否真的进入了视觉/状态表征？**
- **规划结构是否稳定？**
- **动作生成是否在离散结构 → 连续控制时损失信息？**

### 5.2 建议的可执行流程（Physics-of-AI 风格）

- **定义现象**：例如“长任务中突然泛化跃迁/突然崩溃”、“某类属性永远学不会（比如摩擦/拧紧）”
- **定义观测量**：表征几何、token 使用分布、跨模态对齐强度、分层控制误差累积曲线
- **做控变量实验**：只改一个因素（语言监督强度/触觉通道/动作分词长度/正则化）
- **建立规律与边界**：哪些结构性改动会稳定改善该类现象，哪些只是偶然有效

---

## 6) 参考（待补充可点击链接/DOI）

- 专访原文：专访 MIT 刘子鸣《另辟蹊径，不赌规模：Physics of AI 是通往 AGI 的“科学路径”》
- KAN：Kolmogorov–Arnold Networks（KAN）
- Grokking / “The Clock and the Pizza”
- Poisson Flow Generative Models（PFGM）

