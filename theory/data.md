# æ•°æ®å¤„ç† (Data Processing)

åœ¨ VLA æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œæ•°æ®æ˜¯æ ¸å¿ƒå£å’ã€‚æœ¬ç« ä»‹ç»æœºå™¨äººå­¦ä¹ ä¸­é€šç”¨çš„æ•°æ®æ ¼å¼å’Œå¤„ç†ç­–ç•¥ã€‚

## 1. ä¸»æµæ•°æ®æ ¼å¼å¯¹æ¯” (Mainstream Data Formats)

åœ¨ VLA é¢†åŸŸï¼Œæ•°æ®æ ¼å¼çš„é€‰æ‹©ç›´æ¥å½±å“è®­ç»ƒæ•ˆç‡å’Œç”Ÿæ€å…¼å®¹æ€§ã€‚ç›®å‰ä¸»è¦æœ‰ä¸‰ç§ä¸»æµæ ¼å¼ï¼š

### 1.1. RLDS (Robotics Language-Image Datasets)
- **ç”Ÿæ€ä½**: **Google / Open X-Embodiment æ ‡å‡†**ã€‚
- **åº•å±‚**: åŸºäº `TensorFlow Datasets (TFDS)` å’Œ `ProtoBuf`ã€‚
- **ç‰©ç†æ ¼å¼**: **`.tfrecord`** æ–‡ä»¶ã€‚
    - è¿™æ˜¯ä¸€ç§åŸºäºè¡Œ (Row-based) çš„äºŒè¿›åˆ¶åºåˆ—åŒ–æ ¼å¼ï¼Œå°†æ•°æ®åºåˆ—åŒ–ä¸º Protocol Buffers æ¶ˆæ¯ã€‚
- **ç‰¹ç‚¹**:
    - **åºåˆ—åŒ–**: é€‚åˆå¤§è§„æ¨¡åˆ†å¸ƒå¼è¯»å–ï¼ŒGoogle TPU å‹å¥½ã€‚
    - **æ ‡å‡†åŒ–**: å¼ºåˆ¶å®šä¹‰äº† `observation`, `action`, `language` çš„æ ‡å‡†æ¥å£ã€‚
    - **æµå¼è¯»å–**: æ”¯æŒäº‘ç«¯å­˜å‚¨ (GCS) çš„æµå¼è®­ç»ƒï¼Œæ— éœ€ä¸‹è½½æ•´ä¸ªæ•°æ®é›†ã€‚
- **é€‚ç”¨åœºæ™¯**: ä½¿ç”¨ TPU è®­ç»ƒï¼Œæˆ–åŸºäº RT-1/RT-2/Octo æ¶æ„å¼€å‘æ—¶ã€‚

### 1.2. LeRobot Dataset (Hugging Face)
- **ç”Ÿæ€ä½**: **PyTorch / Open Source ç¤¾åŒºæ–°æ ‡å‡†**ã€‚
- **åº•å±‚**: åŸºäº `Parquet` (åˆ—å¼å­˜å‚¨) å’Œ `Hugging Face Datasets` (Apache Arrow)ã€‚
- **ç‰©ç†æ ¼å¼**: **`.parquet`** æ–‡ä»¶ã€‚
    - è¿™æ˜¯ä¸€ç§åŸºäºåˆ— (Column-based) çš„å­˜å‚¨æ ¼å¼ï¼Œå‹ç¼©ç‡æé«˜ï¼Œè¯»å–ç‰¹å®šåˆ—ï¼ˆå¦‚åªè¯» Action ä¸è¯» Imageï¼‰éå¸¸å¿«ã€‚
- **ç‰¹ç‚¹**:
    - **å¯è§†åŒ–**: åœ¨ Hugging Face ç½‘é¡µç«¯å¯ç›´æ¥é¢„è§ˆè§†é¢‘å’Œå…ƒæ•°æ®ã€‚
    - **è½»é‡çº§**: ä¸ä¾èµ– TensorFlowï¼Œå®‰è£…ç®€å• (`pip install lerobot`)ã€‚
    - **PyTorch åŸç”Ÿ**: æ•°æ®åŠ è½½å™¨ç›´æ¥è¾“å‡º PyTorch Tensorsã€‚
- **é€‚ç”¨åœºæ™¯**: ä½¿ç”¨ GPU è®­ç»ƒï¼ŒåŸºäº OpenVLA/ACT/Diffusion Policy å¼€å‘æ–°é¡¹ç›®æ—¶ã€‚

### 1.3. HDF5 / Robomimic
- **ç”Ÿæ€ä½**: **ä¼ ç»Ÿç§‘ç ” / ä»¿çœŸæ•°æ®æ ‡å‡†**ã€‚
- **åº•å±‚**: `HDF5` (Hierarchical Data Format)ã€‚
- **ç‰©ç†æ ¼å¼**: **`.hdf5`** æˆ– **`.h5`** æ–‡ä»¶ã€‚
    - ç±»ä¼¼äºä¸€ä¸ª"æ–‡ä»¶ç³»ç»Ÿ"ï¼Œå†…éƒ¨å¯ä»¥åƒæ–‡ä»¶å¤¹ä¸€æ ·ç»„ç»‡æ•°æ® (Groups/Datasets)ã€‚
- **ç‰¹ç‚¹**:
    - **å•æ–‡ä»¶**: æ•´ä¸ªæ•°æ®é›†é€šå¸¸æ˜¯ä¸€ä¸ªå·¨å¤§çš„äºŒè¿›åˆ¶æ–‡ä»¶ã€‚
    - **éšæœºè®¿é—®**: æ”¯æŒé«˜æ•ˆçš„éšæœºç´¢å¼•è¯»å– (Random Access)ã€‚
    - **ç»“æ„çµæ´»**: ç±»ä¼¼äºæ–‡ä»¶ç³»ç»Ÿçš„å±‚çº§ç»“æ„ã€‚
- **ç¼ºç‚¹**: ä¸é€‚åˆè¶…å¤§è§„æ¨¡æ•°æ®é›† (TB çº§åˆ«)ï¼Œéš¾ä»¥æµå¼è¯»å–ã€‚
- **é€‚ç”¨åœºæ™¯**: ä»¿çœŸç¯å¢ƒ (MuJoCo) æ•°æ®æ”¶é›†ï¼Œå°è§„æ¨¡çœŸæœºå®éªŒã€‚

### ğŸ“Š æ ¼å¼å¯¹æ¯”è¡¨

| ç‰¹æ€§ | RLDS | LeRobot | HDF5 |
| :--- | :--- | :--- | :--- |
| **èƒŒä¹¦æœºæ„** | Google DeepMind | Hugging Face | Stanford (Robomimic) |
| **æ ¸å¿ƒä¾èµ–** | TensorFlow | PyTorch / Arrow | h5py |
| **å­˜å‚¨æ ¼å¼** | TFRecord (åºåˆ—åŒ–) | Parquet (åˆ—å¼) | HDF5 (å±‚çº§) |
| **æµå¼è¯»å–** | â­â­â­ (åŸç”Ÿæ”¯æŒ) | â­â­ (æ”¯æŒ) | â­ (å›°éš¾) |
| **ç”Ÿæ€å…¼å®¹** | Open X-Embodiment | Transformers / Hub | Simulators |
| **æ¨èæŒ‡æ•°** | â­â­â­ (å¤§è§„æ¨¡/TPU) | â­â­â­ (æ–°é¡¹ç›®é¦–é€‰) | â­â­ (ç§‘ç ”/ä»¿çœŸ) |

---

## 2. ä»£ç ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½æ•°æ®

### 2.1. Loading RLDS (TensorFlow)
```python
import tensorflow_datasets as tfds

# åŠ è½½ Open X-Embodiment ä¸­çš„ fractal æ•°æ®
ds = tfds.load('fractal20220817_data', split='train')

for episode in ds.take(1):
    steps = episode['steps']
    for step in steps:
        image = step['observation']['image']
        action = step['action']
        # éœ€è¦æ‰‹åŠ¨è½¬æ¢ä¸º PyTorch Tensor å¦‚æœä¸ç”¨ TF
```

### 2.2. Loading LeRobot (PyTorch)
```python
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset

# ç›´æ¥ä» Hugging Face Hub åŠ è½½
dataset = LeRobotDataset("lerobot/pusht")

# åƒæ ‡å‡†çš„ PyTorch Dataset ä¸€æ ·ä½¿ç”¨
item = dataset[0]
image = item['observation.image']  # è‡ªåŠ¨å½’ä¸€åŒ–å¹¶è½¬ä¸º Tensor (C, H, W)
action = item['action']
print(f"Action shape: {action.shape}")
```

## 3. PyTorch å®Œæ•´è®­ç»ƒæµç¨‹ (PyTorch Training Pipeline)

åœ¨ PyTorch ä¸­è®­ç»ƒ VLA æ¨¡å‹ï¼Œæ•°æ®æµé€šå¸¸éµå¾ªä»¥ä¸‹æ¨¡å¼ï¼š`Dataset` -> `DataLoader` -> `Model`ã€‚

### 3.1. æ ¸å¿ƒç»„ä»¶
1.  **Dataset**: è´Ÿè´£è¯»å–ç£ç›˜ä¸Šçš„æ•°æ® (RLDS/Parquet)ï¼Œå¹¶è¿›è¡Œé¢„å¤„ç† (Resize, Normalize)ã€‚
2.  **Processor/Transform**: å¤„ç†å¤šæ¨¡æ€æ•°æ®ã€‚
    -   **Image**: `Resize((224, 224))`, `Normalize(mean, std)`.
    -   **Text**: Tokenizer (å¦‚ Llama Tokenizer) å°†æŒ‡ä»¤è½¬ä¸º Input IDs.
    -   **Action**: å½’ä¸€åŒ–åˆ° [-1, 1].
3.  **DataLoader**: å°†å¤šä¸ªæ ·æœ¬æ‰“åŒ…æˆ Batchã€‚éœ€è¦è‡ªå®šä¹‰ `collate_fn` æ¥å¤„ç†å˜é•¿åºåˆ— (Padding)ã€‚

### 3.2. ä»£ç å®æˆ˜ (Pseudo-code)

```python
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoProcessor

class VLADataset(Dataset):
    def __init__(self, data_path, processor):
        self.data = load_data(data_path) # e.g., LeRobotDataset
        self.processor = processor

    def __getitem__(self, idx):
        item = self.data[idx]
        
        # 1. è·å–åŸå§‹æ•°æ®
        image = item['observation.image'] # (C, H, W)
        text = item['language_instruction'] # "Pick up the apple"
        action = item['action'] # (Time, Action_Dim)
        
        # 2. å¤šæ¨¡æ€é¢„å¤„ç† (å…³é”®æ­¥éª¤!)
        # VLA æ¨¡å‹é€šå¸¸éœ€è¦åŒæ—¶è¾“å…¥å›¾åƒå’Œæ–‡æœ¬
        inputs = self.processor(
            text=text, 
            images=image, 
            return_tensors="pt", 
            padding="max_length",
            truncation=True
        )
        
        # 3. è¿”å›å­—å…¸
        return {
            "input_ids": inputs.input_ids.squeeze(),
            "pixel_values": inputs.pixel_values.squeeze(),
            "labels": action # åŠ¨ä½œä½œä¸ºç›‘ç£ä¿¡å·
        }

# 4. è®­ç»ƒå¾ªç¯
dataset = VLADataset(path, processor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
model = OpenVLAModel.from_pretrained("openvla/openvla-7b")

for batch in dataloader:
    # å°†æ•°æ®é€å…¥ GPU
    input_ids = batch["input_ids"].cuda()
    pixel_values = batch["pixel_values"].cuda()
    actions = batch["labels"].cuda()
    
    # å‰å‘ä¼ æ’­
    # VLA æ¨¡å‹é€šå¸¸è®¡ç®— Action MSE Loss æˆ– Cross-Entropy Loss (å¦‚æœæ˜¯ Tokenized Action)
    loss = model(
        input_ids=input_ids, 
        pixel_values=pixel_values, 
        labels=actions
    ).loss
    
    loss.backward()
    optimizer.step()
```

### 3.3. å¸¸è§å‘ç‚¹ (Pitfalls)
-   **æ•°æ®ç±»å‹**: ç¡®ä¿ Action æ˜¯ `float32` (å¯¹äº Diffusion/Regression) æˆ– `long` (å¯¹äº Tokenization)ã€‚
-   **å›¾åƒé€šé“**: PyTorch é»˜è®¤æ˜¯ `(C, H, W)`ï¼Œè€Œæœ‰äº›è¯»å–åº“ (å¦‚ OpenCV/PIL) å¯èƒ½æ˜¯ `(H, W, C)`ï¼ŒåŠ¡å¿…æ£€æŸ¥ `permute`ã€‚
-   **å½’ä¸€åŒ–**: åŠ¨ä½œå¿…é¡»ä½¿ç”¨**ç»Ÿè®¡æ•°æ® (Statistics)** è¿›è¡Œå½’ä¸€åŒ– (e.g., min-max æˆ– mean-std)ã€‚**æ¨ç†æ—¶å¿…é¡»ä½¿ç”¨ç›¸åŒçš„ç»Ÿè®¡æ•°æ®åå½’ä¸€åŒ–**ã€‚

---

## 4. æ•°æ®åŠ æƒä¸å¹³è¡¡ (Data Weighting & Balancing)
åœ¨è®­ç»ƒé€šç”¨ VLA æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šæ··åˆå¤šç§æ•°æ®é›†ã€‚ä¸åŒæ•°æ®é›†çš„è´¨é‡ã€è§„æ¨¡å’Œéš¾åº¦å·®å¼‚å·¨å¤§ï¼Œç›´æ¥æ··åˆè®­ç»ƒæ•ˆæœå¾€å¾€ä¸ä½³ã€‚

### å¸¸è§ç­–ç•¥
1. **æŒ‰æ•°æ®é›†è§„æ¨¡åŠ æƒ**:
    - ç®€å•çš„æŒ‰æ¯”ä¾‹é‡‡æ ·ï¼Œä½†è¿™ä¼šå¯¼è‡´å¤§è§„æ¨¡æ•°æ®é›† (é€šå¸¸æ˜¯ç®€å•çš„é‡å¤ä»»åŠ¡) ä¸»å¯¼è®­ç»ƒï¼Œæ¨¡å‹å­¦ä¸åˆ°å¤æ‚ä»»åŠ¡ã€‚
2. **æŒ‰ä»»åŠ¡éš¾åº¦åŠ æƒ**:
    - ç»™åŒ…å«å¤æ‚æ“ä½œ (e.g., ä½¿ç”¨å·¥å…·, é•¿åºåˆ—) çš„æ•°æ®é›†æ›´é«˜çš„æƒé‡ã€‚
3. **æˆåŠŸç‡è¿‡æ»¤ (Success Filtering)**:
    - ä»…ä½¿ç”¨ `is_terminal=True` ä¸” `reward=1` çš„æˆåŠŸè½¨è¿¹è¿›è¡Œ BC (Behavior Cloning) è®­ç»ƒã€‚
    - å¯¹äºå¤±è´¥è½¨è¿¹ï¼Œå¯ä»¥ç”¨äºå¯¹æ¯”å­¦ä¹  (Contrastive Learning) æˆ–ä½œä¸ºè´Ÿæ ·æœ¬ã€‚
4. **Co-training with Web Data**:
    - åœ¨è®­ç»ƒæ‰¹æ¬¡ (Batch) ä¸­ï¼Œå›ºå®šæ¯”ä¾‹ (e.g., 50%) æ··åˆ VQA (Visual Question Answering) æˆ– Captioning æ•°æ®ã€‚
    - **ç›®çš„**: ç»´æŒ VLM backbone çš„è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆåˆ°æœºå™¨äººæ•°æ®åˆ†å¸ƒä¸Š (Catastrophic Forgetting)ã€‚

## 4. æ•°æ®æ”¶é›†å·¥å…·é“¾ (Data Collection Tools)

é«˜è´¨é‡çš„æ•°æ®æºäºé«˜æ•ˆçš„æ”¶é›†å·¥å…·ã€‚

### 4.1. é¥æ“ä½œ (Teleoperation)
- **VR å¤´æ˜¾ (Vision Pro / Quest 3)**:
    - **ä¼˜åŠ¿**: æ²‰æµ¸æ„Ÿå¼ºï¼Œèƒ½æ”¶é›† 6-DoF å§¿æ€ï¼Œé€‚åˆçµå·§æ‰‹æ“ä½œã€‚
    - **æ–¹æ¡ˆ**: ALOHA (VRç‰ˆ), AnyTeleopã€‚
- **ä¸»ä»è‡‚ (Leader-Follower Arms)**:
    - **ä¼˜åŠ¿**: åŠ›åé¦ˆçœŸå®ï¼Œæ“ä½œç²¾åº¦æé«˜ã€‚
    - **æ–¹æ¡ˆ**: ALOHA (ä½¿ç”¨ WidowX ä½œä¸ºä¸»è‡‚), GELLO (ä½æˆæœ¬ 3D æ‰“å°ä¸»è‡‚)ã€‚
- **æ‰‹æŸ„/3D é¼ æ ‡**:
    - **ä¼˜åŠ¿**: æˆæœ¬ä½ï¼Œæ˜“è·å–ã€‚
    - **åŠ£åŠ¿**: éš¾ä»¥æ§åˆ¶é«˜è‡ªç”±åº¦ (å¦‚çµå·§æ‰‹)ã€‚

### 4.2. è‡ªåŠ¨åŒ–æ”¶é›† (Autonomous Collection)
- **Scripted Policy**: åœ¨ä»¿çœŸæˆ–ç®€å•åœºæ™¯ä¸­ï¼Œç”¨ç¡¬ç¼–ç è„šæœ¬ç”Ÿæˆæ•°æ®ã€‚
- **Self-Replay**: æœºå™¨äººå›æ”¾æˆåŠŸçš„è½¨è¿¹ï¼Œå¹¶æ·»åŠ å™ªå£°è¿›è¡Œæ•°æ®å¢å¼ºã€‚

## 5. åŠ¨ä½œç©ºé—´å¯¹é½ (Action Space Alignment)
ä¸åŒæœºå™¨äººçš„åŠ¨ä½œç©ºé—´ä¸åŒ (e.g., 7-DoF æœºæ¢°è‡‚ vs 14-DoF åŒè‡‚ vs å››è¶³)ã€‚

- **å½’ä¸€åŒ– (Normalization)**: å°†æ‰€æœ‰åŠ¨ä½œç»´åº¦å½’ä¸€åŒ–åˆ° [-1, 1] æˆ– [0, 1]ã€‚
- **Proprioception Padding**: å¯¹äºè‡ªç”±åº¦è¾ƒå°‘çš„æœºå™¨äººï¼Œç”¨ 0 å¡«å……å‰©ä½™ç»´åº¦ã€‚
- **ç›¸å¯¹æ§åˆ¶ vs ç»å¯¹æ§åˆ¶**:
    - **Delta Action**: é¢„æµ‹å½“å‰çŠ¶æ€çš„å¢é‡ (dx, dy, dz)ã€‚æ³›åŒ–æ€§æ›´å¥½ã€‚
    - **Absolute Action**: é¢„æµ‹ç»å¯¹åæ ‡ã€‚ç²¾åº¦æ›´é«˜ï¼Œä½†ä¾èµ–æ ‡å®šã€‚
    - **è¶‹åŠ¿**: VLA æ¨¡å‹é€šå¸¸åå‘äºä½¿ç”¨ **Delta Action (End-effector velocity/pose delta)**ã€‚

## 7. é¢è¯•é«˜é¢‘è€ƒç‚¹
1.  **æ•°æ®æ ¼å¼**: RLDS å’Œ LeRobot æ ¼å¼æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆ PyTorch ç”¨æˆ·ç°åœ¨å€¾å‘äº LeRobotï¼Ÿ(ç­”: LeRobot å»é™¤äº† TF ä¾èµ–ï¼ŒåŸç”Ÿæ”¯æŒ PyTorchï¼Œä¸”åŸºäº Parquet å­˜å‚¨æ•ˆç‡é«˜)
2.  **æ•°æ®æµ**: åœ¨ VLA è®­ç»ƒä¸­ï¼ŒProcessor çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ(ç­”: åŒæ—¶å¤„ç†å›¾åƒå½’ä¸€åŒ–å’Œæ–‡æœ¬ Tokenizationï¼Œç¡®ä¿å¤šæ¨¡æ€å¯¹é½)
3.  **æ•°æ®å¹³è¡¡**: å¦‚æœæˆ‘æœ‰ 1000 æ¡ç®€å•çš„ Pick-Place æ•°æ®å’Œ 100 æ¡å¤æ‚çš„ Assembly æ•°æ®ï¼Œåº”è¯¥æ€ä¹ˆè®­ç»ƒï¼Ÿ(ç­”: é‡é‡‡æ · Assembly æ•°æ®ï¼Œæé«˜å…¶åœ¨ Batch ä¸­çš„æ¯”ä¾‹)
4.  **Action Space**: ä¸ºä»€ä¹ˆè¦ç”¨ Delta Actionï¼Ÿ(ç­”: å‡å°‘å¯¹ç»å¯¹åæ ‡çš„ä¾èµ–ï¼Œæ›´å®¹æ˜“è¿ç§»åˆ°ä¸åŒä½ç½®æˆ–ä¸åŒæœºå™¨äºº)
5.  **æ•°æ®æ”¶é›†**: ç›¸æ¯”äº VR é¥æ“ä½œï¼Œä¸»ä»è‡‚ (Leader-Follower) æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ(ç­”: ä¸»ä»è‡‚æœ‰åŠ›åé¦ˆï¼Œç²¾åº¦é«˜ï¼Œä½†æˆæœ¬é«˜ä¸”ä¸ä»…é™äºå¼‚æ„æœºå™¨äººæ˜ å°„)


---
[â† Back to Theory](./README.md)
