# VLA 十大挑战 (10 Open Challenges)

> **来源**: "10 Open Challenges Steering the Future of Vision-Language-Action Models" (NTU + Stanford, 2024)
> [[论文](https://arxiv.org/pdf/2511.05936)]

## 概览

如果说 VLA 的规模化应用是一场跨洋航行，这十座"冰山"是必须正面强攻的核心挑战：

```
┌─────────────────────────────────────────────────────────────────┐
│                    VLA 十大挑战全景图                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   感知层                                                        │
│   ├── 1. 多模态感知: RGB-only → 深度/触觉/听觉                  │
│   └── 2. 稳健推理: VLM 推理 ≠ VLA 执行                          │
│                                                                 │
│   数据层                                                        │
│   ├── 3. 高质量数据: 不够/不干净/太贵                           │
│   └── 4. 可靠评估: Benchmark 太简单                             │
│                                                                 │
│   执行层                                                        │
│   ├── 5. 跨机器人泛化: 动作空间异构                             │
│   ├── 6. 资源效率: 机器人算力弱                                 │
│   └── 7. 全身协调: 移动+操作耦合                                │
│                                                                 │
│   部署层                                                        │
│   ├── 8. 安全性: 物理世界风险放大                               │
│   ├── 9. Agentic 框架: 多机器人协作                             │
│   └── 10. 人机协作: 单向 → 双向交互                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 挑战 1: 多模态感知 (Multimodal Perception)

> **核心问题**: 看得见、听得着，却读不懂 3D 空间、扛不住环境噪声。

### 1.1 现状

绝大多数 VLA 仍然是 **"RGB-only"** 的二维世界生物：

| 缺失模态 | 影响 | 代表方案 |
| :--- | :--- | :--- |
| **深度** | 物体远近难把握，小尺度操作不稳定 | SpatialVLA, MolmoAct (深度监督) |
| **触觉** | 无法完成精细抓取、料理、装配 | Tactile-VLA, VLA-Touch |
| **听觉** | 无法定位求救者、识别危险源 | 尚无成熟方案 |

### 1.2 环境噪声问题

```
┌─────────────────────────────────────────────────────────────────┐
│                    感知噪声来源                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   光学干扰          环境因素           材质问题                  │
│   ├── 反光          ├── 光照变化        ├── 透明物体             │
│   ├── 镜头眩光      ├── 水汽            ├── 镜面反射             │
│   └── 阴影          └── 灰尘/碎片       └── 柔体变形             │
│                                                                 │
│   💡 每一项都给 VLA 的多模态感知带来严峻考验                     │
└─────────────────────────────────────────────────────────────────┘
```

### 1.3 解决方向

- **RGB-D 融合**: 引入深度传感器 (RealSense, LIDAR)
- **多视角**: 多摄像头协同感知
- **触觉集成**: GelSight, DIGIT 等触觉传感器
- **鲁棒性训练**: Domain Randomization, 噪声增强

---

## 挑战 2: 稳健推理 (Robust Reasoning)

> **核心问题**: VLM 很聪明，但 VLA 动作仍然很蠢。

### 2.1 关键矛盾

```
VLA 的推理任务比 LLM 简单得多，但结果却远不如 LLM 稳定。
```

**原因**: VLA 不只是"想得对"，还要"做得对"。

```
┌─────────────────────────────────────────────────────────────────┐
│              VLM 推理 → VLA 执行的误差累积                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   VLM 推理         执行层误差           物理世界约束             │
│   (正确)           (累积)               (放大)                  │
│      │                │                    │                    │
│      ▼                ▼                    ▼                    │
│   "夹取杯子"  →  量化误差/精度偏差  →  夹歪/夹空                │
│   "打开抽屉"  →  感知偏移/噪声      →  开不到位                 │
│   "长序列"    →  误差指数级累积     →  成功率骤降               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 当前方案

| 方法 | 思路 | 代表工作 |
| :--- | :--- | :--- |
| **Chain-of-Thought** | 先推理，再执行 | CoT-VLA, Emma-X |
| **空间推理增强** | 显式建模 3D 关系 | MolmoAct, SpatialVLA |
| **可验证 RL (RLVR)** | 验证推理过程正确性 | π0.6 |

### 2.3 面试 Q&A

**Q: 为什么 VLM 的推理能力在 VLA 中削弱？**

A:
1. **执行误差**: 离散动作量化、连续动作精度偏差
2. **感知偏移**: 环境噪声导致状态估计不准
3. **误差累积**: 长序列任务中误差指数级放大
4. **物理约束**: 控制动力学限制无法在纯推理中建模

---

## 挑战 3: 高质量训练数据 (High-Quality Training Data)

> **核心问题**: 数据很大，但仍然远远不够。

### 3.1 Open-X-Embodiment 的贡献与不足

```
┌─────────────────────────────────────────────────────────────────┐
│              Open-X-Embodiment 数据集                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ✅ 贡献                        ❌ 不足                        │
│   ├── 百万级轨迹                 ├── 机器人形态不同              │
│   ├── 21 个机构                  │   (action space 不兼容)       │
│   └── 开放获取                   ├── 摄像机位置不同              │
│                                  │   (感知风格变化)              │
│                                  ├── 操作者不同                  │
│                                  │   (演示风格混乱)              │
│                                  └── 数据不干净/不稳定           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 真实数据 vs 仿真数据

| 类型 | 优点 | 缺点 |
| :--- | :--- | :--- |
| **真实数据** | 真实、可靠 | 太贵、太慢 |
| **Sim2Real** | 便宜、可扩展 | 光照/材质/摩擦难模拟 |

### 3.3 解决方向

- **数据增强**: 形态对称、视角变换 (GR-RL)
- **Critic 筛选**: 剔除低质量演示 (π0.6, GR-RL)
- **视频生成**: 用生成模型合成训练轨迹
- **世界模型**: 在虚拟环境中生成无限数据

---

## 挑战 4: 可靠评估 (Reliable Evaluation)

> **核心问题**: Benchmark 环境太简单、太固定、太干净。

### 4.1 当前 Benchmark 的局限

```
┌─────────────────────────────────────────────────────────────────┐
│              Simulation vs Reality Gap                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   仿真环境 (SimplerEnv 等)       真实世界                        │
│   ├── 光照扰动 ✅                ├── 复杂光照变化                │
│   ├── 纹理变化 ✅                ├── 未见材质/物体               │
│   ├── 背景切换 ✅                ├── 动态障碍物                  │
│   └── 相机随机性 ✅              ├── 机器人基座不稳定            │
│                                  ├── 关节阻尼差异                │
│                                  ├── 夹爪摩擦系数                │
│                                  └── 柔体/流体交互              │
│                                                                 │
│   💡 真实与模拟的对应性仍然太弱                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 解决方向

- **多样化测试集**: 覆盖更多场景、物体、光照
- **真机验证**: 建立标准化真机测试流程
- **对抗测试**: 故意引入干扰验证鲁棒性

---

## 挑战 5: 跨机器人泛化 (Cross-Robot Generalization)

> **核心问题**: 同一意图，不同机器人需要完全不同的动作指令。

### 5.1 动作空间异构

```
┌─────────────────────────────────────────────────────────────────┐
│              不同机器人的动作表征                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   机械臂                                                        │
│   └── 3D 位置 (x/y/z) + 3D 姿态 (roll/pitch/yaw)               │
│                                                                 │
│   四足机器人                                                    │
│   └── 腿部关节轨迹 (12+ DoF)                                    │
│                                                                 │
│   移动基座                                                      │
│   └── 速度指令 (v_x, v_y, ω)                                   │
│                                                                 │
│   人形机器人                                                    │
│   └── 全身关节控制 (30+ DoF)                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 当前方案

| 方法 | 思路 | 效果 |
| :--- | :--- | :--- |
| **Codebook + Decoder** | 学习通用离散动作，再加 robot-specific decoder | 需要微调 |
| **动作分词** | FAST 的 DCT+BPE 跨机器人压缩 | 部分通用 |
| **上下文学习** | 通过 prompt 描述机器人特性 | 探索中 |

### 5.3 面试 Q&A

**Q: 如何实现"零样本换机器人"？**

A: 当前无法完全实现，最佳实践是：
1. **通用动作表示**: 学习与机器人无关的"原子动作"
2. **轻量微调**: robot-specific decoder 快速适配
3. **上下文提示**: 借鉴 LLM 的 in-context learning

---

## 挑战 6: 资源效率 (Resource Efficiency)

> **核心问题**: 机器人本体算力太弱，VLA 太大。

### 6.1 现状

```
┌─────────────────────────────────────────────────────────────────┐
│              VLA 部署的算力困境                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   VLA 模型规模                    机器人端算力                   │
│   ├── OpenVLA: 7B                 ├── Jetson Orin: ~50 TOPS     │
│   ├── π0: 3B                      ├── Raspberry Pi: ~13 TOPS    │
│   ├── GR-3: 40B                   └── 边缘 GPU: 有限             │
│   └── GR-RL: 50B                                                │
│                                                                 │
│   当前方案: Thin Client (云端推理)                              │
│   ├── 优点: 可用大模型                                          │
│   └── 缺点: 延迟高，网络断开即失能                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 6.2 解决方向

| 方法 | 思路 | 代表工作 |
| :--- | :--- | :--- |
| **模型压缩** | 量化、剪枝 | AWQ, GPTQ |
| **知识蒸馏** | 大模型 → 小模型 | TinyVLA |
| **高效架构** | 设计轻量 VLA | SmolVLA (0.5B) |
| **混合部署** | 本地小模型 + 云端大模型 | 按需切换 |

---

## 挑战 7: 全身协调 (Whole-Body Coordination)

> **核心问题**: VLA 如何同时控制移动与操作？

### 7.1 从桌面操作到全身控制

```
┌─────────────────────────────────────────────────────────────────┐
│              任务复杂度演进                                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Level 1: 固定基座操作                                         │
│   └── 桌面 Pick-and-Place                                       │
│                                                                 │
│   Level 2: 移动 + 操作                                          │
│   └── 移动到目标位置，执行操作                                  │
│                                                                 │
│   Level 3: 全身协调                                             │
│   └── 移动、操作、平衡同时进行 (π0 系列)                        │
│                                                                 │
│   💡 移动控制 + 操作控制 = 高度耦合的复杂控制问题               │
└─────────────────────────────────────────────────────────────────┘
```

### 7.2 两条技术路线

| 路线 | 优点 | 缺点 |
| :--- | :--- | :--- |
| **MPC (模型预测控制)** | 靠谱、有安全保障 | 需要精确模型、耗算力 |
| **数据驱动 (VLA/RL)** | 灵活、端到端 | 不稳定、难迁移 |

### 7.3 未来趋势: 混合控制

```
Hybrid Control = MPC (安全界限) + VLA/RL (策略探索)
```

---

## 挑战 8: 安全性 (Safety)

> **核心问题**: VLA 是"可动的 GPT"，风险比 LLM 大得多。

### 8.1 风险放大

```
┌─────────────────────────────────────────────────────────────────┐
│              LLM 幻觉 vs VLA 幻觉                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   LLM 幻觉                        VLA 幻觉                       │
│   └── 错误文字                    └── 错误动作                   │
│       (可回退)                        (可能造成物理伤害)         │
│                                                                 │
│   案例:                                                         │
│   • 人形机器人在楼梯摔倒 (不锈钢墙面反光误判)                   │
│   • 机械臂夹取时碰撞 (深度估计错误)                             │
│   • 移动机器人撞人 (动态障碍物未检测)                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.2 SafeVLA 方案

```
┌─────────────────────────────────────────────────────────────────┐
│              SafeVLA: 集成安全方法 (ISA)                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   1. 安全约束建模                                               │
│      └── 定义不可违反的物理安全边界                             │
│                                                                 │
│   2. RL-based Safety Alignment                                  │
│      └── 使用约束 RL 学习安全策略                               │
│                                                                 │
│   3. 运行时监控                                                 │
│      └── 实时检测危险动作并干预                                 │
│                                                                 │
│   💡 Safety 不是附加的，是 VLA 部署前必须集成的核心模块         │
└─────────────────────────────────────────────────────────────────┘
```

---

## 挑战 9: Agentic 框架 (Multi-Agent Systems)

> **核心问题**: 未来不是"一台机器人"，而是"一群机器人 + 一群模型"协作。

### 9.1 VLA 在多智能体中的角色

```
┌─────────────────────────────────────────────────────────────────┐
│              多智能体协作架构                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   高层规划 (LLM/VLM)                                            │
│   └── 任务分解、资源调度、全局规划                              │
│                 │                                               │
│                 ▼                                               │
│   中层协调 (Agent Framework)                                    │
│   └── 多机器人通信、视角互补、行为协作                          │
│                 │                                               │
│                 ▼                                               │
│   底层执行 (VLA)                                                │
│   └── 接收指令，执行具体动作                                    │
│                                                                 │
│   💡 VLA 未来更像"手脚"，而不是"大脑"                          │
└─────────────────────────────────────────────────────────────────┘
```

### 9.2 关键能力

- **视角补充**: 多机器人从不同角度感知
- **任务分解**: 复杂任务拆分给多个机器人
- **分布式执行**: 并行完成子任务
- **通信协调**: Agent 之间的语言/信号交互

---

## 挑战 10: 人机协作 (Human-Robot Collaboration)

> **核心问题**: VLA 必须学会"和人说话"。

### 10.1 从单向到双向

```
┌─────────────────────────────────────────────────────────────────┐
│              人机交互演进                                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   当前: 单向通道                                                │
│   └── 人 → 机器人 (指令)                                        │
│                                                                 │
│   未来: 双向交互                                                │
│   ├── 机器人解释"为什么这么做"                                  │
│   ├── 机器人反问不确定步骤                                      │
│   ├── 机器人可视化意图 (CoT-VLA)                                │
│   └── 机器人请求人类帮助                                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 10.2 CoT-VLA 的可解释性

CoT-VLA 通过可视化推理链，让人类理解机器人的决策过程：

```
输入: "把红色杯子放到蓝色盘子上"

CoT 输出:
1. 识别场景中的物体: 红色杯子(左), 蓝色盘子(右)
2. 规划路径: 移动到杯子 → 抓取 → 移动到盘子 → 放置
3. 当前步骤: 移动末端到杯子位置
4. 置信度: 0.95
```

---

## 六大突破方向

论文提出的未来 VLA 技术底座：

```
┌─────────────────────────────────────────────────────────────────┐
│                  VLA 下一代六大突破方向                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   1. 高层/低层分离                                              │
│      └── LLM/VLM 负责规划，VLA 专注执行                         │
│                                                                 │
│   2. 强化 3D 空间理解                                           │
│      └── RGB-D, LIDAR, 多视角融合                               │
│                                                                 │
│   3. 通用动作表示                                               │
│      └── 跨机器人共享的"原子动作"                               │
│                                                                 │
│   4. 世界模型                                                   │
│      └── 预测动作后果，先推演再执行                             │
│                                                                 │
│   5. 视频生成                                                   │
│      └── 合成大规模训练轨迹                                     │
│                                                                 │
│   6. 后训练优化                                                 │
│      └── RL, DPO, 偏好对齐持续提升                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 面试高频 Q&A

**Q1: VLA 当前最大的瓶颈是什么？**

A: 综合来看，**跨机器人泛化** 和 **资源效率** 是最关键的瓶颈：
- 泛化: 无法"零样本换机器人"
- 效率: 大模型无法在机器人本地运行

**Q2: VLA 和 LLM 的核心区别是什么？**

A:
- LLM: 输入文本 → 输出文本 (纯数字世界)
- VLA: 输入多模态 → 输出动作 → **物理执行** (需面对物理约束)
- 关键区别: VLA 的错误会在物理世界造成真实后果

**Q3: 如何看待 VLA 的下一个十年？**

A: 论文观点是"没有中间选项"：
- **乐观**: 啃下十大挑战，定义具身智能黄金时代
- **悲观**: 卡在瓶颈，被更落地的技术淘汰

**Q4: VLA 未来的角色是"大脑"还是"手脚"？**

A: 更像"手脚"。高层规划由 LLM/VLM 负责，VLA 作为 executor 执行具体动作。

---

## 参考资源

- **论文**: [10 Open Challenges Steering the Future of Vision-Language-Action Models](https://arxiv.org/pdf/2511.05936)
- **SafeVLA**: [Towards Safety Alignment of VLA via Constrained Learning](https://arxiv.org/abs/2412.09010)
- **CoT-VLA**: [Visual Chain-of-Thought Reasoning for VLA Models](https://arxiv.org/abs/2403.08540)

---
[← Back to Theory](./README.md)


