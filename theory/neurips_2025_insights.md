# NeurIPS 2025 最佳论文：具身智能视角解读

> **来源**: 深蓝具身智能 · 顶会盘点
> **会议**: NeurIPS 2025 (论文接收率 24.52%，五年最低；投稿量 21575 篇，历史新高)
> **视角**: 从具身智能立场解读通用 AI 研究的底层机制

---

## 概述

NeurIPS 作为机器学习领域的顶级会议，每年评选的最佳论文常被看作 AI 研究的风向标。本文精选 6 篇 NeurIPS 2025 获奖论文（Best Paper & Runner-up），从具身智能视角进行解读。

**核心问题**：这些通用 AI 研究如何回应具身智能的根本挑战？

```
┌─────────────────────────────────────────────────────────────────┐
│                    NeurIPS 2025 获奖论文概览                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   论文 1: Artificial Hivemind (华盛顿大学)                       │
│   └── 语言模型同质化问题 → 机器人需要行为多样性                   │
│                                                                 │
│   论文 2: Gated Attention (阿里千问)                             │
│   └── 门控注意力机制 → 稀疏性+稳定性，适合边缘部署               │
│                                                                 │
│   论文 3: 1000 Layer Networks (普林斯顿)                         │
│   └── 深层自监督 RL → 无奖励无示范的目标达成                     │
│                                                                 │
│   论文 4: Diffusion Memorization (巴黎 PSL)                      │
│   └── 扩散模型泛化机制 → 训练节奏控制防过拟合                    │
│                                                                 │
│   论文 5: Superposition Scaling (MIT)                            │
│   └── 表示叠加理论 → 多技能统一模型基础                          │
│                                                                 │
│   论文 6: RL Reasoning in LLMs (清华)                            │
│   └── RLVR 局限性 → RL 未必能扩展推理能力                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 1. Artificial Hivemind: 语言模型的同质化问题

> **研究团队**: 华盛顿大学等
> **论文链接**: [OpenReview](https://openreview.net/pdf?id=saDOrrnNTz)

### 1.1 核心发现

本文揭示了一个被忽视的问题：在语言模型不断发展的今天，我们是否正在走向一个"思想同质化"的未来？

**"Artificial Hivemind（人工蜂群思维）"现象**：
- **Intra-model repetition**: 同一个模型常常给出类似的回答
- **Inter-model homogeneity**: 不同模型给出的答案也趋于一致

### 1.2 INFINITY-CHAT 基准

团队构建了 INFINITY-CHAT 大规模开放问答基准集：
- 2.6 万条真实用户提问
- 超过 3 万条人类评分
- 用于评估大模型在开放式生成中的多样性与创造性

**实验发现**：25 个语言模型围绕"写一句关于时间的比喻"生成的回答，尽管模型结构与大小不同，但输出却高度趋同，仅集中在"时间是条河流"与"时间是织布者"两个语义簇。

### 1.3 对具身智能的启示

```
┌─────────────────────────────────────────────────────────────────┐
│              模型同质化对机器人的影响                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   问题:                                                         │
│   ├── 一条指令可能有多种理解方式、多个操作路径                    │
│   ├── 不同语境下需要不同的响应方式                                │
│   └── 具身系统需要"差异性"以适应开放世界                         │
│                                                                 │
│   风险:                                                         │
│   ├── 若模型陷入"蜂群思维"                                      │
│   ├── 可能永远只给出最保守的那条路                                │
│   └── 失去探索性与适应性，影响人机协作的自然性                    │
│                                                                 │
│   启示:                                                         │
│   └── 不能只关注模型的平均表现，还必须关心其多样性与创造性        │
│       是否足够支持复杂环境下的自主行为与个性化交互                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 1.4 面试 Q&A

**Q: 为什么机器人系统需要"行为多样性"？**

A:
1. **适应性**: 同一任务在不同环境下可能需要不同策略
2. **鲁棒性**: 单一策略失败时需要备选方案
3. **探索性**: 发现更优解需要多样化尝试
4. **自然性**: 人机交互中避免机械化重复

---

## 2. Gated Attention: 门控注意力机制

> **研究团队**: 阿里巴巴千问等
> **论文链接**: [OpenReview](https://openreview.net/forum?id=1b7whO4SfY)

### 2.1 核心技术

在构建大型语言模型的注意力机制时，团队重新审视了"门控（Gating）"这一经典技巧的潜力。

**关键改进**：在注意力输出后引入基于每个头的 sigmoid 门控函数。

```python
# 门控注意力机制伪代码
class GatedAttention(nn.Module):
    def forward(self, x):
        # 标准注意力计算
        attn_output = self.attention(x)
        
        # 门控机制 (关键改进)
        gate = torch.sigmoid(self.gate_proj(x))  # 每个头独立门控
        
        # 门控输出
        output = attn_output * gate
        return output
```

**效果**：
- 有效缓解"注意力下沉"（attention sink）问题
- 避免模型过度激活某些 token
- 提升整体信息利用效率

### 2.2 对具身智能的启示

```
┌─────────────────────────────────────────────────────────────────┐
│              门控注意力对 VLA 部署的价值                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   边缘部署场景:                                                  │
│   ├── 资源受限 (Jetson, 边缘 GPU)                               │
│   ├── 低延迟需求 (实时交互、在线导航)                            │
│   └── Transformer 计算开销是瓶颈                                 │
│                                                                 │
│   门控机制优势:                                                  │
│   ├── 稀疏性: 降低能耗                                          │
│   ├── 非线性调控: 提升关键语义 token 响应能力                    │
│   └── 训练稳定性: 支持更高学习率，波动更小                       │
│                                                                 │
│   应用方向:                                                      │
│   └── 多模态具身模型 (VLM + 视觉控制) 中的 cross-attention 层    │
│       提供高效、可调、可扩展的替代方案                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.3 面试 Q&A

**Q: 门控注意力如何帮助 VLA 边缘部署？**

A:
1. **稀疏激活**: 不需要的注意力头被抑制，减少计算
2. **训练稳定**: 更稳定的训练过程意味着更可靠的模型
3. **关键信息聚焦**: 对机器人场景中的关键物体/指令更敏感
4. **低延迟**: 适合实时控制需求

---

## 3. 1000 Layer Networks: 深层自监督 RL

> **研究团队**: 普林斯顿大学等
> **论文链接**: [OpenReview](https://openreview.net/forum?id=s0JVsx3bx1)

### 3.1 核心发现

自监督学习在语言与视觉领域"一路狂飞"，但在强化学习（RL）中，类似的规模化突破迟迟未见。本文大胆提出：**也许我们只是"网太浅了"**。

**实验设计**：
- 完全无监督、无奖励、无示范的环境
- 智能体从零开始学会达成目标指令（goal-reaching）
- 网络层数从传统的 2~5 层扩展到 **1024 层**

**实验结果**：
- 网络越深，智能体的探索效率与任务成功率越高
- 甚至学会了更多样、更稳定的行为模式
- 在模拟行走与操控任务中，深层网络显著超越其他目标导向的自监督方法

### 3.2 技术细节

```
┌─────────────────────────────────────────────────────────────────┐
│              深层自监督 RL 的关键设计                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   训练目标:                                                      │
│   └── Goal-Reaching: 从当前状态到达目标状态                      │
│                                                                 │
│   自监督信号:                                                    │
│   ├── 无外部奖励                                                │
│   ├── 无人类示范                                                │
│   └── 仅通过状态间的关系学习                                    │
│                                                                 │
│   网络深度:                                                      │
│   ├── 传统: 2-5 层                                              │
│   ├── 本文: 64-1024 层                                          │
│   └── 关键: 需要特殊的残差连接和归一化技术                       │
│                                                                 │
│   发现:                                                         │
│   └── 深度与性能呈正相关，直到 ~1000 层仍有提升                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.3 对具身智能的启示

**实际机器人应用场景**：
- 如何从无先验知识中自主摸索出"开门"
- "绕障碍物去拿水壶"等复杂行为

**传统浅层模型的局限**：力不从心

**本文证明**：足够深的模型，即使没有奖励和示范，也能逐步形成复杂目标行为。

**未来启示**：
- 越来越多机器人系统引入自监督探索（如模仿微动作、自动理解目标语义）
- "深网络 + 自监督"组合可能成为下一代具身智能的学习基石
- 未来的机器人学习框架，也许不能再依赖"轻量+浅层"的范式

### 3.4 面试 Q&A

**Q: 为什么深层网络能在无奖励环境中学习复杂行为？**

A:
1. **表示能力**: 深层网络能学习更复杂的状态-目标映射
2. **层次抽象**: 不同层学习不同层次的行为原语
3. **探索效率**: 更好的特征表示带来更有效的探索
4. **泛化能力**: 深层特征更具迁移性

---

## 4. Diffusion Models 的泛化机制

> **研究团队**: 巴黎 PSL 大学
> **论文链接**: [OpenReview](https://openreview.net/pdf?id=BSZqpqgqM0)

### 4.1 核心发现

扩散模型一个常见现象：即便模型参数巨大，它们却不像其他模型那样容易"背住"训练集，反而拥有较强的泛化能力。本文深入研究了这一现象背后的原因。

**关键发现**：

1. **两个关键时间点**：
   - 前期：模型就能生成高质量样本
   - 后期：继续训练才会开始出现对训练集的记忆（"过拟合"）

2. **数据集规模效应**：训练集越大，模型能保持"泛化状态"的时间也越长

3. **隐式动态正则化**：训练过程中存在一种隐式机制，使得扩散模型即便在极高容量下也能有效避免过拟合

### 4.2 技术细节

```
┌─────────────────────────────────────────────────────────────────┐
│              扩散模型的"延迟过拟合"机制                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   训练过程:                                                      │
│                                                                 │
│   Step 0        Step T_gen       Step T_mem        Step ∞       │
│     │              │                │                │          │
│     ▼              ▼                ▼                ▼          │
│   [初始化]     [泛化区间]       [记忆开始]        [过拟合]      │
│                    │                │                           │
│                    └── 黄金训练区间 ──┘                         │
│                                                                 │
│   关键洞察:                                                      │
│   ├── T_gen: 模型开始生成高质量样本的时刻                        │
│   ├── T_mem: 模型开始记忆训练集的时刻                            │
│   └── 黄金区间 [T_gen, T_mem]: 最佳停止点                        │
│                                                                 │
│   数据集规模效应:                                                │
│   └── 数据越多 → T_mem 越晚 → 泛化区间越长                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.3 对具身智能的启示

**"如何训练一个既强大又不过拟合的模型"？**

本研究给了一个很好的思路：

1. **视觉模型训练**：机器人感知模型（环境理解、视觉生成、3D重建）需要处理高维数据、适应多变环境

2. **训练节奏控制**：把握训练节奏、控制训练时长，扩散模型可以天然获得较强的泛化能力

3. **策略模型借鉴**：这种"训练过程中自动形成泛化区间"的机制，可能被借鉴到强化学习中的策略模型或世界模型训练

### 4.4 面试 Q&A

**Q: 扩散模型的泛化机制对 VLA 训练有什么启示？**

A:
1. **早停策略**: 不是训练越久越好，存在最优停止点
2. **数据规模**: 更大的数据集允许更长的训练时间
3. **监控指标**: 需要同时监控生成质量和记忆程度
4. **迁移应用**: 可将类似机制引入策略学习

---

## 5. Superposition: 表示叠加与神经缩放

> **研究团队**: 麻省理工学院 (MIT)
> **论文链接**: [OpenReview](https://openreview.net/pdf?id=knPz7gtjPW)

### 5.1 核心发现

模型越大，效果越好——这个被称为"神经网络缩放法则（Neural Scaling Law）"的现象，在多个任务上都屡试不爽。但它的根源到底是什么？

**本文新解释**：**表示叠加（Superposition）** 能力才是模型变强的核心原因。

**表示叠加**：大模型可以在有限的维度中"塞下"远多于维度数量的语义特征，通过几何空间中的"重叠"实现信息编码。

```
┌─────────────────────────────────────────────────────────────────┐
│              表示叠加 (Superposition) 原理                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   无叠加 (传统):                                                 │
│   ┌───────────────────┐                                         │
│   │ 3维空间 → 3个特征 │  每个特征占用一个独立维度                │
│   │   f1 ─┐           │                                         │
│   │   f2 ─┼─ 正交     │                                         │
│   │   f3 ─┘           │                                         │
│   └───────────────────┘                                         │
│                                                                 │
│   有叠加 (本文):                                                 │
│   ┌───────────────────┐                                         │
│   │ 3维空间 → N个特征 │  特征在空间中"重叠"但可区分              │
│   │   f1 ──╲          │                                         │
│   │   f2 ───●── 叠加  │  N >> 3，但信息仍可解码                  │
│   │   f3 ──╱          │                                         │
│   │   ...             │                                         │
│   └───────────────────┘                                         │
│                                                                 │
│   效果: 模型规模 ↑ → 叠加能力 ↑ → 损失 ↓                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 对具身智能的启示

**1. 多技能统一模型的基础**

强叠加能力可能是构建"多技能统一模型"的关键机制。未来的具身系统若希望一个模型同时理解语言、解析图像、控制动作，靠的正是这种"在有限空间中高效编码多模态特征"的能力。

**2. 技能迁移的理论基础**

比如，机器人学会"抓瓶子"后，如果能在同一嵌入空间中表示"拿水杯"，便能快速泛化到新任务。

**3. 规模化前景**

这种叠加式表示方式，比传统"每个任务一套专用模型"的思路更具规模化前景。

### 5.3 面试 Q&A

**Q: 表示叠加对 VLA 的多任务学习有什么意义？**

A:
1. **参数效率**: 一个模型可以编码更多技能
2. **知识共享**: 相似技能在嵌入空间中接近，便于迁移
3. **泛化能力**: 叠加表示具有更好的组合泛化性
4. **可扩展性**: 理论上支持持续学习新技能而不遗忘旧技能

---

## 6. RLVR 的局限性：RL 真的能扩展推理能力吗？

> **研究团队**: 清华大学等
> **论文链接**: [OpenReview](https://openreview.net/forum?id=4OsgYD7em5)

### 6.1 核心发现

随着 RLVR（带可验证奖励的强化学习）在数学、编程等任务中的显著表现，一度被认为能像训练智能体那样"激励"模型不断自我提升，从而学会新的推理能力。

**但这篇论文给这个主流观点"泼了一盆冷水"**。

**关键发现**：

1. **推理路径未变**: RLVR 训练出的模型，其思维路径早就在原始基础模型的输出空间中，只是被"筛"得更准了而已

2. **范围变窄**: 随着 RLVR 训练深入，模型的推理范围反而变窄了，限制了探索多样性的可能

3. **本质**: RLVR 虽然能提高采样效率，让模型更快"说对话"，但并没有产生真正新的推理能力

### 6.2 技术细节

```
┌─────────────────────────────────────────────────────────────────┐
│              RLVR 的"筛选"本质                                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   训练前 (Base Model):                                          │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  问题 Q → 多条推理路径                                   │   │
│   │          ├── 路径 A (正确) ✓                            │   │
│   │          ├── 路径 B (正确) ✓                            │   │
│   │          ├── 路径 C (错误) ✗                            │   │
│   │          └── 路径 D (错误) ✗                            │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│   训练后 (RLVR Model):                                          │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  问题 Q → 更集中的推理路径                               │   │
│   │          ├── 路径 A (正确) ✓✓✓  ← 被强化                │   │
│   │          ├── 路径 B (正确) ✓    ← 被忽略                │   │
│   │          ├── 路径 C (错误)      ← 被抑制                │   │
│   │          └── 路径 D (错误)      ← 被抑制                │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│   问题:                                                         │
│   ├── 没有产生新的推理路径                                     │
│   ├── 多样性降低 (路径 B 也被边缘化)                           │
│   └── 能解决的问题范围变窄                                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 6.3 对具身智能的启示

如果我们将大语言模型作为机器人的"认知大脑"，依赖强化学习去教它如何规划、理解和决策，就必须意识到 **RL 可能并不具备"开智"的能力**。

**现实场景的需求**：
- 机器人面对复杂指令、模糊环境或跨模态信息
- 需要的是真正的推理与泛化能力

**警示**：
- 如果 RLVR 只是在基础模型的"舒适区"里打转
- 它就无法帮助机器人跳出已知，走向更强的认知行为

**替代方向**：
- 蒸馏 (Distillation)
- 多回合交互
- 任务引导式训练
- 帮助机器人构建更具想象力与适应力的内在模型

### 6.4 面试 Q&A

**Q: RLVR 的局限性对 VLA 训练策略有什么警示？**

A:
1. **不能过度依赖 RL**: RL 优化的是采样效率，不是能力边界
2. **基础模型很重要**: 如果基础模型没有某种推理能力，RL 也教不会
3. **多样性需要保护**: RL 可能会压缩策略空间，降低探索能力
4. **组合使用**: 考虑 RL + 蒸馏 + 多任务训练的组合策略

---

## 7. 未来发展方向 (Future Directions)

基于以上 6 篇论文的洞察，我们可以勾勒出具身智能的几个关键研究方向：

### 7.1 技术趋势

```
┌─────────────────────────────────────────────────────────────────┐
│                    技术发展趋势                                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   1. 深层自监督学习                                              │
│   ├── 从 1000 层网络看：深度仍有红利                            │
│   ├── 下一代机器人学习可能不再是"轻量+浅层"                     │
│   └── 自监督 + 目标达成 = 无需人工奖励设计                      │
│                                                                 │
│   2. 稀疏门控架构                                                │
│   ├── 门控注意力 → 边缘部署的架构革新                           │
│   ├── 稀疏激活 → 降低能耗，提高效率                             │
│   └── 训练稳定性 → 更可靠的部署                                 │
│                                                                 │
│   3. 表示叠加与统一模型                                          │
│   ├── 多技能在同一嵌入空间共存                                  │
│   ├── 技能迁移的理论基础                                        │
│   └── 持续学习的可能性                                          │
│                                                                 │
│   4. 训练策略革新                                                │
│   ├── 扩散模型启示：把握"泛化区间"                              │
│   ├── 早停 vs 持续训练的权衡                                    │
│   └── 数据规模与训练时长的关系                                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.2 待解决问题

| 问题 | 来源论文 | 挑战 |
|:---|:---|:---|
| **多样性 vs 一致性** | Artificial Hivemind | 如何在保持一致性的同时鼓励探索？ |
| **RL 的边界** | RL Reasoning | 何时用 RL？何时不用？ |
| **深度的代价** | 1000 Layer | 深层网络的计算和内存开销如何优化？ |
| **泛化区间检测** | Diffusion Memorization | 如何自动检测最佳停止点？ |
| **叠加的可控性** | Superposition | 如何控制哪些技能被叠加、哪些被隔离？ |

### 7.3 下一个突破口预测

```
┌─────────────────────────────────────────────────────────────────┐
│                    突破口预测                                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   短期 (1-2 年):                                                 │
│   ├── 门控注意力在 VLA 边缘部署中普及                           │
│   ├── 深层自监督预训练成为标配                                  │
│   └── 训练早停策略被系统研究                                    │
│                                                                 │
│   中期 (2-3 年):                                                 │
│   ├── 世界模型 + 自监督 RL 的融合                               │
│   ├── 表示叠加用于多技能统一模型                                │
│   └── 轻量化 VLA 实现真正的端侧部署                             │
│                                                                 │
│   长期 (3-5 年):                                                 │
│   ├── 具身智能的"GPT 时刻"：通用机器人基础模型                  │
│   ├── 自主探索 + 持续学习成为现实                               │
│   └── 从仿真到真机的无缝迁移                                    │
│                                                                 │
│   💡 核心洞察: 通用 AI 的基础研究正在为具身智能铺路              │
│      这些看似"理论"的工作，将在 2-3 年内影响实际系统设计        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 总结：从顶会看具身智能的现在与未来

从超深强化学习网络中涌现出的目标策略，到关注大模型多样性、稳定性、叠加结构的底层机制探索；从理解模型是否真正"学会推理"，到揭示训练节奏如何决定模型是否"背题"……

**这些研究在三个层面为机器人提供了理论工具**：

| 层面 | 贡献 | 来源论文 |
|:---|:---|:---|
| **认知** | 多样性、推理边界 | Hivemind, RL Reasoning |
| **控制** | 深层策略、稀疏激活 | 1000 Layer, Gated Attention |
| **学习** | 泛化机制、表示叠加 | Diffusion, Superposition |

**NeurIPS 的价值从来不只在于理论上的优雅，更在于其能否在混乱、充满不确定性的物理世界中经受检验。**

如果说"具身智能"曾被视为 AI 的终极形态，那么今天它更像是一面现实的棱镜——将实验室中光滑的算法，折射出其在真实环境中的全部短板与变形。

**无论论文本身是否获奖，最终我们关心的"永远在代码之外、在现实之中"。**

---

## 面试高频 Q&A

### Q1: NeurIPS 2025 这些论文对 VLA 最大的启示是什么？

A: 三个核心启示：
1. **深度有红利**: 1000 层网络证明深层自监督学习在 RL 中仍有巨大潜力
2. **RL 有边界**: RLVR 不能真正扩展推理能力，需要组合其他方法
3. **架构很重要**: 门控注意力、表示叠加等底层机制直接影响部署效果

### Q2: 如何将这些研究成果应用到实际 VLA 系统中？

A:
| 论文 | 实际应用 |
|:---|:---|
| Gated Attention | 改进 VLA 的 cross-attention 层，提升边缘部署效率 |
| 1000 Layer | 探索更深的策略网络，配合自监督预训练 |
| Diffusion Generalization | 设计更好的训练早停策略，监控泛化指标 |
| Superposition | 设计多技能统一架构，利用叠加实现知识共享 |
| RL Reasoning | 谨慎使用 RL 微调，保留策略多样性 |
| Hivemind | 在评估中加入多样性指标，避免单一策略陷阱 |

### Q3: 具身智能的"GPT 时刻"还有多远？

A: 基于这些论文的洞察：
- **技术层面**: 基础研究正在快速积累，深层学习、稀疏架构、表示学习都在进步
- **数据层面**: 仍然是最大瓶颈，需要更多高质量机器人数据
- **部署层面**: 边缘计算能力在提升，但仍有差距
- **预测**: 2-3 年内可能出现真正的"通用机器人基础模型"原型

### Q4: 对于准备面试的同学，这篇文章最值得记住的点是什么？

A:
1. **趋势判断**: 能够从顶会论文中看出技术发展方向
2. **跨领域视角**: 将通用 AI 研究与具身智能联系起来
3. **批判性思维**: 不盲目相信 RL 的能力，理解其边界
4. **工程落地**: 关注门控注意力、训练策略等可立即应用的技术

---

## 参考资源

| 论文 | 链接 |
|:---|:---|
| Artificial Hivemind | [OpenReview](https://openreview.net/pdf?id=saDOrrnNTz) |
| Gated Attention for LLMs | [OpenReview](https://openreview.net/forum?id=1b7whO4SfY) |
| 1000 Layer Networks | [OpenReview](https://openreview.net/forum?id=s0JVsx3bx1) |
| Diffusion Memorization | [OpenReview](https://openreview.net/pdf?id=BSZqpqgqM0) |
| Superposition Scaling | [OpenReview](https://openreview.net/pdf?id=knPz7gtjPW) |
| RL Reasoning in LLMs | [OpenReview](https://openreview.net/forum?id=4OsgYD7em5) |
| NeurIPS 2025 最佳论文公告 | [NeurIPS Blog](https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/) |

---

**编辑**: VLA Handbook Team
**来源**: 深蓝具身智能
**最后更新**: 2025-12-13

---
[← Back to Theory](./README.md) | [Paper Index](./paper_index.md)

