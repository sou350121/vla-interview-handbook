# å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)

> **æ ¸å¿ƒæ¦‚å¿µ**: å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) æ˜¯ä¸€ç§é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„æ–¹æ³•ã€‚æ™ºèƒ½ä½“é€šè¿‡**è¯•é”™** (Trial-and-Error) å’Œ**å¥–åŠ±åé¦ˆ** (Reward Feedback) ä¸æ–­æ”¹è¿›è¡Œä¸ºã€‚

## 1. ä¸ºä»€ä¹ˆ VLA éœ€è¦å¼ºåŒ–å­¦ä¹ ? (Why RL for VLA?)

### 1.1 è¡Œä¸ºå…‹éš† (BC) çš„å±€é™

ä¼ ç»Ÿ VLA ä¸»è¦ä¾èµ– **è¡Œä¸ºå…‹éš† (Behavior Cloning)**ï¼šæ¨¡ä»¿äººç±»æ¼”ç¤ºã€‚

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
| :--- | :--- | :--- |
| **BC** | ç®€å•ï¼Œæ•°æ®æ•ˆç‡é«˜ | ä¸Šé™æ˜¯äººç±»æ°´å¹³ï¼Œæ— æ³•è¶…è¶Šæ¼”ç¤ºè€… |
| **RL** | å¯ä»¥è¶…è¶Šäººç±»ï¼Œè‡ªæˆ‘è¿›åŒ– | éœ€è¦å¤§é‡äº¤äº’ï¼Œç¨€ç–å¥–åŠ±éš¾ä¼˜åŒ– |

### 1.2 RL åœ¨ VLA ä¸­çš„ä»·å€¼

- **è¶…è¶Šäººç±»ç¤ºæ•™**: é€šè¿‡è‡ªæˆ‘åšå¼ˆ/æ¢ç´¢å‘ç°æ›´ä¼˜ç­–ç•¥
- **é•¿åºåˆ—ä¼˜åŒ–**: BC åªæ¨¡ä»¿æ¯æ­¥åŠ¨ä½œï¼ŒRL ä¼˜åŒ–æ•´ä¸ªè½¨è¿¹çš„ç´¯ç§¯å›æŠ¥
- **é€‚åº”æ€§å­¦ä¹ **: åœ¨çœŸæœºéƒ¨ç½²æ—¶æŒç»­è‡ªæˆ‘æ”¹è¿›
- **ç¨€ç–å¥–åŠ±ä»»åŠ¡**: åªæœ‰ä»»åŠ¡æˆåŠŸæ—¶ç»™å¥–åŠ±çš„åœºæ™¯ï¼ˆå¦‚ç»„è£…ï¼‰

### 1.3 VLA ä¸­çš„ RL åº”ç”¨æ¡ˆä¾‹

- **Ï€*0.6 (Pi-Star)**: ä½¿ç”¨ Recap ç®—æ³•ï¼ˆOffline RLï¼‰è¶…è¶Šäººç±»ç¤ºæ•™
- **RT-2**: ä½¿ç”¨ RL from Human Feedback (RLHF) æ”¹è¿›è¯­ä¹‰æ¨ç†
- **RoboCasa**: ä½¿ç”¨ PPO è®­ç»ƒå®¶åº­æ“ä½œç­–ç•¥

## 2. RL åŸºç¡€æ¦‚å¿µ (RL Fundamentals)

### 2.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

```
MDP = (S, A, P, R, Î³)
```

- **S**: çŠ¶æ€ç©ºé—´ (State Space) - æœºå™¨äººè§‚æµ‹åˆ°çš„ä¸€åˆ‡
- **A**: åŠ¨ä½œç©ºé—´ (Action Space) - å¯æ‰§è¡Œçš„åŠ¨ä½œ
- **P(s'|s,a)**: çŠ¶æ€è½¬ç§»æ¦‚ç‡ - ç¯å¢ƒåŠ¨åŠ›å­¦
- **R(s,a)**: å¥–åŠ±å‡½æ•° - è¡Œä¸ºå¥½åçš„åé¦ˆ
- **Î³**: æŠ˜æ‰£å› å­ - æœªæ¥å¥–åŠ±çš„æƒé‡ (é€šå¸¸ 0.99)

### 2.2 é©¬å°”å¯å¤«æ€§ (Markov Property)

**å®šä¹‰**: ä¸‹ä¸€çŠ¶æ€åªä¾èµ–äºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä¸å†å²æ— å…³ã€‚

```
P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    é©¬å°”å¯å¤«æ€§å›¾è§£                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   éé©¬å°”å¯å¤«:  sâ‚€ â†’ sâ‚ â†’ sâ‚‚ â†’ sâ‚ƒ â†’ sâ‚„                           â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                  æ‰€æœ‰å†å²éƒ½å½±å“æœªæ¥                               â”‚
â”‚                                                                 â”‚
â”‚   é©¬å°”å¯å¤«:    sâ‚€ â†’ sâ‚ â†’ sâ‚‚ â†’ sâ‚ƒ â†’ sâ‚„                           â”‚
â”‚                          â””â”€â”€â”€â”˜                                  â”‚
â”‚                    åªæœ‰ sâ‚ƒ å½±å“ sâ‚„                               â”‚
â”‚                                                                 â”‚
â”‚   ğŸ’¡ "å½“å‰çŠ¶æ€æ˜¯å¯¹å†å²çš„å……åˆ†ç»Ÿè®¡é‡"                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆé‡è¦**:
- **ç®€åŒ–è®¡ç®—**: ä¸éœ€è¦è®°å¿†æ•´ä¸ªå†å²ï¼Œåªéœ€å½“å‰çŠ¶æ€
- **Bellman æ–¹ç¨‹æˆç«‹**: ä»·å€¼å‡½æ•°å¯ä»¥é€’å½’å®šä¹‰
- **å®é™…åº”ç”¨**: æœºå™¨äººçŠ¶æ€é€šå¸¸åŒ…å«ä½ç½®+é€Ÿåº¦ï¼Œæ»¡è¶³é©¬å°”å¯å¤«æ€§ï¼›è‹¥åªæœ‰ä½ç½®åˆ™ä¸æ»¡è¶³

### 2.3 æ ¸å¿ƒç›®æ ‡

æœ€å¤§åŒ–**ç´¯ç§¯æŠ˜æ‰£å›æŠ¥ (Cumulative Discounted Return)**:

```
G_t = Î£_{k=0}^{âˆ} Î³^k Ã— R_{t+k+1}
```

### 2.4 ä»·å€¼å‡½æ•° (Value Functions)

**çŠ¶æ€ä»·å€¼å‡½æ•° (State Value)**:

```
V^Ï€(s) = E_Ï€[ G_t | S_t = s ]
```

**åŠ¨ä½œä»·å€¼å‡½æ•° (Action Value / Q-Function)**:

```
Q^Ï€(s, a) = E_Ï€[ G_t | S_t = s, A_t = a ]
```

**Bellman æ–¹ç¨‹**:

```
Q^Ï€(s, a) = R(s, a) + Î³ Ã— E_{s' ~ P}[ V^Ï€(s') ]
```

### 2.5 æœ€ä¼˜ä»·å€¼å‡½æ•°ä¸æœ€ä¼˜ç­–ç•¥

**æœ€ä¼˜ä»·å€¼å‡½æ•°**:

```
V*(s) = max_Ï€ V^Ï€(s)
Q*(s, a) = max_Ï€ Q^Ï€(s, a)
```

**æœ€ä¼˜ç­–ç•¥**:

```
Ï€*(s) = argmax_a Q*(s, a)
```

**ä¸ºä»€ä¹ˆæœ€ä¼˜ä»·å€¼å‡½æ•°å°±æ˜¯æœ€ä¼˜ç­–ç•¥ï¼Ÿ**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                æœ€ä¼˜ä»·å€¼å‡½æ•° â†” æœ€ä¼˜ç­–ç•¥                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   æ ¸å¿ƒå®šç†: ç»™å®š Q*(s, a)ï¼Œæœ€ä¼˜ç­–ç•¥å¯ç›´æ¥å¯¼å‡º                    â”‚
â”‚                                                                 â”‚
â”‚   Ï€*(s) = argmax_a Q*(s, a)                                     â”‚
â”‚                                                                 â”‚
â”‚   è¯æ˜æ€è·¯:                                                     â”‚
â”‚   1. Q*(s,a) è¡¨ç¤ºåœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ a åï¼ŒæŒ‰æœ€ä¼˜ç­–ç•¥è¡ŒåŠ¨çš„æœŸæœ›å›æŠ¥ â”‚
â”‚   2. è¦æœ€å¤§åŒ–å›æŠ¥ï¼Œåªéœ€åœ¨æ¯ä¸ªçŠ¶æ€é€‰æ‹© Q* æœ€å¤§çš„åŠ¨ä½œ              â”‚
â”‚   3. è¿™æ­£æ˜¯è´ªå¿ƒç­–ç•¥ï¼Œè€Œå¯¹äº Q* è´ªå¿ƒå°±æ˜¯æœ€ä¼˜çš„                    â”‚
â”‚                                                                 â”‚
â”‚   åè¿‡æ¥:                                                       â”‚
â”‚   ç»™å®šæœ€ä¼˜ç­–ç•¥ Ï€*ï¼Œå¯ä»¥è®¡ç®—å‡º Q*(s,a) = Q^{Ï€*}(s,a)             â”‚
â”‚                                                                 â”‚
â”‚   ğŸ’¡ æœ€ä¼˜ä»·å€¼å‡½æ•°å’Œæœ€ä¼˜ç­–ç•¥æ˜¯"ä¸€ä½“ä¸¤é¢"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ•°å­¦æ¨å¯¼**:
1. **Bellman æœ€ä¼˜æ–¹ç¨‹**: `V*(s) = max_a [ R(s,a) + Î³ Ã— Î£_{s'} P(s'|s,a) Ã— V*(s') ]`
2. å¦‚æœæˆ‘ä»¬çŸ¥é“ `V*`ï¼Œåˆ™æœ€ä¼˜åŠ¨ä½œæ˜¯ä½¿ä¸Šå¼å–æœ€å¤§å€¼çš„ `a`
3. è¿™ç­‰ä»·äº `Ï€*(s) = argmax_a Q*(s,a)`

### 2.6 ç­–ç•¥ (Policy)

ç­–ç•¥ `Ï€(a|s)` å®šä¹‰äº†åœ¨çŠ¶æ€ `s` ä¸‹é‡‡å–åŠ¨ä½œ `a` çš„æ¦‚ç‡ã€‚

- **ç¡®å®šæ€§ç­–ç•¥**: `a = Ï€(s)`
- **éšæœºç­–ç•¥**: `a ~ Ï€(Â·|s)`

## 3. RL ç®—æ³•åˆ†ç±» (RL Algorithm Taxonomy)

```
                        RL ç®—æ³•
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚               â”‚
      Model-Free      Model-Based    Offline RL
           â”‚               â”‚               â”‚
     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”         â”‚          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
     â”‚           â”‚         â”‚          â”‚         â”‚
 Value-Based  Policy-Based â”‚       CQL/IQL   Recap
     â”‚           â”‚         â”‚
   DQN/SAC   PPO/TRPO   Dreamer/MBPO
```

### 3.1 Model-Free vs Model-Based

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Model-Free vs Model-Based                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Model-Free (æ— æ¨¡å‹):                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Agent â†â”€â”€â”€â”€ äº¤äº’ â”€â”€â”€â”€â†’ Environment                     â”‚   â”‚
â”‚   â”‚    â”‚                         â”‚                          â”‚   â”‚
â”‚   â”‚    â””â”€â”€ ç›´æ¥å­¦ä¹  Ï€ æˆ– Q â”€â”€â”€â”€â”€â”€â”˜                          â”‚   â”‚
â”‚   â”‚        (ä¸å…³å¿ƒç¯å¢ƒå¦‚ä½•å·¥ä½œ)                              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   ä»£è¡¨: DQN, PPO, SAC                                           â”‚
â”‚   ç‰¹ç‚¹: ç®€å•ç›´æ¥ï¼Œä½†éœ€è¦å¤§é‡äº¤äº’æ•°æ®                            â”‚
â”‚                                                                 â”‚
â”‚   Model-Based (åŸºäºæ¨¡å‹):                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Agent â†â”€â”€â”€â”€ äº¤äº’ â”€â”€â”€â”€â†’ Environment                     â”‚   â”‚
â”‚   â”‚    â”‚                         â”‚                          â”‚   â”‚
â”‚   â”‚    â”œâ”€â”€ å­¦ä¹ ç¯å¢ƒæ¨¡å‹ P(s'|s,a) â”˜                         â”‚   â”‚
â”‚   â”‚    â”‚                                                    â”‚   â”‚
â”‚   â”‚    â””â”€â”€ åœ¨æ¨¡å‹ä¸­è§„åˆ’/æ¨¡æ‹Ÿ â”€â”€â†’ ç­–ç•¥                       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   ä»£è¡¨: Dreamer, MBPO, MuZero                                   â”‚
â”‚   ç‰¹ç‚¹: æ ·æœ¬é«˜æ•ˆï¼Œä½†æ¨¡å‹è¯¯å·®ä¼šç´¯ç§¯                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| ç±»å‹ | åŸç† | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
| :--- | :--- | :--- | :--- | :--- |
| **Model-Free** | ç›´æ¥å­¦ä¹ ç­–ç•¥/ä»·å€¼ | ç®€å•ï¼Œæ— æ¨¡å‹åå·® | æ ·æœ¬æ•ˆç‡ä½ | çœŸæœºäº¤äº’æˆæœ¬ä½ |
| **Model-Based** | å­¦ä¹ ç¯å¢ƒåŠ¨åŠ›å­¦ $P(s'|s,a)$ | æ ·æœ¬æ•ˆç‡é«˜ | æ¨¡å‹è¯¯å·®ç´¯ç§¯ | ä»¿çœŸç¯å¢ƒã€Sim-to-Real |

**å…³é”®åŒºåˆ«**:
- **Model-Free**: ä¸å°è¯•ç†è§£ç¯å¢ƒå¦‚ä½•å·¥ä½œï¼Œåªå…³å¿ƒ"ä»€ä¹ˆåŠ¨ä½œèƒ½è·å¾—é«˜å›æŠ¥"
- **Model-Based**: å…ˆå­¦ä¹ ç¯å¢ƒçš„"è§„åˆ™"ï¼ˆçŠ¶æ€è½¬ç§»ï¼‰ï¼Œå†åˆ©ç”¨è§„åˆ™è¿›è¡Œè§„åˆ’

**VLA ä¸­çš„åº”ç”¨**:
- **Model-Free**: Ï€0.6 çš„ Recap ç®—æ³•ï¼ˆç›´æ¥ä»æ•°æ®å­¦ä¹ ç­–ç•¥ï¼‰
- **Model-Based**: ä¸–ç•Œæ¨¡å‹ (World Model) ç”¨äºé¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œè¾…åŠ©è§„åˆ’

### 3.2 ç­–ç•¥è¿­ä»£ vs å€¼è¿­ä»£ (Policy Iteration vs Value Iteration)

ä¸¤ç§ç»å…¸çš„åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œç”¨äºæ±‚è§£ MDPã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç­–ç•¥è¿­ä»£ vs å€¼è¿­ä»£                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   ç­–ç•¥è¿­ä»£ (Policy Iteration):                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  åˆå§‹åŒ– Ï€â‚€                                               â”‚   â”‚
â”‚   â”‚      â”‚                                                   â”‚   â”‚
â”‚   â”‚      â–¼                                                   â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚   â”‚
â”‚   â”‚  â”‚ ç­–ç•¥è¯„ä¼° (PE)    â”‚ â† è®¡ç®— V^Ï€ (è¿­ä»£è‡³æ”¶æ•›)            â”‚   â”‚
â”‚   â”‚  â”‚ V^Ï€(s) = E[...]  â”‚                                    â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚   â”‚
â”‚   â”‚           â”‚                                              â”‚   â”‚
â”‚   â”‚           â–¼                                              â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚   â”‚
â”‚   â”‚  â”‚ ç­–ç•¥æ”¹è¿› (PI)    â”‚ â† Ï€(s) = argmax_a Q(s,a)           â”‚   â”‚
â”‚   â”‚  â”‚ Ï€ â† greedy(V)    â”‚                                    â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚   â”‚
â”‚   â”‚           â”‚                                              â”‚   â”‚
â”‚   â”‚           â””â”€â”€â”€â”€ é‡å¤ç›´åˆ° Ï€ ä¸å†å˜åŒ– â”€â”€â”€â”€â”˜                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   å€¼è¿­ä»£ (Value Iteration):                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  åˆå§‹åŒ– Vâ‚€                                               â”‚   â”‚
â”‚   â”‚      â”‚                                                   â”‚   â”‚
â”‚   â”‚      â–¼                                                   â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚  â”‚ V(s) â† max_a [R(s,a) + Î³ Î£ P(s'|s,a) V(s')]      â”‚   â”‚   â”‚
â”‚   â”‚  â”‚       (Bellman æœ€ä¼˜æ–¹ç¨‹çš„è¿­ä»£æ›´æ–°)                â”‚   â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚           â”‚                                              â”‚   â”‚
â”‚   â”‚           â””â”€â”€â”€â”€ é‡å¤ç›´åˆ° V æ”¶æ•› â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚   â”‚                                                          â”‚   â”‚
â”‚   â”‚  æœ€å: Ï€(s) = argmax_a Q(s,a)                            â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| å¯¹æ¯” | ç­–ç•¥è¿­ä»£ | å€¼è¿­ä»£ |
| :--- | :--- | :--- |
| **æ ¸å¿ƒæ“ä½œ** | è¯„ä¼° + æ”¹è¿›äº¤æ›¿ | ç›´æ¥è¿­ä»£ Bellman æœ€ä¼˜æ–¹ç¨‹ |
| **æ¯è½®è®¡ç®—** | ç­–ç•¥è¯„ä¼°éœ€è¿­ä»£è‡³æ”¶æ•› | åªåšä¸€æ¬¡ Bellman æ›´æ–° |
| **æ”¶æ•›é€Ÿåº¦** | è¿­ä»£æ¬¡æ•°å°‘ | æ¯è½®è®¡ç®—é‡å° |
| **æ€»ä½“æ•ˆç‡** | å¤§çŠ¶æ€ç©ºé—´æ›´å¿« | å°çŠ¶æ€ç©ºé—´æ›´å¿« |
| **ç­–ç•¥è¾“å‡º** | æ¯è½®éƒ½æœ‰æ˜¾å¼ç­–ç•¥ | æœ€åæ‰æå–ç­–ç•¥ |

**ç›´è§‰ç†è§£**:
- **ç­–ç•¥è¿­ä»£**: "å…ˆå®Œæ•´è¯„ä¼°å½“å‰ç­–ç•¥æœ‰å¤šå¥½ï¼Œå†æ”¹è¿›"
- **å€¼è¿­ä»£**: "ç›´æ¥æœæœ€ä¼˜ä»·å€¼å‡½æ•°è¿­ä»£ï¼Œæœ€åå†æå–ç­–ç•¥"

### 3.3 On-Policy vs Off-Policy

| ç±»å‹ | ä»£è¡¨ç®—æ³• | ç‰¹ç‚¹ |
| :--- | :--- | :--- |
| **On-Policy** | PPO, TRPO | åªç”¨å½“å‰ç­–ç•¥çš„æ•°æ®ï¼Œç¨³å®šä½†ä½æ•ˆ |
| **Off-Policy** | SAC, TD3 | å¯å¤ç”¨å†å²æ•°æ®ï¼Œé«˜æ•ˆä½†ä¸ç¨³å®š |

## 4. VLA å¸¸ç”¨ RL ç®—æ³• (RL Algorithms for VLA)

### 4.1 PPO (Proximal Policy Optimization)

**æ ¸å¿ƒæ€æƒ³**: é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢è®­ç»ƒå´©æºƒã€‚

**ç›®æ ‡å‡½æ•°**:

```
L_CLIP(Î¸) = E_t[ min( r_t(Î¸) Ã— Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) Ã— Ã‚_t ) ]
```

å…¶ä¸­ `r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_{Î¸_old}(a_t|s_t)` æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class PPO:
    def __init__(self, policy, value_net, clip_epsilon=0.2, lr=3e-4):
        self.policy = policy
        self.value_net = value_net
        self.clip_epsilon = clip_epsilon
        self.optimizer = torch.optim.Adam(
            list(policy.parameters()) + list(value_net.parameters()), lr=lr
        )
    
    def compute_gae(self, rewards, values, dones, gamma=0.99, lam=0.95):
        """Generalized Advantage Estimation"""
        advantages = []
        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + gamma * lam * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        return torch.tensor(advantages)
    
    def update(self, states, actions, old_log_probs, rewards, dones):
        # è®¡ç®—ä»·å€¼å’Œä¼˜åŠ¿
        values = self.value_net(states).squeeze()
        advantages = self.compute_gae(rewards, values.detach(), dones)
        returns = advantages + values.detach()
        
        # å½’ä¸€åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO æ›´æ–°
        for _ in range(10):  # å¤šè½®æ›´æ–°
            # è®¡ç®—æ–°çš„ log_prob
            dist = self.policy(states)
            new_log_probs = dist.log_prob(actions)
            
            # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            ratio = torch.exp(new_log_probs - old_log_probs)
            
            # Clipped ç›®æ ‡
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼æŸå¤±
            value_loss = F.mse_loss(self.value_net(states).squeeze(), returns)
            
            # ç†µæ­£åˆ™åŒ– (é¼“åŠ±æ¢ç´¢)
            entropy = dist.entropy().mean()
            
            # æ€»æŸå¤±
            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
```

### 4.2 SAC (Soft Actor-Critic)

**æ ¸å¿ƒæ€æƒ³**: æœ€å¤§åŒ–å¥–åŠ±çš„åŒæ—¶æœ€å¤§åŒ–ç­–ç•¥ç†µï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰ã€‚

**ç›®æ ‡**:

```
J(Ï€) = E_{Ï„ ~ Ï€}[ Î£_t R(s_t, a_t) + Î± Ã— H(Ï€(Â·|s_t)) ]
```

```python
class SAC:
    def __init__(self, actor, critic1, critic2, target_critic1, target_critic2, 
                 alpha=0.2, gamma=0.99, tau=0.005):
        self.actor = actor
        self.critic1 = critic1
        self.critic2 = critic2
        self.target_critic1 = target_critic1
        self.target_critic2 = target_critic2
        self.alpha = alpha  # ç†µç³»æ•°
        self.gamma = gamma
        self.tau = tau
    
    def update(self, batch):
        states, actions, rewards, next_states, dones = batch
        
        # ===== Critic æ›´æ–° =====
        with torch.no_grad():
            # ä»å½“å‰ç­–ç•¥é‡‡æ ·ä¸‹ä¸€ä¸ªåŠ¨ä½œ
            next_actions, next_log_probs = self.actor.sample(next_states)
            
            # ç›®æ ‡ Q å€¼ (å–ä¸¤ä¸ª critic çš„æœ€å°å€¼)
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            
            # TD ç›®æ ‡
            target = rewards + self.gamma * (1 - dones) * target_q
        
        # å½“å‰ Q å€¼
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic_loss = F.mse_loss(current_q1, target) + F.mse_loss(current_q2, target)
        
        # ===== Actor æ›´æ–° =====
        new_actions, log_probs = self.actor.sample(states)
        q1 = self.critic1(states, new_actions)
        q2 = self.critic2(states, new_actions)
        q = torch.min(q1, q2)
        
        # æœ€å¤§åŒ– Q - Î± * log_prob
        actor_loss = (self.alpha * log_probs - q).mean()
        
        # ===== è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ =====
        self.soft_update(self.critic1, self.target_critic1)
        self.soft_update(self.critic2, self.target_critic2)
        
        return critic_loss, actor_loss
    
    def soft_update(self, source, target):
        for src_param, tgt_param in zip(source.parameters(), target.parameters()):
            tgt_param.data.copy_(self.tau * src_param.data + (1 - self.tau) * tgt_param.data)
```

### 4.3 Offline RL (ç¦»çº¿å¼ºåŒ–å­¦ä¹ )

**æ ¸å¿ƒé—®é¢˜**: åªæœ‰å›ºå®šçš„å†å²æ•°æ®é›†ï¼Œæ— æ³•ä¸ç¯å¢ƒäº¤äº’ã€‚

**æŒ‘æˆ˜**: åˆ†å¸ƒåç§» (Distribution Shift) - ç­–ç•¥å¯èƒ½é€‰æ‹©æ•°æ®é›†ä¸­æ²¡è§è¿‡çš„åŠ¨ä½œã€‚

#### 4.3.1 CQL (Conservative Q-Learning)

**æ€æƒ³**: ä¿å®ˆä¼°è®¡ Q å€¼ï¼Œæƒ©ç½šæ•°æ®é›†å¤–çš„åŠ¨ä½œã€‚

```
L_CQL = Î± Ã— E_{s ~ D}[ log Î£_a exp(Q(s,a)) - E_{a ~ D}[Q(s,a)] ] + L_TD
```

#### 4.3.2 IQL (Implicit Q-Learning)

**æ€æƒ³**: ä½¿ç”¨åˆ†ä½æ•°å›å½’é¿å…æ˜¾å¼æœ€å¤§åŒ– Qã€‚

#### 4.3.3 Recap (Ï€*0.6 çš„æ ¸å¿ƒç®—æ³•)

+**æ€æƒ³**: ä»æˆåŠŸå’Œå¤±è´¥çš„è½¨è¿¹ä¸­å­¦ä¹ ï¼ˆè¯¦ç»†æœºåˆ¶å¯å‚è€ƒ [pi0_6_dissection.md](./pi0_6_dissection.md) ä¸­çš„ Recap è§£æï¼‰ã€‚

```python
class RecapAlgorithm:
    """Ï€*0.6 çš„ Recap ç¦»çº¿ RL ç®—æ³•"""
    def __init__(self, policy, value_net):
        self.policy = policy
        self.value_net = value_net
    
    def label_trajectories(self, trajectories):
        """æ ‡æ³¨è½¨è¿¹: æˆåŠŸ vs å¤±è´¥"""
        labeled = []
        for traj in trajectories:
            success = traj['final_reward'] > 0  # ä»»åŠ¡æ˜¯å¦æˆåŠŸ
            for t, transition in enumerate(traj['transitions']):
                # å…³é”®: æ‰¾åˆ°å¤±è´¥è½¨è¿¹ä¸­"å¼€å§‹å‡ºé”™"çš„æ—¶åˆ»
                if not success and self.is_critical_failure(traj, t):
                    transition['label'] = 'negative'  # è´Ÿæ ·æœ¬
                elif success:
                    transition['label'] = 'positive'  # æ­£æ ·æœ¬
                labeled.append(transition)
        return labeled
    
    def is_critical_failure(self, traj, t):
        """åˆ¤æ–­æ˜¯å¦æ˜¯å¯¼è‡´å¤±è´¥çš„å…³é”®æ—¶åˆ»"""
        # ä½¿ç”¨ä»·å€¼å‡½æ•°ä¼°è®¡: å¦‚æœ V éª¤é™ï¼Œè¯´æ˜è¿™æ­¥å‡ºé”™äº†
        v_t = self.value_net(traj['states'][t])
        v_t1 = self.value_net(traj['states'][t+1])
        return (v_t - v_t1) > threshold
    
    def update(self, labeled_data):
        """å¯¹æ¯”å­¦ä¹ : æå‡æ­£æ ·æœ¬æ¦‚ç‡ï¼Œé™ä½è´Ÿæ ·æœ¬æ¦‚ç‡"""
        loss = 0
        for sample in labeled_data:
            state, action = sample['state'], sample['action']
            log_prob = self.policy.log_prob(state, action)
            
            if sample['label'] == 'positive':
                loss -= log_prob  # æå‡æ­£æ ·æœ¬æ¦‚ç‡
            else:
                loss += log_prob  # é™ä½è´Ÿæ ·æœ¬æ¦‚ç‡
        
        return loss / len(labeled_data)
```

## 5. å¥–åŠ±è®¾è®¡ (Reward Engineering)

### 5.1 ç¨€ç–å¥–åŠ± vs ç¨ å¯†å¥–åŠ±

| ç±»å‹ | ç¤ºä¾‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
| :--- | :--- | :--- | :--- |
| **ç¨€ç–** | ä»»åŠ¡æˆåŠŸ +1ï¼Œå¦åˆ™ 0 | ä¸éœ€è¦äººå·¥è®¾è®¡ | éš¾ä»¥å­¦ä¹  |
| **ç¨ å¯†** | è·ç¦»ç›®æ ‡è¶Šè¿‘å¥–åŠ±è¶Šé«˜ | å­¦ä¹ å®¹æ˜“ | å¯èƒ½å¯¼è‡´å±€éƒ¨æœ€ä¼˜ |

### 5.2 å¥–åŠ±è®¾è®¡ç¤ºä¾‹

```python
def compute_reward(state, action, next_state, task_type="pick_and_place"):
    """æœºå™¨äººæ“ä½œä»»åŠ¡çš„å¥–åŠ±å‡½æ•°"""
    
    if task_type == "pick_and_place":
        gripper_pos = state['gripper_position']
        object_pos = state['object_position']
        target_pos = state['target_position']
        
        # é˜¶æ®µ 1: æ¥è¿‘ç‰©ä½“
        dist_to_object = np.linalg.norm(gripper_pos - object_pos)
        
        # é˜¶æ®µ 2: æŠ“å–ç‰©ä½“
        is_grasping = state['gripper_closed'] and dist_to_object < 0.02
        
        # é˜¶æ®µ 3: ç§»åŠ¨åˆ°ç›®æ ‡
        if is_grasping:
            dist_to_target = np.linalg.norm(object_pos - target_pos)
        else:
            dist_to_target = 1.0  # æƒ©ç½šæ²¡æŠ“åˆ°ç‰©ä½“
        
        # ç»„åˆå¥–åŠ±
        reward = -0.1 * dist_to_object  # æ¥è¿‘ç‰©ä½“
        reward += 0.5 * is_grasping     # æŠ“å–å¥–åŠ±
        reward -= 0.1 * dist_to_target  # æ¥è¿‘ç›®æ ‡
        
        # ç¨€ç–æˆåŠŸå¥–åŠ±
        if dist_to_target < 0.05 and is_grasping:
            reward += 10.0  # ä»»åŠ¡æˆåŠŸ
        
        return reward
```

### 5.3 å¥–åŠ±å¡‘å½¢ (Reward Shaping)

```python
def shaped_reward(state, next_state, potential_func, gamma=0.99):
    """
    åŸºäºåŠ¿å‡½æ•°çš„å¥–åŠ±å¡‘å½¢ (ä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥)
    F(s, s') = Î³ * Î¦(s') - Î¦(s)
    """
    phi_s = potential_func(state)
    phi_s_next = potential_func(next_state)
    shaping = gamma * phi_s_next - phi_s
    return shaping
```

## 6. RL + VLA çš„ç»“åˆ (RL + VLA Integration)

### 6.1 RLHF å®Œæ•´æµç¨‹ (RLHF Pipeline)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLHF ä¸‰é˜¶æ®µæµç¨‹                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   é˜¶æ®µ 1: SFT (Supervised Fine-Tuning)                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  é¢„è®­ç»ƒæ¨¡å‹ + é«˜è´¨é‡æ•°æ® â†’ åŸºç¡€ç­–ç•¥ Ï€_SFT                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚   é˜¶æ®µ 2: Reward Model Training                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  æ”¶é›†äººç±»åå¥½ (A > B) â†’ è®­ç»ƒå¥–åŠ±æ¨¡å‹ R(s, a)              â”‚   â”‚
â”‚   â”‚  Loss: -log Ïƒ(R(y_w) - R(y_l))  (Bradley-Terry)          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚   é˜¶æ®µ 3: RL Fine-tuning (PPO)                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  ä½¿ç”¨ R(s, a) ä½œä¸ºå¥–åŠ±ï¼ŒPPO ä¼˜åŒ–ç­–ç•¥                      â”‚   â”‚
â”‚   â”‚  + KL æƒ©ç½š: é˜²æ­¢åç¦» Ï€_SFT å¤ªè¿œ                          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   éœ€è¦åŒæ—¶åŠ è½½: Ï€_SFT (å‚è€ƒ), Ï€_Î¸ (è®­ç»ƒ), R (å¥–åŠ±), V (ä»·å€¼)    â”‚
â”‚   æ˜¾å­˜éœ€æ±‚: 4 ä¸ªæ¨¡å‹ â†’ éå¸¸æ˜‚è´µ!                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
class RLHF_VLA:
    """ä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ”¹è¿› VLA"""
    def __init__(self, vla_model, reward_model):
        self.vla = vla_model
        self.reward_model = reward_model  # ä»äººç±»åå¥½è®­ç»ƒ
    
    def collect_comparisons(self, states, num_samples=2):
        """æ”¶é›†äººç±»åå¥½æ¯”è¾ƒ"""
        actions = [self.vla.sample(states) for _ in range(num_samples)]
        # äººç±»é€‰æ‹©æ›´å¥½çš„åŠ¨ä½œ
        human_preference = get_human_preference(states, actions)
        return actions, human_preference
    
    def train_reward_model(self, comparisons):
        """ä»åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹"""
        for (action_win, action_lose, state) in comparisons:
            r_win = self.reward_model(state, action_win)
            r_lose = self.reward_model(state, action_lose)
            
            # Bradley-Terry æ¨¡å‹
            loss = -torch.log(torch.sigmoid(r_win - r_lose))
            loss.backward()
    
    def rl_finetune(self, states):
        """ä½¿ç”¨å­¦åˆ°çš„å¥–åŠ±è¿›è¡Œ RL å¾®è°ƒ"""
        actions = self.vla.sample(states)
        rewards = self.reward_model(states, actions)
        
        # PPO æ›´æ–°
        self.ppo_update(states, actions, rewards)
```

### 6.2 ä»æ¼”ç¤ºåˆå§‹åŒ– RL (Demo-Guided RL)

```python
class DemoGuidedRL:
    """ç»“åˆ BC é¢„è®­ç»ƒå’Œ RL å¾®è°ƒ"""
    def __init__(self, policy):
        self.policy = policy
    
    def phase1_bc_pretrain(self, demonstrations):
        """Phase 1: è¡Œä¸ºå…‹éš†é¢„è®­ç»ƒ"""
        for state, action in demonstrations:
            pred_action = self.policy(state)
            loss = F.mse_loss(pred_action, action)
            loss.backward()
    
    def phase2_rl_finetune(self, env, num_episodes=1000):
        """Phase 2: RL å¾®è°ƒè¶…è¶Šæ¼”ç¤º"""
        for episode in range(num_episodes):
            state = env.reset()
            while not done:
                action = self.policy.sample(state)
                next_state, reward, done, _ = env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self.buffer.add(state, action, reward, next_state, done)
                state = next_state
            
            # SAC/PPO æ›´æ–°
            self.rl_update()
```

## 7. é¢è¯•é«˜é¢‘é—®é¢˜ (Q&A)

**Q1: VLA ä¸­ BC å’Œ RL å¦‚ä½•å–èˆ?**

A:
- **ä¼˜å…ˆ BC**: æ•°æ®å……è¶³ã€ä»»åŠ¡ç®€å•ã€éœ€è¦å¿«é€Ÿè¿­ä»£
- **å¼•å…¥ RL**: éœ€è¦è¶…è¶Šäººç±»ã€é•¿åºåˆ—ä¼˜åŒ–ã€ç¨€ç–å¥–åŠ±ä»»åŠ¡
- **æœ€ä½³å®è·µ**: BC é¢„è®­ç»ƒ + RL å¾®è°ƒ (å¦‚ Ï€*0.6)

**Q2: Offline RL å’Œ Online RL çš„æ ¸å¿ƒåŒºåˆ«?**

A:
- **Online RL**: å¯ä»¥ä¸ç¯å¢ƒäº¤äº’ï¼Œæ¢ç´¢æ–°çŠ¶æ€
- **Offline RL**: åªæœ‰å›ºå®šæ•°æ®é›†ï¼Œéœ€è¦å¤„ç†åˆ†å¸ƒåç§»
- **VLA ç°çŠ¶**: å› ä¸ºçœŸæœºäº¤äº’æˆæœ¬é«˜ï¼ŒOffline RL æ›´å®ç”¨

**Q3: SAC ä¸­æ¸©åº¦å‚æ•° Î± çš„ä½œç”¨?**

A:
- **Î± å¤§**: æ›´é‡è§†ç†µ â†’ æ›´å¤šæ¢ç´¢ â†’ ç­–ç•¥æ›´éšæœº
- **Î± å°**: æ›´é‡è§†å¥–åŠ± â†’ æ›´å¤šåˆ©ç”¨ â†’ ç­–ç•¥æ›´ç¡®å®š
- **è‡ªåŠ¨è°ƒèŠ‚**: å¯ä»¥å°† Î± è®¾ä¸ºå¯å­¦ä¹ å‚æ•°

**Q4: ä¸ºä»€ä¹ˆ PPO æ¯” TRPO æ›´æµè¡Œ?**

A:
- **ç®€å•**: PPO åªéœ€ Clipï¼ŒTRPO éœ€è¦è®¡ç®— Fisher çŸ©é˜µ
- **é«˜æ•ˆ**: PPO å¯ä»¥ç”¨ SGDï¼ŒTRPO éœ€è¦å…±è½­æ¢¯åº¦
- **æ•ˆæœ**: åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šæ€§èƒ½ç›¸å½“

**Q5: Recap ç®—æ³•ç›¸æ¯”ä¼ ç»Ÿ Offline RL çš„ä¼˜åŠ¿?**

A:
- **åˆ©ç”¨å¤±è´¥æ•°æ®**: ä¼ ç»Ÿæ–¹æ³•åªæ¨¡ä»¿æˆåŠŸè½¨è¿¹ï¼ŒRecap ä»å¤±è´¥ä¸­å­¦ä¹ 
- **å…³é”®æ—¶åˆ»è¯†åˆ«**: é€šè¿‡ä»·å€¼å‡½æ•°å®šä½"å‡ºé”™ç‚¹"
- **ç®€å•é«˜æ•ˆ**: ä¸éœ€è¦å¤æ‚çš„çº¦æŸä¼˜åŒ–

**Q6: RLHF çš„åŸºæœ¬æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿä¸ DPO çš„å·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿ**

A: **RLHF ä¸‰é˜¶æ®µæµç¨‹**:
1. **SFT**: åœ¨é«˜è´¨é‡æ•°æ®ä¸Šç›‘ç£å¾®è°ƒï¼Œå¾—åˆ°åŸºç¡€ç­–ç•¥ $\pi_{SFT}$
2. **Reward Model**: ä»äººç±»åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ $R(s, a)$
3. **RL (PPO)**: ä½¿ç”¨ $R$ ä½œä¸ºå¥–åŠ±ï¼ŒPPO ä¼˜åŒ–ç­–ç•¥ï¼ŒåŠ  KL æƒ©ç½šé˜²æ­¢åç¦» $\pi_{SFT}$

**DPO (Direct Preference Optimization)** çš„å·®å¼‚:
- **è·³è¿‡ Reward Model**: ç›´æ¥ä»åå¥½æ•°æ®ä¼˜åŒ–ç­–ç•¥
- **æ•°å­¦æ¨å¯¼**: å°† RL ç›®æ ‡é‡å‚æ•°åŒ–ä¸ºåˆ†ç±»é—®é¢˜

```
L_DPO = -E[ log Ïƒ( Î² Ã— log(Ï€_Î¸(y_w|x) / Ï€_ref(y_w|x)) - Î² Ã— log(Ï€_Î¸(y_l|x) / Ï€_ref(y_l|x)) ) ]
```

| å¯¹æ¯” | RLHF | DPO |
| :--- | :--- | :--- |
| **é˜¶æ®µæ•°** | 3 é˜¶æ®µ | 1 é˜¶æ®µ |
| **æ¨¡å‹æ•°** | 4 ä¸ª (Ï€, Ï€_ref, R, V) | 2 ä¸ª (Ï€, Ï€_ref) |
| **ç¨³å®šæ€§** | PPO æ˜“å´© | æ›´ç¨³å®š |
| **è®¡ç®—é‡** | é«˜ | ä½ |
| **æ•ˆæœ** | ç•¥ä¼˜ (ç†è®ºä¸Š) | æ¥è¿‘ RLHF |

## 8. ä¸»æµ RL æ¡†æ¶ (RL Frameworks)

### 8.1 æ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | å®šä½ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
| :--- | :--- | :--- | :--- |
| **Stable Baselines3** | æ˜“ç”¨ã€ç¨³å®š | æ–‡æ¡£å®Œå–„ï¼ŒAPI ç®€æ´ | å¿«é€Ÿå®éªŒã€æ•™å­¦ |
| **RLlib** | åˆ†å¸ƒå¼ã€å¯æ‰©å±• | Ray ç”Ÿæ€ï¼Œå¤š GPU/èŠ‚ç‚¹ | å¤§è§„æ¨¡è®­ç»ƒ |
| **CleanRL** | å•æ–‡ä»¶å®ç° | ä»£ç æ¸…æ™°ï¼Œæ˜“äºä¿®æ”¹ | å­¦ä¹ ã€ç ”ç©¶ |
| **TorchRL** | PyTorch å®˜æ–¹ | ä¸ PyTorch æ·±åº¦é›†æˆ | ç”Ÿäº§çº§åº”ç”¨ |
| **SKRL** | Isaac Lab é›†æˆ | GPU å¹¶è¡Œï¼Œæœºå™¨äººä¸“ç”¨ | æœºå™¨äºº RL |

### 8.2 Stable Baselines3 (SB3)

**ç‰¹ç‚¹**: æœ€æ˜“ä¸Šæ‰‹çš„ RL åº“ï¼ŒAPI è®¾è®¡ä¼˜é›…ã€‚

```python
# Stable Baselines3 å¿«é€Ÿä¸Šæ‰‹
from stable_baselines3 import PPO, SAC, TD3
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback

# åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
env = make_vec_env("Pendulum-v1", n_envs=4)

# åˆ›å»ºæ¨¡å‹
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    verbose=1,
    tensorboard_log="./logs/"
)

# è®­ç»ƒ
model.learn(total_timesteps=100_000, callback=EvalCallback(env))

# ä¿å­˜/åŠ è½½
model.save("ppo_pendulum")
model = PPO.load("ppo_pendulum")

# æ¨ç†
obs = env.reset()
for _ in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
```

**è‡ªå®šä¹‰ç½‘ç»œ**:

```python
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import torch.nn as nn

class CustomCNN(BaseFeaturesExtractor):
    """è‡ªå®šä¹‰ CNN ç‰¹å¾æå–å™¨ (ç”¨äºå›¾åƒè¾“å…¥)"""
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        n_input_channels = observation_space.shape[0]
        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
        )
        # è®¡ç®—è¾“å‡ºç»´åº¦
        with torch.no_grad():
            n_flatten = self.cnn(torch.zeros(1, *observation_space.shape)).shape[1]
        self.linear = nn.Linear(n_flatten, features_dim)
    
    def forward(self, observations):
        return self.linear(self.cnn(observations))

# ä½¿ç”¨è‡ªå®šä¹‰ç½‘ç»œ
policy_kwargs = dict(
    features_extractor_class=CustomCNN,
    features_extractor_kwargs=dict(features_dim=256),
)
model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs)
```

### 8.3 RLlib (Ray)

**ç‰¹ç‚¹**: åˆ†å¸ƒå¼è®­ç»ƒé¦–é€‰ï¼Œæ”¯æŒå¤š GPU/å¤šèŠ‚ç‚¹ã€‚

```python
# RLlib åˆ†å¸ƒå¼è®­ç»ƒ
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig

# é…ç½®
config = (
    PPOConfig()
    .environment("CartPole-v1")
    .framework("torch")
    .rollouts(
        num_rollout_workers=4,      # å¹¶è¡Œ worker æ•°
        num_envs_per_worker=2,      # æ¯ä¸ª worker çš„ç¯å¢ƒæ•°
    )
    .training(
        lr=5e-5,
        gamma=0.99,
        train_batch_size=4000,
        sgd_minibatch_size=128,
        num_sgd_iter=30,
        model={"fcnet_hiddens": [256, 256]},
    )
    .resources(
        num_gpus=1,                  # è®­ç»ƒç”¨ GPU
        num_cpus_per_worker=1,
    )
)

# è®­ç»ƒ
algo = config.build()
for i in range(100):
    result = algo.train()
    print(f"Iter {i}: reward = {result['episode_reward_mean']:.2f}")

# æˆ–ä½¿ç”¨ Ray Tune è¿›è¡Œè¶…å‚æœç´¢
tune.run(
    "PPO",
    config=config.to_dict(),
    stop={"episode_reward_mean": 200},
    num_samples=4,  # å¹¶è¡Œæœç´¢ 4 ç»„è¶…å‚
)
```

**å¤šæ™ºèƒ½ä½“ RL**:

```python
# RLlib å¤šæ™ºèƒ½ä½“
from ray.rllib.algorithms.ppo import PPOConfig

config = (
    PPOConfig()
    .environment("MultiAgentCartPole")
    .multi_agent(
        policies={"policy_0", "policy_1"},
        policy_mapping_fn=lambda agent_id, episode, **kwargs: f"policy_{agent_id}",
    )
)
```

### 8.4 SKRL (Isaac Lab é›†æˆ)

**ç‰¹ç‚¹**: ä¸“ä¸º Isaac Lab è®¾è®¡ï¼ŒGPU å¹¶è¡Œè®­ç»ƒã€‚

```python
# SKRL + Isaac Lab
from skrl.agents.torch.ppo import PPO, PPO_DEFAULT_CONFIG
from skrl.trainers.torch import SequentialTrainer
from skrl.envs.wrappers.torch import wrap_env

# åŒ…è£… Isaac Lab ç¯å¢ƒ
env = wrap_env(isaac_lab_env)

# é…ç½®
cfg = PPO_DEFAULT_CONFIG.copy()
cfg["rollouts"] = 16
cfg["learning_epochs"] = 8
cfg["mini_batches"] = 4
cfg["discount_factor"] = 0.99
cfg["lambda"] = 0.95
cfg["learning_rate"] = 3e-4

# åˆ›å»º Agent
agent = PPO(
    models={"policy": policy_net, "value": value_net},
    memory=memory,
    cfg=cfg,
    observation_space=env.observation_space,
    action_space=env.action_space,
)

# è®­ç»ƒ
trainer = SequentialTrainer(cfg=trainer_cfg, env=env, agents=agent)
trainer.train()
```

### 8.5 æ¡†æ¶é€‰æ‹©æŒ‡å—

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RL æ¡†æ¶é€‰æ‹©å†³ç­–æ ‘                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Q1: æ˜¯å¦éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒ (å¤š GPU/å¤šèŠ‚ç‚¹)?                        â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€ æ˜¯ â†’ RLlib (Ray ç”Ÿæ€ï¼Œåˆ†å¸ƒå¼é¦–é€‰)                     â”‚
â”‚       â”‚                                                         â”‚
â”‚       â””â”€â”€ å¦ â†’ Q2: æ˜¯å¦ä½¿ç”¨ Isaac Lab?                          â”‚
â”‚                    â”‚                                            â”‚
â”‚                    â”œâ”€â”€ æ˜¯ â†’ SKRL (å®˜æ–¹æ¨è)                     â”‚
â”‚                    â”‚                                            â”‚
â”‚                    â””â”€â”€ å¦ â†’ Q3: ç›®æ ‡æ˜¯ä»€ä¹ˆ?                     â”‚
â”‚                                 â”‚                               â”‚
â”‚                                 â”œâ”€â”€ å¿«é€Ÿå®éªŒ â†’ SB3              â”‚
â”‚                                 â”œâ”€â”€ å­¦ä¹ ç ”ç©¶ â†’ CleanRL          â”‚
â”‚                                 â””â”€â”€ ç”Ÿäº§éƒ¨ç½² â†’ TorchRL          â”‚
â”‚                                                                 â”‚
â”‚   ğŸ’¡ VLA å¸¸ç”¨ç»„åˆ:                                              â”‚
â”‚   â€¢ ä»¿çœŸè®­ç»ƒ: Isaac Lab + SKRL/RSL-RL                           â”‚
â”‚   â€¢ å¤§è§„æ¨¡: RLlib + Ray Cluster                                 â”‚
â”‚   â€¢ å¿«é€ŸéªŒè¯: SB3 + Gymnasium                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.6 TORCS ä¸è‡ªåŠ¨é©¾é©¶ RL

**TORCS** (The Open Racing Car Simulator) æ˜¯ç»å…¸çš„è‡ªåŠ¨é©¾é©¶ RL æµ‹è¯•å¹³å°ã€‚

```python
# TORCS ç¯å¢ƒä½¿ç”¨ (gym_torcs)
import gym
import gym_torcs

env = gym.make("Torcs-v0", vision=True, throttle=True)

# è§‚æµ‹ç©ºé—´: è½¦è¾†çŠ¶æ€ + å¯é€‰è§†è§‰
# - speed, angle, trackPos, track sensors (19)
# - å¯é€‰: RGB å›¾åƒ (64x64x3)

# åŠ¨ä½œç©ºé—´: [steering, throttle, brake]
# - steering: [-1, 1]
# - throttle: [0, 1]
# - brake: [0, 1]

obs = env.reset()
for _ in range(1000):
    action = agent.act(obs)  # ä½ çš„ç­–ç•¥
    obs, reward, done, info = env.step(action)
    if done:
        obs = env.reset()
```

**æ³¨**: TORCS ä¸»è¦ç”¨äºè‡ªåŠ¨é©¾é©¶ç ”ç©¶ï¼ŒVLA æœºå™¨äººé¢†åŸŸæ›´å¸¸ç”¨ Isaac Lab/MuJoCoã€‚

## 9. å‚è€ƒèµ„æº (References)

- **PPO**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- **SAC**: [Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL](https://arxiv.org/abs/1801.01290)
- **CQL**: [Conservative Q-Learning for Offline RL](https://arxiv.org/abs/2006.04779)
- **IQL**: [Offline RL with Implicit Q-Learning](https://arxiv.org/abs/2110.06169)
- **Spinning Up**: [OpenAI Spinning Up in Deep RL](https://spinningup.openai.com/)
- **Stable Baselines3**: [Docs](https://stable-baselines3.readthedocs.io/)
- **RLlib**: [Docs](https://docs.ray.io/en/latest/rllib/)
- **SKRL**: [Docs](https://skrl.readthedocs.io/)

---
[â† Back to Theory](./README.md)

