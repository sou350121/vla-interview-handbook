<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>VLA 面试手册：从理论到实践</title>
    <style>
        * {
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Noto Sans CJK SC", "PingFang SC", "Microsoft YaHei", sans-serif;
            font-size: 14px;
            line-height: 1.7;
            color: #24292e;
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
            background: #fff;
        }
        h1 {
            color: #1a5f7a;
            border-bottom: 3px solid #1a5f7a;
            padding-bottom: 12px;
            margin-top: 50px;
            font-size: 28px;
        }
        h2 {
            color: #2c3e50;
            border-bottom: 2px solid #eaecef;
            padding-bottom: 8px;
            margin-top: 40px;
            font-size: 22px;
        }
        h3 {
            color: #34495e;
            margin-top: 30px;
            font-size: 18px;
        }
        h4 {
            color: #444;
            margin-top: 25px;
            font-size: 16px;
        }
        code {
            background-color: rgba(27, 31, 35, 0.05);
            padding: 3px 6px;
            border-radius: 4px;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            font-size: 13px;
        }
        pre {
            background-color: #f6f8fa;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 16px;
            overflow-x: auto;
            font-size: 12px;
            line-height: 1.5;
        }
        pre code {
            background: none;
            padding: 0;
            font-size: 12px;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            font-size: 13px;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 10px 14px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
            font-weight: 600;
        }
        tr:nth-child(even) {
            background-color: #fafbfc;
        }
        blockquote {
            border-left: 4px solid #1a5f7a;
            margin: 20px 0;
            padding: 12px 20px;
            background-color: #f6f8fa;
            color: #555;
        }
        a {
            color: #0366d6;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        hr {
            border: none;
            border-top: 2px solid #eaecef;
            margin: 40px 0;
        }
        ul, ol {
            padding-left: 25px;
        }
        li {
            margin: 6px 0;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        /* 打印样式 */
        @media print {
            body {
                font-size: 11pt;
                max-width: 100%;
                padding: 0;
            }
            h1 {
                page-break-before: always;
                font-size: 20pt;
            }
            h1:first-of-type {
                page-break-before: avoid;
            }
            pre {
                white-space: pre-wrap;
                word-wrap: break-word;
            }
            a {
                color: #000;
            }
        }
    </style>
</head>
<body>
<h1>VLA 面试手册：从理论到实践</h1>

<blockquote><strong>Vision-Language-Action 完全指南</strong></blockquote>
>
<blockquote>生成日期: 2025年12月03日</blockquote>
>
<blockquote>在线版本: https://github.com/sou350121/vla-interview-handbook</blockquote>

<hr></p>

<h1>目录</h1>

<h1>第一部分：基础架构</h1>

<ul><li>第1章 Transformer vs CNN</li>
<li>第2章 Flash Attention 与推理优化</li>
<li>第3章 多模态模型基础</li>
<li>第4章 VLA 架构总览</li>
</ul>
<h1>第二部分：策略生成与动作表示</h1>

<ul><li>第5章 动作表示方法</li>
<li>第6章 Diffusion Policy</li>
<li>第7章 Action Chunking Transformer (ACT)</li>
<li>第8章 Flow Matching</li>
<li>第9章 FAST 动作序列编码</li>
</ul>
<h1>第三部分：训练技术与优化</h1>

<ul><li>第10章 参数高效微调 (PEFT/LoRA)</li>
<li>第11章 强化学习基础与 RLHF</li>
<li>第12章 知识蒸馏</li>
<li>第13章 自监督学习</li>
<li>第14章 迁移学习与 Co-training</li>
<li>第14章附 Co-training 详解</li>
<li>第15章 量化技术</li>
</ul>
<h1>第四部分：感知与空间智能</h1>

<ul><li>第16章 空间数学基础</li>
<li>第17章 机器人控制方法</li>
<li>第18章 感知技术</li>
<li>第19章 点云与 SLAM</li>
<li>第20章 状态估计</li>
</ul>
<h1>第五部分：抓取与运动规划</h1>

<ul><li>第21章 抓取算法</li>
<li>第22章 运动规划</li>
<li>第23章 触觉 VLA</li>
</ul>
<h1>第六部分：前沿模型解析</h1>

<ul><li>第24章 RDT (Robotics Diffusion Transformer)</li>
<li>第25章 π0.5 解析</li>
<li>第26章 π0.6 解析</li>
<li>第27章 Galaxea G0</li>
<li>第28章 WALL-OSS</li>
</ul>
<h1>第七部分：评估与推理</h1>

<ul><li>第29章 Chain-of-Thought 推理</li>
<li>第30章 评估方法论</li>
<li>第31章 知识隔离</li>
</ul>
<h1>附录</h1>

<ul><li>附录A 数据格式与处理</li>
<li>附录B 文献综述</li>
<li>附录C ASCII 图表速查</li>
</ul>

<hr></p>

<hr></p>

<h1>第一部分：基础架构</h1>

<hr></p>

<h2>第1章 Transformer vs CNN</h2>

<p>在 VLA (Vision-Language-Action) 面试中，理解 Backbone (骨干网络) 的差异至关重要。虽然现在的趋势是 Transformer (ViT) 一统天下，但 CNN (ResNet, EfficientNet) 依然在 RT-1 等经典模型中扮演重要角色。</p>

<h3>1. 核心差异一览表</h3>

<table>
<thead><tr>
<th>特性</th>
<th>CNN (卷积神经网络)</th>
<th>Transformer (自注意力机制)</th>
</tr></thead>
<tbody>
<tr>
<td><strong>核心操作</strong></td>
<td>Convolution (卷积)</td>
<td>Self-Attention (自注意力)</td>
</tr>
<tr>
<td><strong>感受野 (Receptive Field)</strong></td>
<td>局部 (Local) -> 逐渐扩大</td>
<td>全局 (Global)</td>
</tr>
<tr>
<td><strong>归纳偏置 (Inductive Bias)</strong></td>
<td>强 (平移不变性, 局部性)</td>
<td>弱 (需海量数据学习)</td>
</tr>
<tr>
<td><strong>计算复杂度</strong></td>
<td>$O(N)$ (线性)</td>
<td>$O(N^2)$ (平方级)</td>
</tr>
<tr>
<td><strong>并行性</strong></td>
<td>高</td>
<td>极高</td>
</tr>
<tr>
<td><strong>擅长领域</strong></td>
<td>图像纹理, 边缘检测</td>
<td>语义理解, 长距离依赖</td>
</tr>
<tr>
<td><strong>代表模型</strong></td>
<td>ResNet, EfficientNet</td>
<td>ViT, BERT, GPT</td>
</tr>
</tbody></table>

<h3>2. CNN (Convolutional Neural Networks)</h3>

<h4>原理</h4>
CNN 模仿生物视觉皮层，通过<strong>滑动窗口 (Sliding Window)</strong> 提取特征。
<ul><li><strong>局部性 (Locality)</strong>: 每次只看一小块区域 (e.g., 3x3 像素)。</li>
<li><strong>平移不变性 (Translation Invariance)</strong>: 猫在图片左上角还是右下角，识别出的特征是一样的。</li>
<li><strong>层级结构</strong>: 浅层学边缘/纹理，深层学形状/物体。</li>
</ul>
<h4>在 VLA 中的应用</h4>
<ul><li><strong>RT-1</strong>: 使用 <strong>EfficientNet-B3</strong> 作为视觉编码器。</li>
<li><strong>优势</strong>: 训练收敛快，对小数据集友好 (因为有很强的归纳偏置)。</li>
<li><strong>劣势</strong>: 难以捕捉长距离关系 (比如：桌子左边的杯子和桌子右边的壶之间的关系，CNN 需要堆叠很多层才能"看"到两者)。</li>
</ul>
<h3>3. Transformer (Attention Is All You Need)</h3>

<h4>原理</h4>
Transformer 抛弃了卷积，完全依赖 <strong>Self-Attention (自注意力机制)</strong>。
<ul><li><strong>全局感受野</strong>: 每一个 Token (像素块) 都能直接"关注"到图像中的其他所有 Token。</li>
<li><strong>动态权重</strong>: 卷积核的权重是固定的 (训练好后)，而 Attention Map 是根据输入动态生成的。</li>
</ul>
<h4>在 VLA 中的应用</h4>
<ul><li><strong>RT-2 / OpenVLA / Pi0</strong>: 使用 <strong>ViT (Vision Transformer)</strong> 或 <strong>SigLIP</strong>。</li>
<li><strong>优势</strong>:</li>
</ul>    - <strong>多模态统一</strong>: 图像 Patch 和文本 Token 可以被同等对待，直接拼接输入 Transformer。
    - <strong>Scaling Law</strong>: 数据越多，模型越大，效果越好 (由弱归纳偏置决定)。
<ul><li><strong>劣势</strong>: 训练极其昂贵，需要海量数据 (JFT-300M, LAION-5B) 才能超越 CNN。</li>
</ul>
<h3>4. 为什么 VLA 转向 Transformer?</h3>

<p><li> <strong>多模态融合</strong>: 机器人需要同时处理视觉 (Vision) 和语言 (Language)。Transformer 是目前唯一能完美统一这两种模态的架构 (Early Fusion)。</li>
<li> <strong>语义理解</strong>: 机器人不再只是"执行动作"，而是需要"理解环境"。Transformer 在语义提取上远强于 CNN。</li>
<li> <strong>时序建模</strong>: 动作序列 (Action Sequence) 本质上是时间序列。Transformer (GPT 风格) 天生适合处理序列预测问题。</li></p>

<h3>5. 深度解析: ViT & SigLIP 技术细节</h3>
在 OpenVLA 和 Pi0 等现代模型中，ViT (Vision Transformer) 通常搭配 <strong>SigLIP</strong> 预训练目标使用。</p>

<h4>5.1 Vision Transformer (ViT) 核心组件</h4>
ViT 将图像视为一系列 Patch 的序列，完全摒弃了卷积。</p>

<p><li> <strong>Patchify & Linear Projection (切片与线性映射)</strong>:</li>
    - 输入图像 $x \in \mathbb{R}^{H \times W \times C}$ 被切分为 $N$ 个 $P \times P$ 的 Patch $x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$。
    - <strong>公式</strong>:
<pre><code class="math">      z_0 = [x_p^1 E; x_p^2 E; \cdots; x_p^N E] + E_{pos}
</code></pre>
      其中 $E \in \mathbb{R}^{(P^2 \cdot C) \times D}$ 是可学习的线性投影矩阵，$E_{pos} \in \mathbb{R}^{(N+1) \times D}$ 是位置编码。
    - <strong>关键细节</strong>: 这一步等价于一个 <code>Conv2d(in_channels=3, out_channels=D, kernel_size=P, stride=P)</code> 操作。</p>

<p><li> <strong>Positional Embedding Interpolation (位置编码插值)</strong>:</li>
    - <strong>问题</strong>: VLA 任务中，输入图像分辨率可能变化 (e.g., 224x224 -> 384x384)，导致 Patch 数量 $N$ 变化。
    - <strong>解决方案</strong>: 预训练的 1D 位置编码不能直接用。需要将其 reshape 成 2D 网格，进行 <strong>双线性插值 (Bicubic/Bilinear Interpolation)</strong> 到新的尺寸，再展平回 1D。这是 ViT 能处理不同分辨率的关键。</p>

<p><li> <strong>CLS Token vs Average Pooling</strong>:</li>
    - <strong>CLS Token</strong>: 原始 ViT 在序列开头加一个特殊的 <code>[CLS]</code> Token，其输出作为整张图的特征 (BERT 风格)。
    - <strong>Average Pooling (GAP)</strong>: 现代 ViT (如 SigLIP) 往往去掉 CLS Token，直接对所有 Patch 的输出取平均 (Global Average Pooling)。
      - <strong>优势</strong>: 能够利用全图信息，且对 Learning Rate 更鲁棒 (MAP 论文指出 GAP 优于 CLS)。</p>

<h4>5.2 SigLIP (Sigmoid Loss for Language Image Pre-training)</h4>
OpenVLA 的视觉编码器使用的是 <strong>SigLIP</strong> (来自 Google DeepMind)，而非传统的 CLIP。</p>

<h4>1. 为什么不用 CLIP (Softmax Loss)?</h4>
传统的 CLIP 使用 <strong>InfoNCE Loss</strong> (基于 Softmax)，需要维护巨大的负样本对 (Negative Pairs)。
<pre><code class="math">L_{CLIP} = -\frac{1}{N} \sum_{i=1}^N \log \frac{e^{x_i \cdot y_i / \tau}}{\sum_{j=1}^N e^{x_i \cdot y_j / \tau}}
</code></pre>
<ul><li><strong>通信瓶颈</strong>: 分母 $\sum e^{...}$ 需要聚合所有 GPU 上的所有样本 (Global Reduction)。在分布式训练中，这会导致巨大的通信开销。</li>
</ul>
<h4>2. SigLIP 的创新 (Sigmoid Loss)</h4>
SigLIP 将 $N \times N$ 的匹配问题转化为 <strong>$N^2$ 个独立的二分类问题</strong>。
<pre><code class="math">L_{SigLIP} = - \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^N \left[ \mathbb{I}_{i=j} \log \sigma(x_i \cdot y_j / \tau + b) + \mathbb{I}_{i \neq j} \log (1 - \sigma(x_i \cdot y_j / \tau + b)) \right]
</code></pre>
<ul><li><strong>$\mathbb{I}_{i=j}$</strong>: 正样本对 (对角线)，标签为 1。</li>
<li><strong>$\mathbb{I}_{i \neq j}$</strong>: 负样本对 (非对角线)，标签为 0。</li>
<li><strong>优势</strong>:</li>
</ul>    1.  <strong>无需全局同步</strong>: 每个 GPU 只需处理自己手头的负样本，梯度计算是局部的。
    2.  <strong>Batch Size 独立</strong>: 性能不再强依赖于超大 Batch Size (CLIP 需要大 Batch 提供足够负样本，SigLIP 对 Batch 大小不敏感)。</p>

<h4>3. 关键实现细节: Bias Initialization</h4>
SigLIP 引入了一个可学习的 Bias $b$ (通常初始化为 $- \log N$)。
<ul><li><strong>原因</strong>: 在训练初期，正样本极少 (1个)，负样本极多 ($N-1$个)。如果 Bias 为 0，Sigmoid 输出 0.5，会导致巨大的初始 Loss (因为大部分应该是 0)。</li>
<li><strong>Trick</strong>: 初始化 $b = -10$ 或 $- \log N$，强制初始概率接近 0，匹配负样本占主导的先验分布，极大地稳定了训练。</li>
</ul>
<h3>6. 自注意力机制详解 (Self-Attention Deep Dive)</h3>

<h4>6.1 计算公式</h4>

<p>自注意力机制的核心公式：</p>

<pre><code class="math">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
</code></pre>

<p>其中：
<ul><li>$Q = XW_Q$, $K = XW_K$, $V = XW_V$ (线性投影)</li>
<li>$d_k$: Key 的维度 (用于缩放，防止点积过大导致 softmax 饱和)</li>
<li>$X \in \mathbb{R}^{N \times d}$: 输入序列 (N 个 Token，每个 d 维)</li>
</ul>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                   Self-Attention 计算流程                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   输入 X [N, d]                                                 │
│       │                                                         │
│       ├──▶ W_Q ──▶ Q [N, d_k]                                   │
│       ├──▶ W_K ──▶ K [N, d_k]                                   │
│       └──▶ W_V ──▶ V [N, d_v]                                   │
│                                                                 │
│   Step 1: QK^T / √d_k  ──▶  Attention Scores [N, N]             │
│   Step 2: softmax(...)  ──▶  Attention Weights [N, N]           │
│   Step 3: Weights × V   ──▶  Output [N, d_v]                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>6.2 计算复杂度分析</h4>

<table>
<thead><tr>
<th>步骤</th>
<th>操作</th>
<th>时间复杂度</th>
<th>空间复杂度</th>
</tr></thead>
<tbody>
<tr>
<td>线性投影</td>
<td>$XW_Q, XW_K, XW_V$</td>
<td>$O(Nd^2)$</td>
<td>$O(Nd)$</td>
</tr>
<tr>
<td>$QK^T$</td>
<td>矩阵乘法</td>
<td>$O(N^2 d)$</td>
<td>$O(N^2)$ ⚠️</td>
</tr>
<tr>
<td>Softmax</td>
<td>按行归一化</td>
<td>$O(N^2)$</td>
<td>$O(N^2)$</td>
</tr>
<tr>
<td>$\text{Attn} \times V$</td>
<td>矩阵乘法</td>
<td>$O(N^2 d)$</td>
<td>$O(Nd)$</td>
</tr>
<tr>
<td><strong>总计</strong></td>
<td></td>
<td><strong>$O(N^2 d)$</strong></td>
<td><strong>$O(N^2)$</strong></td>
</tr>
</tbody></table>

<p><strong>关键洞察</strong>:
<ul><li><strong>时间复杂度</strong>: $O(N^2 d)$，对序列长度 $N$ 是平方级</li>
<li><strong>空间复杂度</strong>: $O(N^2)$，需要存储完整的 $N \times N$ 注意力矩阵</li>
<li><strong>瓶颈</strong>: 当 $N$ 很大时 (如 VLA 中多帧图像 + 语言 Token)，显存成为主要瓶颈</li>
<li><strong>解决方案</strong>: Flash Attention (参见 <a href="./flash_attention.md">flash_attention.md</a>)</li>
</ul>
<h4>6.3 Multi-Head Attention</h4>

<p>将注意力分成多个"头"，每个头关注不同的特征子空间：</p>

<pre><code class="math">\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
</code></pre>

<pre><code class="math">\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
</code></pre>

<p><strong>优势</strong>:
<ul><li>不同头可以关注不同类型的关系 (位置、语义、纹理等)</li>
<li>参数量不变 ($d_k = d / h$)，但表达能力更强</li>
</ul>
<hr></p>

<h3>7. 面试常见问题</h3>

<p><strong>Q: 自注意力机制是什么？计算复杂度怎么算？</strong>
A: 自注意力让序列中每个位置都能直接关注其他所有位置。计算 $QK^T$ 需要 $O(N^2 d)$ 时间和 $O(N^2)$ 空间，其中 $N$ 是序列长度，$d$ 是特征维度。这是 Transformer 处理长序列时的主要瓶颈。</p>

<p><strong>Q: 为什么要除以 $\sqrt{d_k}$？</strong>
A: 当 $d_k$ 较大时，$QK^T$ 的点积值会很大，导致 softmax 输出接近 one-hot (梯度消失)。除以 $\sqrt{d_k}$ 可以稳定梯度。</p>

<p><strong>Q: 为什么 ViT 需要比 ResNet 更多的数据?</strong>
A: 因为 ViT 缺乏 <strong>归纳偏置 (Inductive Bias)</strong>。CNN 天生知道"相邻像素相关"和"平移不变"，而 ViT 必须从数据中自己学习这些规律。</p>

<p><strong>Q: 什么是 Patchify?</strong>
A: 将一张图片切成一个个小方块 (e.g., 16x16 像素)，拉平成向量，作为 Transformer 的输入 Token。这相当于 NLP 中的分词 (Tokenization)。</p>

<hr></p>

<hr></p>

<h2>第2章 Flash Attention 与推理优化</h2>

<p>Flash Attention 是 Transformer 模型（包括 VLA）在部署时的核心优化技术，解决了标准 Attention 的内存瓶颈问题。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│              Flash Attention vs 标准 Attention                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   标准 Attention (内存瓶颈)           Flash Attention (分块)    │
│                                                                 │
│   ┌───────────────────┐               ┌───────────────────┐     │
│   │      Q · K^T      │               │   Q₁·K₁  Q₁·K₂   │     │
│   │   ┌─────────────┐ │               │  ┌─────┐┌─────┐  │     │
│   │   │             │ │   ────▶       │  │ 块1 ││ 块2 │  │     │
│   │   │   N × N     │ │   分块        │  └─────┘└─────┘  │     │
│   │   │  (巨大!)    │ │               │  ┌─────┐┌─────┐  │     │
│   │   │             │ │               │  │ 块3 ││ 块4 │  │     │
│   │   └─────────────┘ │               │  └─────┘└─────┘  │     │
│   └───────────────────┘               └───────────────────┘     │
│   内存: O(N²) ❌                       内存: O(N) ✅             │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                        内存层级优化                              │
│                                                                 │
│   ┌────────────────┐     ┌────────────────┐                     │
│   │      HBM       │     │     SRAM       │                     │
│   │   (显存)       │     │   (L2 Cache)   │                     │
│   │   24GB+        │ ◀─▶ │    ~20MB       │                     │
│   │   慢 1TB/s     │     │   快 19TB/s    │                     │
│   └────────────────┘     └────────────────┘                     │
│          │                      │                               │
│          │   标准: 多次读写      │   Flash: 一次加载             │
│          │   Q,K ──▶ S ──▶ P    │   全部在 SRAM 计算            │
│          │   每步写回 HBM       │   只写最终结果                 │
│          │                      │                               │
│   结果: 2-4x 加速, 显存 O(N²) → O(N)                            │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. 为什么需要 Flash Attention?</h3>

<h4>标准 Attention 的问题</h4>
标准的 Scaled Dot-Product标准 Attention 计算公式：</p>

<pre><code class="">Attention(Q, K, V) = softmax(Q·K^T / √d_k) · V
</code></pre>

<ul><li><strong>内存瓶颈</strong>: 计算 $QK^T$ 需要存储 $N \times N$ 的注意力矩阵，其中 $N$ 是序列长度。</li>
<li><strong>实例</strong>: 对于 ViT (序列长度 196), $196 \times 196 = 38,416$ 个浮点数。VLA 中若包含多帧图像，$N$ 可能达到数千。</li>
<li><strong>显存占用</strong>: $O(N^2)$ 的内存占用使得长序列推理几乎不可行。</li>
</ul>
<h3>2. Flash Attention 的核心思想</h3>

<h4>Tiling (分块计算)</h4>
Flash Attention 通过 <strong>分块 (Tiling)</strong> 避免实际存储完整的 $QK^T$ 矩阵。</p>

<p><strong>算法流程</strong>:
<li>将 $Q, K, V$ 分割成小块 (Tiles)。</li>
<li>逐块计算 Attention，在 SRAM (L2 Cache) 中完成。</li>
<li>使用 <strong>在线 Softmax (Online Softmax)</strong> 技术增量更新归一化。</li>
<li>最终合并结果，避免将中间结果写回 HBM (High Bandwidth Memory)。</li></p>

<h4>2.1 Kernel Fusion (算子融合): IO-Aware Computing</h4>
Flash Attention 的核心洞察是：<strong>Transformer 的瓶颈不在计算 (FLOPs)，而在显存读写 (HBM IO)</strong>。</p>

<ul><li>  <strong>HBM vs SRAM</strong>:</li>
</ul>    -   <strong>HBM (High Bandwidth Memory)</strong>: 显存，容量大 (24GB+)，但速度慢 (1-2 TB/s)。
    -   <strong>SRAM (L2 Cache)</strong>: 片上缓存，容量小 (几十 MB)，但速度极快 (19 TB/s+)。
<ul><li>  <strong>标准 Attention</strong>: 需要多次读写 HBM。</li>
</ul>    1.  读 $Q, K$ -> 算 $S = QK^T$ -> 写回 HBM ($N^2$)。
    2.  读 $S$ -> 算 $P = \text{softmax}(S)$ -> 写回 HBM ($N^2$)。
    3.  读 $P, V$ -> 算 $O = PV$ -> 写回 HBM。
<ul><li>  <strong>Flash Attention Fusion</strong>:</li>
</ul>    -   将上述所有步骤融合进<strong>同一个 CUDA Kernel</strong>。
    -   数据一旦从 HBM 加载到 SRAM，就在 SRAM 中完成 $QK^T$, Softmax, $PV$ 的所有计算，只把最终结果 $O$ 写回 HBM。
    -   <strong>结果</strong>: HBM 读写量从 $O(N^2)$ 降低到 $O(N)$，尽管 FLOPs 没变，但端到端速度提升了 2-4 倍。</p>

<h4>2.2 Recomputation (重计算): 换取显存的艺术</h4>
在训练时的反向传播 (Backward Pass) 中，通常需要保存前向传播的中间激活值 (Activations) 来计算梯度。</p>

<ul><li>  <strong>标准做法</strong>: 保存巨大的 $N \times N$ 注意力矩阵 $P$。这直接导致了 OOM (Out of Memory)。</li>
<li>  <strong>Flash Attention 做法</strong>:</li>
</ul>    -   <strong>不保存</strong> $P$ 矩阵。
    -   在反向传播时，利用保存在 SRAM 中的 $Q, K, V$ 块，<strong>重新计算</strong>一遍 Attention。
<ul><li>  <strong>为什么更快?</strong></li>
</ul>    -   直觉上，重计算会增加 FLOPs，应该变慢。
    -   但由于 Attention 是 <strong>IO-Bound</strong> (受限于带宽) 的，重计算带来的额外 FLOPs 开销，远小于从 HBM 读取巨大矩阵 $P$ 的时间开销。
    -   <strong>结论</strong>: Recomputation 不仅省了显存，反而因为减少了 IO 而变快了。</p>

<h4>数学推导：在线 Softmax</h4>

<p>标准 Softmax 需要两次扫描序列（一次求和，一次归一化）。Flash Attention 使用增量更新：</p>

<pre><code class="">softmax(x)_i = exp(x_i) / Σ exp(x_j)  (j=1 到 N)
</code></pre>

<p>通过维护运行中的 <strong>最大值 m</strong> 和 <strong>累积和 l</strong>，可以逐块更新：</p>

<pre><code class="">m_new = max(m_old, m_block)</p>

<p>l_new = exp(m_old - m_new) <em> l_old + exp(m_block - m_new) </em> l_block
</code></pre>

<h3>3. 在 VLA 中的应用</h3>

<h4>Wall-X / OpenVLA</h4>
<ul><li><strong>Wall-X</strong>: requirements.txt 中明确依赖 <code>flash-attn==2.7.4</code>。</li>
<li><strong>OpenVLA</strong>: 支持 Flash Attention 2 加速推理，尤其在处理长历史序列时。</li>
</ul>
<h4>Pi0</h4>
<ul><li>Pi0 使用 Flow Matching，推理时需要多步 ODE Solver。Flash Attention 在每一步都能显著减少显存占用。</li>
</ul>
<h4>性能提升</h4>
<ul><li><strong>速度</strong>: 2-4x 加速（相比标准 Attention）。</li>
<li><strong>显存</strong>: 内存占用从 $O(N^2)$ 降至 $O(N)$。</li>
<li><strong>部署</strong>: 使得在消费级 GPU (e.g., RTX 4090) 上部署 7B VLA 成为可能。</li>
</ul>
<h3>4. Flash Attention vs 其他优化</h3>

<table>
<thead><tr>
<th>技术</th>
<th>内存复杂度</th>
<th>精度</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Standard Attention</strong></td>
<td>$O(N^2)$</td>
<td>精确</td>
<td>短序列</td>
</tr>
<tr>
<td><strong>Flash Attention</strong></td>
<td>$O(N)$</td>
<td>精确</td>
<td>长序列，真机推理</td>
</tr>
<tr>
<td><strong>Sparse Attention</strong></td>
<td>$O(N \log N)$</td>
<td>近似</td>
<td>超长文本 (不适合 VLA)</td>
</tr>
<tr>
<td><strong>Linear Attention</strong></td>
<td>$O(N)$</td>
<td>近似</td>
<td>研究阶段</td>
</tr>
</tbody></table>

<h3>5. KV-Cache 推理加速 (KV-Cache for Inference)</h3>

<h4>5.1 问题背景</h4>

<p>在自回归生成 (Autoregressive Generation) 时，每生成一个新 Token 都需要计算 Attention：</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                   无 KV-Cache 的推理                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   生成 Token 1:  计算 K₁, V₁                                    │
│   生成 Token 2:  重新计算 K₁, K₂, V₁, V₂  ← 重复计算!            │
│   生成 Token 3:  重新计算 K₁, K₂, K₃, V₁, V₂, V₃  ← 更多重复!   │
│   ...                                                           │
│   生成 Token N:  重新计算 K₁...K_N, V₁...V_N  ← O(N²) 总计算量   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>5.2 KV-Cache 原理</h4>

<p><strong>核心思想</strong>: 缓存已计算的 Key 和 Value，新 Token 只需计算增量。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                   有 KV-Cache 的推理                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   生成 Token 1:  计算 K₁, V₁ → 存入 Cache                        │
│   生成 Token 2:  只计算 K₂, V₂ → 追加到 Cache                    │
│   生成 Token 3:  只计算 K₃, V₃ → 追加到 Cache                    │
│   ...                                                           │
│   生成 Token N:  只计算 K_N, V_N → O(N) 总计算量                 │
│                                                                 │
│   Attention 计算:                                               │
│   Q_new (1, d) × K_cache^T (N, d) → Scores (1, N)               │
│   Scores × V_cache (N, d) → Output (1, d)                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>5.3 加速效果</h4>

<table>
<thead><tr>
<th>指标</th>
<th>无 KV-Cache</th>
<th>有 KV-Cache</th>
</tr></thead>
<tbody>
<tr>
<td>每 Token 计算量</td>
<td>$O(N^2 d)$</td>
<td>$O(Nd)$</td>
</tr>
<tr>
<td>生成 N 个 Token</td>
<td>$O(N^3 d)$</td>
<td>$O(N^2 d)$</td>
</tr>
<tr>
<td>N=1000 时加速比</td>
<td>1x</td>
<td><strong>~1000x</strong></td>
</tr>
</tbody></table>

<h4>5.4 显存代价</h4>

<p>KV-Cache 需要额外显存存储历史 K 和 V：</p>

<pre><code class="math">\text{KV-Cache 显存} = 2 \times L \times N \times d \times \text{batch\_size} \times \text{bytes}
</code></pre>

<ul><li>$L$: Transformer 层数</li>
<li>$N$: 序列长度</li>
<li>$d$: 隐藏维度</li>
<li>Factor 2: K 和 V 各一份</li>
</ul>
<strong>示例 (Llama-7B, FP16)</strong>:
<ul><li>$L=32, d=4096, N=2048, \text{batch}=1$</li>
<li>KV-Cache = $2 \times 32 \times 2048 \times 4096 \times 2 \text{ bytes} \approx 1 \text{ GB}$</li>
</ul>
<h4>5.5 KV-Cache 优化技术</h4>

<table>
<thead><tr>
<th>技术</th>
<th>原理</th>
<th>节省比例</th>
</tr></thead>
<tbody>
<tr>
<td><strong>GQA (Grouped Query Attention)</strong></td>
<td>多个 Q 头共享 K/V 头</td>
<td>4-8x</td>
</tr>
<tr>
<td><strong>MQA (Multi-Query Attention)</strong></td>
<td>所有 Q 头共享一组 K/V</td>
<td>更激进</td>
</tr>
<tr>
<td><strong>Paged Attention (vLLM)</strong></td>
<td>类似虚拟内存，按需分配</td>
<td>动态优化</td>
</tr>
<tr>
<td><strong>KV-Cache 量化</strong></td>
<td>INT8/INT4 存储</td>
<td>2-4x</td>
</tr>
</tbody></table>

<hr></p>

<h3>6. 面试常见问题</h3>

<p><strong>Q: Flash Attention 的原理是什么？</strong>
A: 三个关键技术：
<li><strong>Tiling (分块)</strong>: 将 $QK^T$ 分成小块计算，避免存储完整 $N \times N$ 矩阵</li>
<li><strong>Kernel Fusion</strong>: 将 $QK^T \to \text{softmax} \to \times V$ 融合进单个 CUDA Kernel，减少 HBM 读写</li>
<li><strong>Online Softmax</strong>: 增量更新归一化常数，支持分块计算</li>
结果: 内存 $O(N^2) \to O(N)$，速度 2-4x 加速。</p>

<p><strong>Q: KV-Cache 如何加速推理？</strong>
A: 在自回归生成时，缓存已计算的 K 和 V，新 Token 只需计算增量 $K_{new}, V_{new}$。将每 Token 计算量从 $O(N^2 d)$ 降到 $O(Nd)$，N=1000 时约 1000 倍加速。代价是额外显存 $O(LNd)$。</p>

<p><strong>Q: Flash Attention 是如何做到精确计算的？</strong>
A: 通过在线 Softmax 和分块计算，Flash Attention 在数学上等价于标准 Attention，只是改变了计算顺序和内存访问模式，没有引入任何近似。</p>

<p><strong>Q: 为什么不能直接用 Sparse Attention?</strong>
A: VLA 的注意力模式通常是密集的（视觉 Patch 之间关系紧密），稀疏假设不成立。Flash Attention 保持了密集计算，只是优化了内存。</p>

<p><strong>Q: Flash Attention 对训练和推理都有效吗？</strong>
A: 是的。训练时通过 Recomputation 节省显存，推理时通过 Kernel Fusion 加速。Wall-X 等模型在两者都使用。</p>

<hr></p>

<hr></p>

<h2>第3章 多模态模型基础</h2>

<blockquote><strong>核心概念</strong>: 多模态模型 (Multimodal Models) 是指能够同时处理多种数据模态（如视觉、语言、音频、触觉等）的深度学习模型。在 VLA 领域，多模态能力是连接"看"、"说"、"做"的关键。</blockquote>

<h3>1. 为什么需要多模态? (Why Multimodal?)</h3>

<h4>1.1 机器人的感知需求</h4>

<p>机器人在真实世界中需要同时处理多种信息：</p>

<table>
<thead><tr>
<th>模态</th>
<th>来源</th>
<th>作用</th>
</tr></thead>
<tbody>
<tr>
<td><strong>视觉 (Vision)</strong></td>
<td>RGB 相机、深度相机</td>
<td>理解场景、识别物体</td>
</tr>
<tr>
<td><strong>语言 (Language)</strong></td>
<td>语音指令、文本</td>
<td>理解任务意图</td>
</tr>
<tr>
<td><strong>本体感知 (Proprioception)</strong></td>
<td>关节编码器、IMU</td>
<td>感知自身状态</td>
</tr>
<tr>
<td><strong>触觉 (Tactile)</strong></td>
<td>触觉传感器</td>
<td>感知接触力、纹理</td>
</tr>
<tr>
<td><strong>音频 (Audio)</strong></td>
<td>麦克风</td>
<td>环境声音、语音交互</td>
</tr>
</tbody></table>

<h4>1.2 单模态的局限性</h4>

<ul><li><strong>仅视觉</strong>: 无法理解抽象指令（"把那个危险的东西拿走"）</li>
<li><strong>仅语言</strong>: 无法定位具体物体（"桌上的红色杯子"在哪？）</li>
<li><strong>缺乏本体感知</strong>: 不知道机械臂当前姿态，无法闭环控制</li>
</ul>
<h4>1.3 多模态的优势</h4>

<pre><code class="math">\text{多模态理解} &gt; \sum \text{单模态理解}
</code></pre>

<ul><li><strong>语义接地 (Grounding)</strong>: 将语言概念与视觉实体绑定</li>
<li><strong>跨模态推理</strong>: "红色的东西"（语言）→ 锁定红色物体（视觉）→ 抓取动作</li>
<li><strong>鲁棒性</strong>: 一个模态失效时，其他模态可以补偿</li>
</ul>
<h3>2. 多模态架构演进 (Architecture Evolution)</h3>

<h4>2.1 早期：双塔模型 (Dual-Encoder)</h4>

<pre><code class="">          ┌─────────────┐      ┌─────────────┐
图像 ────▶│  Image      │      │   Text      │◀──── 文本
          │  Encoder    │      │   Encoder   │
          │  (ResNet)   │      │   (BERT)    │
          └──────┬──────┘      └──────┬──────┘
                 │                    │
                 ▼                    ▼
              img_emb              text_emb
                 │                    │
                 └────────┬───────────┘
                          │
                    Cosine Similarity
</code></pre>

<p><strong>代表</strong>: CLIP, ALIGN
<strong>特点</strong>: 图像和文本独立编码，通过对比学习对齐到同一空间
<strong>局限</strong>: 无法进行深度的跨模态交互</p>

<h4>2.2 中期：融合编码器 (Fusion Encoder)</h4>

<pre><code class="">          ┌─────────────┐      ┌─────────────┐
图像 ────▶│  Image      │      │   Text      │◀──── 文本
          │  Encoder    │      │   Encoder   │
          └──────┬──────┘      └──────┬──────┘
                 │                    │
                 └────────┬───────────┘
                          ▼
                 ┌─────────────────┐
                 │  Fusion Module  │
                 │  (Cross-Attn)   │
                 └────────┬────────┘
                          ▼
                   Fused Features
</code></pre>

<p><strong>代表</strong>: ViLBERT, LXMERT, UNITER
<strong>特点</strong>: 通过 Cross-Attention 实现深度交互
<strong>改进</strong>: 支持更复杂的多模态推理</p>

<h4>2.3 现代：统一解码器 (Unified Decoder)</h4>

<pre><code class="">          ┌─────────────┐
图像 ────▶│  Vision     │──┐
          │  Encoder    │  │
          └─────────────┘  │
                           │   ┌─────────────────────┐
                           ├──▶│     LLM Decoder     │──▶ 输出
                           │   │  (Unified Token)    │
          ┌─────────────┐  │   └─────────────────────┘
文本 ────▶│  Tokenizer  │──┘
          └─────────────┘
</code></pre>

<p><strong>代表</strong>: Flamingo, LLaVA, GPT-4V, Gemini
<strong>特点</strong>: 将视觉特征作为"虚拟 Token"输入到 LLM
<strong>优势</strong>: 利用 LLM 的强大推理能力，支持任意输入输出组合</p>

<h3>3. VLA 中的多模态融合策略 (Fusion Strategies in VLA)</h3>

<h4>3.1 早期融合 (Early Fusion)</h4>

<p>在特征提取阶段就进行融合。</p>

<pre><code class="python">class EarlyFusion(nn.Module):
    def __init__(self):
        self.vision_proj = nn.Linear(vision_dim, hidden_dim)
        self.language_proj = nn.Linear(language_dim, hidden_dim)
        self.proprio_proj = nn.Linear(proprio_dim, hidden_dim)
        
    def forward(self, image_feat, text_feat, proprio):
        # 直接拼接
        fused = torch.cat([
            self.vision_proj(image_feat),
            self.language_proj(text_feat),
            self.proprio_proj(proprio)
        ], dim=1)  # [B, L_v + L_t + 1, D]
        return fused
</code></pre>

<p><strong>优点</strong>: 简单高效
<strong>缺点</strong>: 不同模态的特征尺度可能不匹配</p>

<h4>3.2 中期融合 (Mid Fusion / Cross-Attention)</h4>

<p>通过注意力机制动态融合。</p>

<pre><code class="python">class CrossModalAttention(nn.Module):
    def __init__(self, hidden_dim, num_heads=8):
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        
    def forward(self, query_feat, context_feat):
        """
        query_feat: 需要被增强的特征 (e.g., 动作 query)
        context_feat: 提供上下文的特征 (e.g., 图像 + 语言)
        """
        # Query attends to Context
        attended, attn_weights = self.cross_attn(
            query=query_feat,
            key=context_feat,
            value=context_feat
        )
        return attended, attn_weights
</code></pre>

<p><strong>代表</strong>: RT-1 (TokenLearner)，Octo
<strong>优点</strong>: 动态学习模态间关系
<strong>缺点</strong>: 计算开销大</p>

<h4>3.3 晚期融合 (Late Fusion)</h4>

<p>各模态独立处理后再合并决策。</p>

<pre><code class="python">class LateFusion(nn.Module):
    def __init__(self):
        self.vision_policy = VisionPolicy()
        self.language_policy = LanguagePolicy()
        self.fusion_head = nn.Linear(hidden_dim * 2, action_dim)
        
    def forward(self, image, text):
        vision_out = self.vision_policy(image)
        language_out = self.language_policy(text)
        
        # 决策层融合
        fused = torch.cat([vision_out, language_out], dim=-1)
        action = self.fusion_head(fused)
        return action
</code></pre>

<p><strong>优点</strong>: 各模态可以独立优化
<strong>缺点</strong>: 无法学习复杂的跨模态交互</p>

<h4>3.4 VLA 中的主流方案：FiLM 调制</h4>

<p><strong>FiLM (Feature-wise Linear Modulation)</strong> 是 VLA 中最常用的条件注入方式。</p>

<pre><code class="python">class FiLM(nn.Module):
    """Feature-wise Linear Modulation"""
    def __init__(self, cond_dim, feature_dim):
        self.gamma = nn.Linear(cond_dim, feature_dim)  # Scale
        self.beta = nn.Linear(cond_dim, feature_dim)   # Shift
        
    def forward(self, feature, condition):
        """
        feature: 要调制的特征 [B, L, D]
        condition: 条件信息 [B, C]
        """
        gamma = self.gamma(condition).unsqueeze(1)  # [B, 1, D]
        beta = self.beta(condition).unsqueeze(1)
        
        # 调制: γ * feature + β
        return gamma * feature + beta
</code></pre>

<p><strong>应用场景</strong>:
<ul><li><strong>RT-1</strong>: 语言特征通过 FiLM 调制视觉特征</li>
<li><strong>Diffusion Policy</strong>: 时间步 $t$ 通过 FiLM 注入到 U-Net</li>
</ul>
<h3>4. 核心视觉编码器 (Vision Encoders)</h3>

<h4>4.1 ViT (Vision Transformer)</h4>

<pre><code class="">图像 [H, W, 3] 
    │
    ▼ Patch Embedding (16x16)
[N_patches, D] where N = (H/16) * (W/16)
    │
    ▼ + Position Embedding
    │
    ▼ Transformer Encoder (L layers)
    │
    ▼
[CLS] token 或 全局平均池化
</code></pre>

<p><strong>特点</strong>:
<ul><li>将图像切分为 Patch (如 16x16)</li>
<li>每个 Patch 作为一个 Token</li>
<li>通过 Self-Attention 建模全局关系</li>
</ul>
<h4>4.2 SigLIP (Sigmoid Loss for Language-Image Pre-training)</h4>

<p><strong>改进 CLIP</strong>:
<ul><li>使用 Sigmoid 替代 Softmax (更好的批量对比学习)</li>
<li>支持更大的 batch size</li>
<li>VLA 首选的视觉编码器 (OpenVLA, RDT)</li>
</ul>
<h4>4.3 DINOv2 (Self-supervised Vision Transformer)</h4>

<p><strong>特点</strong>:
<ul><li>自监督预训练，无需标签</li>
<li>强大的低层视觉特征 (边缘、纹理)</li>
<li>适合需要精确空间信息的任务</li>
</ul>
<h4>4.4 对比与选择</h4>

<table>
<thead><tr>
<th>编码器</th>
<th>预训练方式</th>
<th>特点</th>
<th>VLA 应用</th>
</tr></thead>
<tbody>
<tr>
<td><strong>ResNet</strong></td>
<td>监督学习</td>
<td>高效，适合 CNN 策略</td>
<td>RT-1, Diffusion Policy</td>
</tr>
<tr>
<td><strong>ViT</strong></td>
<td>监督/自监督</td>
<td>全局建模强</td>
<td>通用</td>
</tr>
<tr>
<td><strong>CLIP/SigLIP</strong></td>
<td>对比学习</td>
<td>语义对齐好</td>
<td>OpenVLA, RDT</td>
</tr>
<tr>
<td><strong>DINOv2</strong></td>
<td>自监督</td>
<td>空间特征强</td>
<td>精细操作</td>
</tr>
</tbody></table>

<h3>5. 语言编码器 (Language Encoders)</h3>

<h4>5.1 BERT-style (Encoder-only)</h4>

<pre><code class="python">from transformers import BertModel</p>

<p>text = "pick up the red cup"
inputs = tokenizer(text, return_tensors="pt")
outputs = bert_model(**inputs)</p>

<p>text_embedding = outputs.last_hidden_state[:, 0, :]  # [B, D]
</code></pre>

<p><strong>适用</strong>: 理解型任务，指令嵌入</p>

<h4>5.2 T5-style (Encoder-Decoder)</h4>

<p><strong>适用</strong>: 需要生成文本的任务 (如 CoT 推理)</p>

<h4>5.3 LLM-style (Decoder-only)</h4>

<p><strong>代表</strong>: Llama, Gemma, Qwen
<strong>适用</strong>: 现代 VLA 的标准选择，利用强大的 In-context Learning</p>

<h3>6. 投影层设计 (Projector Design)</h3>

<p>将视觉特征映射到语言空间是 VLA 的关键。</p>

<h4>6.1 简单 MLP</h4>

<pre><code class="python">class MLPProjector(nn.Module):
    def __init__(self, vision_dim, language_dim):
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, language_dim),
            nn.GELU(),
            nn.Linear(language_dim, language_dim)
        )
    
    def forward(self, vision_feat):
        return self.proj(vision_feat)
</code></pre>

<h4>6.2 Perceiver Resampler (Flamingo)</h4>

<pre><code class="python">class PerceiverResampler(nn.Module):
    """将可变数量的视觉 Token 压缩为固定数量"""
    def __init__(self, num_latents=64):
        self.latents = nn.Parameter(torch.randn(num_latents, hidden_dim))
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        
    def forward(self, vision_tokens):
        # vision_tokens: [B, N_patches, D] (N_patches 可变)
        # 输出: [B, num_latents, D] (固定)
        
        latents = self.latents.unsqueeze(0).expand(B, -1, -1)
        output, _ = self.cross_attn(
            query=latents,
            key=vision_tokens,
            value=vision_tokens
        )
        return output  # [B, 64, D]
</code></pre>

<p><strong>优势</strong>: 控制视觉 Token 数量，减少 LLM 的计算负担</p>

<h4>6.3 Q-Former (BLIP-2)</h4>

<p>使用可学习的 Query 从视觉编码器中提取与任务相关的特征。</p>

<h3>7. 实战：构建简单的多模态 VLA</h3>

<pre><code class="python">import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer</p>

<p>class SimpleMultimodalVLA(nn.Module):
    def __init__(
        self,
        vision_encoder_name="google/siglip-base-patch16-224",
        language_model_name="meta-llama/Llama-2-7b-hf",
        action_dim=7,
        chunk_size=16
    ):
        super().__init__()
        
        # 视觉编码器 (冻结)
        self.vision_encoder = AutoModel.from_pretrained(vision_encoder_name)
        for param in self.vision_encoder.parameters():
            param.requires_grad = False
        
        # 投影层 (可训练)
        vision_dim = self.vision_encoder.config.hidden_size
        language_dim = 4096  # Llama 2 hidden dim
        self.vision_projector = nn.Sequential(
            nn.Linear(vision_dim, language_dim),
            nn.GELU(),
            nn.Linear(language_dim, language_dim)
        )
        
        # 语言模型 (LoRA 微调)
        self.language_model = AutoModel.from_pretrained(
            language_model_name,
            load_in_4bit=True  # QLoRA
        )
        
        # 动作头 (可训练)
        self.action_head = nn.Sequential(
            nn.Linear(language_dim, language_dim // 2),
            nn.ReLU(),
            nn.Linear(language_dim // 2, action_dim * chunk_size)
        )
        
        self.chunk_size = chunk_size
        self.action_dim = action_dim
    
    def forward(self, images, input_ids, attention_mask):
        """
        images: [B, C, H, W]
        input_ids: [B, L]
        attention_mask: [B, L]
        """
        batch_size = images.shape[0]
        
        # 1. 视觉编码
        with torch.no_grad():
            vision_outputs = self.vision_encoder(images)
            vision_features = vision_outputs.last_hidden_state  # [B, N_patches, D_v]
        
        # 2. 投影到语言空间
        vision_tokens = self.vision_projector(vision_features)  # [B, N_patches, D_l]
        
        # 3. 获取语言嵌入
        text_embeds = self.language_model.get_input_embeddings()(input_ids)
        
        # 4. 拼接 [Vision Tokens | Text Tokens]
        inputs_embeds = torch.cat([vision_tokens, text_embeds], dim=1)
        
        # 5. 通过语言模型
        outputs = self.language_model(
            inputs_embeds=inputs_embeds,
            output_hidden_states=True
        )
        
        # 6. 取最后一个 hidden state 作为动作条件
        last_hidden = outputs.hidden_states[-1][:, -1, :]  # [B, D_l]
        
        # 7. 预测动作
        actions = self.action_head(last_hidden)  # [B, action_dim * chunk_size]
        actions = actions.view(batch_size, self.chunk_size, self.action_dim)
        
        return actions
</code></pre>

<h3>8. 面试高频问题 (Q&A)</h3>

<p><strong>Q1: CLIP 和 SigLIP 的区别是什么？</strong></p>

<p>A:
<ul><li><strong>损失函数</strong>: CLIP 使用 Softmax + Cross-Entropy (InfoNCE)，SigLIP 使用 Sigmoid + Binary CE</li>
<li><strong>batch 依赖</strong>: CLIP 的 Softmax 需要对比 batch 内所有样本，SigLIP 的 Sigmoid 每对独立计算</li>
<li><strong>扩展性</strong>: SigLIP 更适合大 batch 训练，负样本利用更高效</li>
</ul>
<strong>Q2: 为什么 VLA 普遍选择 Decoder-only LLM 而不是 BERT？</strong></p>

<p>A:
<ul><li><strong>生成能力</strong>: Decoder-only 天然支持自回归生成（包括动作 Token）</li>
<li><strong>In-context Learning</strong>: 可以通过 Prompt 引导模型理解新任务</li>
<li><strong>规模效应</strong>: 大规模 LLM (7B+) 主要是 Decoder-only 架构，可以直接复用</li>
</ul>
<strong>Q3: 多模态融合中 Early / Mid / Late Fusion 如何选择？</strong></p>

<p>A:
<ul><li><strong>Early Fusion</strong>: 数据模态相似度高（如多相机图像）</li>
<li><strong>Mid Fusion (Cross-Attention)</strong>: 需要动态建模模态间关系（VLA 首选）</li>
<li><strong>Late Fusion</strong>: 各模态任务独立性强，或需要模块化解释性</li>
</ul>
<strong>Q4: 视觉 Token 数量如何选择？</strong></p>

<p>A:
<ul><li><strong>多了</strong>: LLM 计算开销大，长序列 Attention 变慢</li>
<li><strong>少了</strong>: 丢失空间细节，影响精细操作</li>
<li><strong>常见选择</strong>: 256 tokens (16x16 patches @ 224px)，或使用 Perceiver Resampler 压缩到 64</li>
</ul>
<strong>Q5: 为什么要冻结视觉编码器？</strong></p>

<p>A:
<ul><li><strong>防止灾难性遗忘</strong>: 视觉编码器的预训练特征很重要</li>
<li><strong>计算效率</strong>: 减少可训练参数</li>
<li><strong>数据效率</strong>: 机器人数据少，全量训练容易过拟合</li>
<li><strong>例外</strong>: 如果视觉任务差异大（如从 ImageNet 迁移到内窥镜），可能需要微调</li>
</ul>
<strong>Q6: 如果视觉模块误判，如何通过语言纠错？</strong></p>

<p>A: 这是多模态 VLA 的核心优势之一，有以下几种机制：</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                   视觉误判 → 语言纠错机制                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   场景: 视觉模块误判 "红色杯子" 为 "橙色杯子"                    │
│                                                                 │
│   方案 1: 闭环语言反馈 (Human-in-the-Loop)                      │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  用户: "不对，是红色的那个"                               │   │
│   │  VLA: 重新定位 → 修正目标                                │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│   方案 2: Chain-of-Thought 自检                                 │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  VLA 输出: "我看到一个橙色物体..."                        │   │
│   │  用户指令: "抓红色杯子"                                   │   │
│   │  CoT 推理: "指令说红色，但我识别为橙色，可能有误"          │   │
│   │  动作: 请求确认 或 重新感知                              │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│   方案 3: 多模态一致性检查                                      │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  计算: sim(语言描述 Embedding, 视觉特征 Embedding)        │   │
│   │  如果 sim &lt; threshold: 触发重新感知/询问                  │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│   方案 4: 主动询问 (Uncertainty-aware)                          │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  VLA: "你是指这个吗？" (显示候选物体)                     │   │
│   │  用户: "是的" / "不是，是左边那个"                        │   │
│   └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>实现要点</strong>:
<li><strong>语义接地 (Grounding)</strong>: 语言指令必须与视觉检测结果绑定，而非独立处理</li>
<li><strong>置信度输出</strong>: 视觉模块输出检测置信度，低置信度时触发纠错机制</li>
<li><strong>多轮对话</strong>: VLA 需要支持多轮交互，而非单次指令执行</li>
<li><strong>CoT 推理</strong>: 显式输出推理过程，便于发现矛盾 (参见 <a href="./chain_of_thought.md">chain_of_thought.md</a>)</li></p>

<h3>9. 参考资源 (References)</h3>

<ul><li><strong>CLIP</strong>: <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></li>
<li><strong>LLaVA</strong>: <a href="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning</a></li>
<li><strong>Flamingo</strong>: <a href="https://arxiv.org/abs/2204.14198">A Visual Language Model for Few-Shot Learning</a></li>
<li><strong>SigLIP</strong>: <a href="https://arxiv.org/abs/2303.15343">Sigmoid Loss for Language Image Pre-Training</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第4章 VLA 架构总览</h2>

<p>本章节深入解析 Vision-Language-Action (VLA) 模型的核心架构演进，从 Google 的 RT 系列到开源的 OpenVLA，再到最新的 Physical Intelligence (Pi) 模型。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    VLA 架构演进路线图                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   RT-1 (2022)              RT-2 (2023)              π0 (2024)   │
│   ┌─────────┐              ┌─────────┐              ┌─────────┐ │
│   │EffNet   │              │ PaLI-X  │              │ Gemma   │ │
│   │   +     │   ────▶      │  55B    │   ────▶      │  +Flow  │ │
│   │Tokenize │              │ VLM+Act │              │ Matching│ │
│   └─────────┘              └─────────┘              └─────────┘ │
│     小模型                   大VLM                    高效推理   │
│     机器人数据               +互联网数据               +精细控制  │
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                    通用 VLA 架构模板                     │   │
│   │                                                         │   │
│   │    📷 Image          📝 Language                        │   │
│   │       │                  │                              │   │
│   │       ▼                  ▼                              │   │
│   │  ┌─────────┐       ┌───────────┐                        │   │
│   │  │ Vision  │       │ Language  │                        │   │
│   │  │ Encoder │       │ Encoder   │                        │   │
│   │  │(ViT/CNN)│       │(LLM/BERT) │                        │   │
│   │  └────┬────┘       └─────┬─────┘                        │   │
│   │       │                  │                              │   │
│   │       └────────┬─────────┘                              │   │
│   │                │ Fusion                                 │   │
│   │                ▼                                        │   │
│   │       ┌─────────────────┐                               │   │
│   │       │  Transformer    │                               │   │
│   │       │    Backbone     │                               │   │
│   │       └────────┬────────┘                               │   │
│   │                │                                        │   │
│   │                ▼                                        │   │
│   │       ┌─────────────────┐                               │   │
│   │       │   Action Head   │                               │   │
│   │       │ Token/Diff/Flow │                               │   │
│   │       └────────┬────────┘                               │   │
│   │                │                                        │   │
│   │                ▼                                        │   │
│   │          🦾 Robot Action                                │   │
│   └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. RT-1 (Robotics Transformer 1)</h3>
<blockquote><strong>核心思想</strong>: 将机器人控制建模为 Token 生成问题，使用 Transformer 处理多模态输入。</blockquote>

<ul><li><strong>架构</strong>: EfficientNet (Vision) + FiLM (Language conditioning) + TokenLearner + Transformer.</li>
<li><strong>Action Tokenization</strong>:</li>
</ul>    - 将连续的动作空间 (x, y, z, roll, pitch, yaw, gripper) 离散化为 256 个 bins。
    - 输出是一个离散的 Token 序列，代表动作维度。
<ul><li><strong>关键点</strong>:</li>
</ul>    - <strong>TokenLearner</strong>: 显著减少了视觉 Token 的数量 (e.g., 81 -> 8)，提高了推理速度。
    - <strong>数据规模</strong>: 130k episodes (17 months of data).
    - <strong>局限性</strong>: 泛化能力有限，主要依赖于大规模收集的机器人数据，缺乏互联网知识的迁移。</p>

<h3>2. RT-2 (Robotics Transformer 2)</h3>
<blockquote><strong>核心思想</strong>: VLA = VLM + Action Tokens. 直接微调预训练的 VLM (如 PaLI-X, PaLM-E) 用于机器人控制。</blockquote>

<ul><li><strong>架构</strong>: 基于大规模 VLM (Vision-Language Model) 微调。</li>
<li><strong>Co-fine-tuning</strong>:</li>
</ul>    - 混合训练：互联网 VQA 数据 + 机器人操作数据。
    - 机器人动作被表示为特殊的文本 Token (e.g., "1 128 90 ...")，与自然语言 Token 共享词表。
<ul><li><strong>关键点</strong>:</li>
</ul>    - <strong>涌现能力 (Emergent Capabilities)</strong>: 能够理解未见过的指令 (e.g., "pick up the extinct animal" -> 抓恐龙玩具)，得益于 VLM 的语义理解能力。
    - <strong>Chain-of-Thought</strong>: 支持简单的推理步骤。
    - <strong>局限性</strong>: 推理速度慢 (大模型)，难以在边缘端实时运行。</p>

<h3>3. OpenVLA / Octo</h3>
<blockquote><strong>核心思想</strong>: 开源、高效、基于 Diffusion 或 Llama 的 VLA 策略。</blockquote>

<h4>Octo</h4>
<ul><li><strong>架构</strong>: 基于 Diffusion Policy 的 Transformer 架构。</li>
<li><strong>特点</strong>: 支持多种观察空间 (Proprioception, Images) 和动作空间。</li>
<li><strong>训练</strong>: 在 Open X-Embodiment (OXE) 数据集上训练。</li>
</ul>
<h4>OpenVLA</h4>
<ul><li><strong>架构</strong>: 基于 Llama 2 (7B) + DINOv2 / SigLIP (Vision Encoder)。</li>
<li><strong>Action Head</strong>:</li>
</ul>    - 并没有直接输出文本 Token，而是使用专门的 Action Head (Linear Layer) 预测去离散化的动作 Token。
    - 使用 <strong>Action Detokenization</strong> 还原为连续动作。
<ul><li><strong>优化</strong>: 支持 4-bit 量化 (QLoRA) 训练和推理，适合消费级显卡。</li>
</ul>
<h3>4. Physical Intelligence (Pi) Models</h3>
<blockquote><strong>核心思想</strong>: 通用机器人基础模型 (Generalist Robot Foundation Models)，强调跨形态 (Cross-Embodiment) 和物理世界的理解。</blockquote>

<h4>π0 (Pi-Zero)</h4>
<ul><li><strong>定位</strong>: 基础 VLA 模型，旨在解决通用的物理操作问题。<strong>已开源 (Open Weights)</strong>。</li>
<li><strong>架构特点</strong>:</li>
</ul>    - 类似于 RT 系列，但更强调对物理动力学的理解。
    - 可能采用了更高效的 Action Tokenizer，以适应高频控制需求。
<ul><li><strong>数据</strong>: 混合了多种机器人的数据 (Arms, Quadrupeds, Humanoids)。</li>
</ul>
<h4>π0.5 (Pi-Zero-Point-Five) - April 2025</h4>
<blockquote><strong><a href="./pi0_5_dissection.md">Deep Dive: Pi0.5 模型解剖</a></strong></blockquote>

<ul><li><strong>核心升级</strong>: <strong>Open-world Generalization</strong> (开放世界泛化) 与 <strong>Hierarchical Inference</strong> (分层推理)。</li>
<li><strong>架构特点</strong>:</li>
</ul>    - <strong>统一模型 (Unified Model)</strong>: 同时负责高层语义规划 (Subtask Prediction) 和底层电机控制 (Motor Control)。模型"自言自语"下一步要做什么，然后执行。
    - <strong>异构数据训练</strong>: 引入了大量的互联网视频数据 (YouTube) 和模拟数据，通过 Co-training 实现对新环境 (如从未见过的厨房) 的适应。
    - <strong>混合架构</strong>: 预训练阶段可能使用离散 Token (FAST tokenizer) 以提高效率，推理阶段使用 Flow Matching 生成连续动作。</p>

<h4>π0.6 & π*0.6 (Pi-Star) - November 2025</h4>
<blockquote><strong><a href="./pi0_6_dissection.md">Deep Dive: Pi0.6 模型解剖</a></strong></blockquote>

<ul><li><strong>核心升级</strong>: <strong>RL (Reinforcement Learning) 强化</strong> 与 <strong>Recap 算法</strong>。</li>
<li><strong>Backbone</strong>: 升级为 <strong>5B Parameter VLM</strong>，增强了对复杂指令和环境的理解。</li>
<li><strong>π*0.6 (Pi-Star)</strong>:</li>
</ul>    - <strong>Recap 算法</strong>: 一种 Offline RL 方法。模型通过"复盘" (Recap) 过去的成功与失败经验进行自我提升，而不仅仅是模仿 (BC)。
    - <strong>性能飞跃</strong>: 在长序列任务 (如折叠衣物、组装纸箱) 上，吞吐量翻倍，失败率降低 2x。
    - <strong>Self-Improvement</strong>: 具备在真机运行中持续学习的能力。
<ul><li><strong>Action Expert</strong>: 引入了专门的动作专家模块，专门处理精细操作，解决了大语言模型在精细运动控制上的"手笨"问题。</li>
</ul>
<h3>5. 模型对比总结 (Model Comparison)</h3>

<table>
<thead><tr>
<th>模型</th>
<th>基础架构</th>
<th>Action 输出</th>
<th>训练数据</th>
<th>优势</th>
<th>劣势</th>
</tr></thead>
<tbody>
<tr>
<td><strong>RT-1</strong></td>
<td>EfficientNet + Transformer</td>
<td>Discrete Tokens</td>
<td>Robot Data Only</td>
<td>推理快，稳定性高</td>
<td>泛化差，无语义推理</td>
</tr>
<tr>
<td><strong>RT-2</strong></td>
<td>PaLI-X / PaLM-E</td>
<td>Text Tokens</td>
<td>Web + Robot Data</td>
<td>语义理解强，Zero-shot</td>
<td>推理慢，闭源</td>
</tr>
<tr>
<td><strong>OpenVLA</strong></td>
<td>Llama 2 + SigLIP</td>
<td>Action Head</td>
<td>OXE Dataset</td>
<td>开源，支持量化，易部署</td>
<td>性能略逊于闭源 SOTA</td>
</tr>
<tr>
<td><strong>WALL-OSS</strong></td>
<td>Qwen2.5 VLMoE</td>
<td><strong>Dual Branches</strong> (Flow + FAST)</td>
<td>Cross-Embodiment</td>
<td><strong>COT 推理</strong>，双分支，<strong>已开源</strong></td>
<td>训练资源需求高</td>
</tr>
<tr>
<td><strong>π0.6</strong></td>
<td>Gemma 3 + Action Expert</td>
<td>Specialized</td>
<td>Cross-Embodiment + RL</td>
<td>泛化强，精细操作好，<strong>已开源</strong></td>
<td>训练极其昂贵</td>
</tr>
</tbody></table>

<h3>面试高频考点</h3>
<li><strong>Action Tokenization</strong>: 为什么要离散化？连续回归 (Regression) 有什么问题？(答: 多模态分布处理能力)</li>
<li><strong>Co-fine-tuning</strong>: 为什么要混合互联网数据？(答: 保持 VLM 的语义能力，防止灾难性遗忘)</li>
<li><strong>Sim-to-Real</strong>: OpenVLA 如何在真机上部署？(答: 量化，VLM 蒸馏)</li></p>

<hr></p>

<hr></p>

<h1>第二部分：策略生成与动作表示</h1>

<hr></p>

<h2>第5章 动作表示方法</h2>

<p>在 VLA 模型中，"如何输出动作" 是一个核心设计选择。本章详细对比三种主流的动作生成范式：<strong>离散化 (Discrete Tokenization)</strong>、<strong>扩散策略 (Diffusion Policy)</strong> 和 <strong>流匹配 (Flow Matching)</strong>。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│              三种动作生成范式对比 (Action Generation)            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. 离散化 Tokenization (RT-1, RT-2)                            │
│     ┌─────────────────────────────────────────┐                 │
│     │ 连续动作 ──▶ 离散 Bins ──▶ Token 预测   │                 │
│     │  0.234   ──▶  [0-255]  ──▶   Token 60   │                 │
│     └─────────────────────────────────────────┘                 │
│     ⚡ 快速 (1步)  |  📉 精度损失  |  ✅ 多模态                  │
│                                                                 │
│  2. 扩散策略 Diffusion (Octo)                                   │
│     ┌─────────────────────────────────────────┐                 │
│     │ 噪声 ════▶ 去噪 ════▶ 去噪 ════▶ 动作   │                 │
│     │  x_T  ──▶  x_{T-1} ──▶ ... ──▶  x_0    │                 │
│     └─────────────────────────────────────────┘                 │
│     🐢 慢 (50-100步)  |  🎯 高精度  |  ✅ 多模态                 │
│                                                                 │
│  3. 流匹配 Flow Matching (π0)                                   │
│     ┌─────────────────────────────────────────┐                 │
│     │ 噪声 ════════════════════════════▶ 动作 │                 │
│     │  x_T  ────── 直线 ODE ──────────▶  x_0  │                 │
│     └─────────────────────────────────────────┘                 │
│     ⚡ 极快 (1-10步)  |  🎯 高精度  |  ✅ 多模态                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. 离散化 (Discrete Tokenization)</h3>
<blockquote><strong>代表模型</strong>: <strong>RT-1</strong>, <strong>RT-2</strong>, <strong>Gato</strong></blockquote>

<h4>核心思想</h4>
将连续的物理动作 (如关节角度、末端坐标) 离散化为整数 Token，从而可以使用标准的 Transformer (分类任务) 进行预测。</p>

<h4>数学公式</h4>
假设动作 $a \in [min, max]$，我们将其划分为 $N$ 个区间 (Bins)。
<pre><code class="math">Token = \text{round}\left( \frac{a - min}{max - min} \times (N - 1) \right)
</code></pre>
<ul><li><strong>RT-1</strong>: 使用 $N=256$。</li>
<li><strong>预测</strong>: 模型输出一个 Logits 向量 $z \in \mathbb{R}^N$，通过 Softmax 得到概率分布：</li>
</ul><pre><code class="math">P(Token = i) = \frac{e^{z_i}}{\sum_{j=0}^{N-1} e^{z_j}}
</code></pre>

<h4>优缺点</h4>
<ul><li><strong>优点</strong>:</li>
</ul>    - <strong>多模态分布 (Multimodal)</strong>: 可以很好地建模"向左走或向右走" (双峰分布)，而不会输出中间的平均值 (撞墙)。
    - <strong>架构统一</strong>: 可以直接复用 LLM 的 Cross-Entropy Loss。
<ul><li><strong>缺点</strong>:</li>
</ul>    - <strong>精度损失</strong>: 丢失了 Bin 内部的精度。对于高精度装配任务，256 个 Bin 可能不够。
    - <strong>高频抖动</strong>: 预测结果在相邻 Bin 之间跳变会导致动作不平滑。</p>

<hr></p>

<h3>2. 扩散策略 (Diffusion Policy)</h3>
<blockquote><strong>代表模型</strong>: <strong>Octo</strong>, <strong>MimicGen</strong>, <strong>Toyota HPT</strong></blockquote>

<h4>核心思想</h4>
将动作生成建模为从高斯噪声中 <strong>去噪 (Denoising)</strong> 的过程。</p>

<h4>数学公式</h4>
<ul><li><strong>前向过程 (加噪)</strong>:</li>
</ul><pre><code class="math">q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)
</code></pre>
<ul><li><strong>逆向过程 (去噪)</strong>:</li>
</ul>模型 $\epsilon_\theta(x_t, t, \text{cond})$ 预测噪声，从而逐步还原动作：
<pre><code class="math">x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t, \text{cond}) \right) + \sigma_t z
</code></pre>

<h4>优缺点</h4>
<ul><li><strong>优点</strong>:</li>
</ul>    - <strong>高精度</strong>: 输出是连续值，没有离散化误差。
    - <strong>多模态</strong>: 天然支持多解分布 (Multimodal Distribution)。
    - <strong>稳定性</strong>: 相比 GAN，训练更稳定。
<ul><li><strong>缺点</strong>:</li>
</ul>    - <strong>推理慢</strong>: 需要迭代去噪 (通常 50-100 步)，导致推理延迟高 (Latency)，难以用于高频控制 (如 >50Hz)。</p>

<hr></p>

<h3>3. 流匹配 (Flow Matching)</h3>
<blockquote><strong>代表模型</strong>: <strong>Pi0 (Physical Intelligence)</strong></blockquote>

<h4>核心思想</h4>
学习一个 <strong>确定性的向量场 (Vector Field)</strong>，将噪声分布平滑地变换为数据分布。</p>

<h4>与 Diffusion 的对比</h4>
<ul><li><strong>Diffusion</strong>: 走的是随机游走 (Stochastic) 的去噪路径。</li>
<li><strong>Flow Matching</strong>: 走的是 <strong>直线 (Straight)</strong> 路径 (Optimal Transport)。</li>
</ul>
<h4>优缺点</h4>
<ul><li><strong>优点</strong>:</li>
</ul>    - <strong>极速推理</strong>: 由于轨迹是直的，ODE 求解器只需要很少的步数 (e.g., 1-10 步) 就能得到高质量结果。
    - <strong>高频控制</strong>: 使得大模型也能跑在 50Hz+ 的控制频率上。
<ul><li><strong>缺点</strong>:</li>
</ul>    - <strong>训练复杂</strong>: 数学理论相对较新，训练稳定性需要技巧。</p>

<hr></p>

<h3>总结对比 (Comparison)</h3>

<table>
<thead><tr>
<th>范式</th>
<th>代表模型</th>
<th>输出类型</th>
<th>推理速度</th>
<th>精度</th>
<th>多模态支持</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Discrete Tokenization</strong></td>
<td>RT-1, RT-2</td>
<td>整数 Token</td>
<td>快 (1步)</td>
<td>低 (受限于 Bin)</td>
<td>支持</td>
</tr>
<tr>
<td><strong>Diffusion Policy</strong></td>
<td>Octo</td>
<td>连续浮点数</td>
<td>慢 (50+步)</td>
<td>高</td>
<td>支持</td>
</tr>
<tr>
<td><strong>Flow Matching</strong></td>
<td>Pi0</td>
<td>连续浮点数</td>
<td>极快 (1-10步)</td>
<td>高</td>
<td>支持</td>
</tr>
</tbody></table>

<blockquote><strong>面试 Tip</strong>: 如果面试官问 "为什么现在的 VLA 模型开始从 Tokenization 转向 Diffusion/Flow?"</blockquote>
<blockquote><strong>答</strong>: 为了追求<strong>高精度操作</strong> (如穿针、装配)。Tokenization 在处理这种任务时，量化误差是致命的，而 Diffusion/Flow 提供了连续空间的精细控制能力。</blockquote>

<hr></p>

<hr></p>

<h2>第6章 Diffusion Policy</h2>

<blockquote><strong>核心论文</strong>: <a href="https://arxiv.org/abs/2303.04137">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</a> (Cheng Chi et al., RSS 2023)</blockquote>
<blockquote><strong>代表模型</strong>: <strong>Octo</strong>, <strong>MimicGen</strong>, <strong>Toyota HPT</strong></blockquote>

<h3>1. 为什么需要 Diffusion Policy? (Why?)</h3>

<p>在 VLA 出现之前，主流的动作生成方式是 <strong>MSE Regression</strong> (均方误差回归) 或 <strong>GMM</strong> (高斯混合模型)。</p>

<h4>1.1 多模态分布问题 (The Multimodality Problem)</h4>
机器人经常面临"多解"情况。例如，绕过障碍物可以<strong>从左绕</strong>也可以<strong>从右绕</strong>。
<ul><li><strong>MSE (均值回归)</strong>: 会预测出左和右的平均值 -> <strong>直直撞向障碍物</strong>。</li>
<li><strong>Diffusion</strong>: 可以完美拟合双峰分布，随机采样出"左"或"右"的一条完整轨迹，而不会取平均。</li>
</ul>    - <strong>Energy-Based Model (EBM) 视角</strong>: 我们可以把 Diffusion 看作是在学习一个能量函数 $E(x)$。真实数据的能量低，噪声的能量高。
    - MSE 试图最小化单一模态的误差，相当于在两个低谷之间强行找一个"平均低谷" (往往是能量很高的高地)。
    - Diffusion 则是学习整个地貌 (Landscape)，允许存在多个分离的低谷 (Modes)。</p>

<h4>1.2 连续空间的高精度 (High Precision)</h4>
相比于 Tokenization (RT-1) 将动作离散化为 256 个桶，Diffusion 直接在连续空间生成浮点数，精度理论上无限，非常适合<strong>穿针引线、精密装配</strong>等任务。</p>

<h3>2. 数学原理 (Mathematical Formulation)</h3>

<p>Diffusion Policy 将动作生成建模为一个 <strong>条件去噪过程 (Conditional Denoising Process)</strong>。</p>

<h4>2.1 前向过程 (Forward Process / Diffusion)</h4>
将真实的动作轨迹 $x_0$ 逐步加噪，变成纯高斯噪声 $x_T$。</p>

<pre><code class="math">q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
</code></pre>

<p>经过 $t$ 步后，可以直接写出 $x_t$ 与 $x_0$ 的关系：
<pre><code class="math">q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)
</code></pre>

<p>其中 $\alpha_t = 1 - \beta_t$, $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$。</p>

<h4>2.2 噪声调度器 (Noise Scheduler)</h4>
$\beta_t$ 的选择至关重要，通常有两种策略：
<ul><li><strong>Linear Schedule</strong>: $\beta_t$ 从 $\beta_{min}=10^{-4}$ 线性增加到 $\beta_{max}=0.02$。</li>
</ul>    - $\beta_t = \beta_{min} + \frac{t}{T}(\beta_{max} - \beta_{min})$
<ul><li><strong>Cosine Schedule</strong>: $\beta_t$ 随余弦函数变化，能更好地保留中间时刻的信息，防止噪声过早"淹没"信号。</li>
</ul>    - $\bar{\alpha}_t = \frac{f(t)}{f(0)}, \quad f(t) = \cos^2 \left( \frac{t/T + s}{1+s} \cdot \frac{\pi}{2} \right)$
    - 这种调度在 $t$ 较小时噪声增加得很慢，保留了更多原始信号，对微小动作的生成更有利。</p>

<h4>2.3 逆向过程 (Reverse Process / Denoising)</h4>
训练一个神经网络 $\epsilon_\theta(x_t, t, \text{Obs})$ 来预测噪声。
<ul><li><strong>输入</strong>: 当前带噪动作 $x_t$，时间步 $t$，观测条件 $\text{Obs}$ (图像/语言)。</li>
<li><strong>输出</strong>: 预测的噪声 $\hat{\epsilon}$。</li>
</ul>
去噪公式 (DDPM):</p>

<pre><code class="math">x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t, \text{Obs}) \right) + \sigma_t z
</code></pre>

<p>其中 $z \sim \mathcal{N}(0, I)$ 是随机噪声 (但在最后一步 $t=0$ 时设为 0)。$\sigma_t$ 是方差项，通常取 $\sqrt{\beta_t}$ 或 $\tilde{\beta}_t$。</p>

<h4>2.4 损失函数 (Loss Function)</h4>
非常简单，就是预测噪声与真实噪声的 MSE：</p>

<pre><code class="math">\mathcal{L} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t, \text{Obs}) \|^2 \right]
</code></pre>

<h3>3. 网络架构 (Network Architecture)</h3>

<p>Diffusion Policy 的核心是那个预测噪声的网络 $\epsilon_\theta$。主要有两种流派：</p>

<h4>3.1 CNN-based (1D Temporal CNN / U-Net)</h4>
<ul><li><strong>原理</strong>: 将动作轨迹看作是一个 1D 的时间序列 (Sequence Length = $T_p$, Action Dim = $D_a$)。</li>
<li><strong>结构</strong>: 使用类似于 U-Net 的结构，但在时间维度上进行下采样和上采样 (Downsample/Upsample)。</li>
</ul>    - <strong>Conditioning</strong>: 图像特征 (ResNet/ViT) 和 语言特征 (CLIP) 通常通过 <strong>FiLM (Feature-wise Linear Modulation)</strong> 层注入到 U-Net 的每个 Residual Block 中。
    - <strong>Downsample</strong>: Conv1d + GroupNorm + Mish
    - <strong>Upsample</strong>: ConvTranspose1d
    - <strong>Skip Connection</strong>: 将 Encoder 的特征拼接到 Decoder，保留高频细节。
<ul><li><strong>特点</strong>: 计算效率高，适合处理短时序依赖，是 Diffusion Policy 论文中的默认选择。</li>
</ul>
<h4>3.2 Transformer-based (DiT / Octo)</h4>
<ul><li><strong>原理</strong>: 将动作轨迹切成 Patch，或者直接作为 Token 输入 Transformer。</li>
<li><strong>结构</strong>: 标准的 Transformer Encoder/Decoder (如 DiT - Diffusion Transformer)。</li>
<li><strong>特点</strong>: </li>
</ul>    - 能够处理更长的时间依赖。
    - <strong>多模态融合</strong>: 可以直接 Cross-Attention 图像 Patch 和 语言 Token。
    - <strong>Scalability</strong>: 参数量可以做得很大 (e.g., Octo-Base 93M, Octo-Small 27M)，适合作为 Foundation Model。</p>

<h3>4. 推理加速 (Inference Acceleration)</h3>

<p>Diffusion 的最大缺点是慢。DDPM 需要 100 步去噪，推理一次可能要几百毫秒，无法满足机器人 50Hz 的控制要求。</p>

<h4>4.1 DDIM (Denoising Diffusion Implicit Models)</h4>
<ul><li><strong>原理</strong>: 将随机游走过程变为确定性过程 (Deterministic)，跳过中间步骤。DDIM 重新定义了前向过程，使得它是一个非马尔可夫过程，从而允许更大的步长。</li>
<li><strong>公式</strong>:</li>
</ul>
<pre><code class="math">x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_\theta}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{predicted } x_0} + \underbrace{\sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta}_{\text{direction pointing to } x_t} + \sigma_t \epsilon_t
</code></pre>

<ul><li><strong>效果</strong>: 可以将步数从 100 步压缩到 <strong>10-15 步</strong>，同时保持较高的生成质量。</li>
</ul>
<h4>4.2 Receding Horizon Control (RHC)</h4>
<ul><li><strong>原理</strong>: 模型一次预测未来 $H$ 步的动作 (Action Chunk)，但机器人只执行前 $M$ 步 (例如 $M=8$)，然后重新推理。</li>
<li><strong>优势</strong>: 掩盖了推理延迟，保证动作的连贯性。</li>
</ul>
<h3>5. 面试常见问题 (Q&A)</h3>

<p><strong>Q: Diffusion Policy 和 GAN 有什么区别?</strong>
A: GAN 容易模式坍塌 (Mode Collapse)，即只学会一种解；Diffusion 训练更稳定，能覆盖所有模态 (Mode Coverage)。</p>

<p><strong>Q: 为什么 Diffusion 推理慢? 如何解决?</strong>
A: 因为它是迭代去噪。解决方法包括使用 DDIM 减少步数，或者使用 Consistency Distillation (一致性蒸馏) 将步数压缩到 1 步。</p>

<p><strong>Q: 什么是 Action Chunking?</strong>
A: 一次预测一段未来的动作序列，而不是只预测下一步。这利用了动作的时间相关性，提高了平滑度。</p>

<hr></p>

<hr></p>

<h2>第7章 Action Chunking Transformer (ACT)</h2>

<blockquote><strong>核心论文</strong>: <a href="https://arxiv.org/abs/2304.13705">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</a> (Tony Z. Zhao et al., RSS 2023)</blockquote>
<blockquote><strong>代表项目</strong>: <strong>ALOHA</strong>, <strong>Mobile ALOHA</strong>, <strong>ACT++</strong></blockquote>

<h3>1. 为什么需要 ACT? (Why?)</h3>

<p>传统的行为克隆 (Behavior Cloning, BC) 方法通常采用<strong>单步预测</strong>：每次只预测下一时刻的动作。这种方式存在严重的<strong>误差累积 (Compounding Error)</strong> 问题。</p>

<h4>1.1 单步预测的致命缺陷</h4>

<pre><code class="">单步预测流程:
观测 o_t → 策略 π(o_t) → 动作 a_t → 执行 → 观测 o_{t+1} → ...
</code></pre>

<p><strong>问题</strong>:
<ul><li><strong>误差累积</strong>: 每一步的预测误差会传递到下一步。如果每步有 1% 的误差，100 步后误差可能达到 100%。</li>
<li><strong>分布偏移 (Distribution Shift)</strong>: 训练数据来自专家轨迹，但执行时的状态分布可能偏离专家，导致"越错越离谱"。</li>
<li><strong>高频闭环压力</strong>: 需要每一帧都精确预测，对模型要求极高。</li>
</ul>
<h4>1.2 ACT 的解决思路</h4>

<p>ACT 提出了一个简单而有效的想法：<strong>一次预测一段未来的动作序列 (Action Chunk)</strong>，而不是只预测下一步。</p>

<pre><code class="math">\text{单步 BC}: \pi(o_t) \rightarrow a_t
</code></pre>

<pre><code class="math">\text{ACT}: \pi(o_t) \rightarrow [a_t, a_{t+1}, ..., a_{t+k-1}]
</code></pre>

<p><strong>核心优势</strong>:
<ul><li><strong>减少决策点</strong>: 如果 chunk 大小 $k=100$，决策频率从 50Hz 降到 0.5Hz，大大降低了误差累积的机会。</li>
<li><strong>隐式任务分解</strong>: 预测一整段动作，模型需要"理解"整个子任务的结构 (如"伸手→抓握→抬起")，而不只是盲目模仿下一帧。</li>
<li><strong>平滑轨迹</strong>: 一次生成的轨迹天然连贯，避免了单步预测的抖动。</li>
</ul>
<h3>2. 核心技术 (Core Techniques)</h3>

<h4>2.1 动作分块 (Action Chunking)</h4>

<p><strong>定义</strong>: 将连续的动作序列分成固定长度 $k$ 的"块"，模型一次预测一整个块。</p>

<p><strong>数学表示</strong>:
<ul><li>输入: 当前观测 $o_t$ (图像 + 本体感知 + 语言指令)</li>
<li>输出: 动作序列 $\mathbf{a} = [a_t, a_{t+1}, ..., a_{t+k-1}] \in \mathbb{R}^{k \times D_a}$</li>
<li>其中 $D_a$ 是动作维度 (如 7-DoF 机械臂 + 1 夹爪 = 8)</li>
</ul>
<strong>典型参数</strong>:
<ul><li><strong>ALOHA</strong>: $k = 100$ (在 50Hz 控制下，对应 2 秒的动作)</li>
<li><strong>Mobile ALOHA</strong>: $k = 50$ (对应 1 秒)</li>
</ul>
<h4>2.2 时间集成 (Temporal Ensemble)</h4>

<p>仅用 Action Chunking 还不够。如果在 $t=0$ 时预测了 $[a_0, ..., a_{99}]$，然后在 $t=100$ 时再预测 $[a_{100}, ..., a_{199}]$，两段轨迹的交界处可能不连续。</p>

<p><strong>解决方案</strong>: <strong>重叠预测 + 指数加权平均</strong></p>

<pre><code class="">时间步 t=0:   预测 [a_0, a_1, ..., a_99]
时间步 t=1:   预测 [a_1', a_2', ..., a_100']
时间步 t=2:   预测 [a_2'', a_3'', ..., a_101'']
...</p>

<p>执行 a_t 时，融合所有对 a_t 的预测:
a_t^{final} = Σ w_i * a_t^{(i)}
</code></pre>

<p><strong>指数加权公式</strong>:
<pre><code class="math">w_i = \exp(-m \cdot i)
</code></pre>

<p>其中 $m$ 是衰减系数 (通常 $m=0.01$)，$i$ 是预测的"年龄"(越新的预测权重越大)。</p>

<p><strong>效果</strong>:
<ul><li><strong>平滑过渡</strong>: 新旧预测的融合消除了不连续。</li>
<li><strong>鲁棒性</strong>: 即使某次预测有误，也会被其他预测"稀释"。</li>
</ul>
<h4>2.3 CVAE 架构 (Conditional Variational Autoencoder)</h4>

<p>ACT 的另一个核心创新是使用 <strong>CVAE</strong> 来处理动作的<strong>多模态分布</strong>。</p>

<h4>2.3.1 为什么需要 CVAE?</h4>

<p>与 Diffusion Policy 类似，机器人动作存在<strong>多解</strong>问题 (如绕障碍物可以从左绕或从右绕)。</p>

<ul><li><strong>MSE 回归</strong>: 会预测左右的平均值 → 撞向障碍物。</li>
<li><strong>CVAE</strong>: 学习动作分布的<strong>隐变量表示</strong>，采样不同的 $z$ 可以生成不同的合理轨迹。</li>
</ul>
<h4>2.3.2 CVAE 数学原理</h4>

<p><strong>训练时 (有真实动作)</strong>:
<li><strong>编码器 (Encoder)</strong> $q_\phi(z | o, a)$: 将观测 $o$ 和真实动作序列 $a$ 编码为隐变量 $z$ 的分布 (均值 $\mu$, 方差 $\sigma^2$)。</li>
<li><strong>解码器 (Decoder)</strong> $p_\theta(a | o, z)$: 从隐变量 $z$ 和观测 $o$ 重建动作序列。</li></p>

<p><strong>损失函数</strong>:
<pre><code class="math">\mathcal{L} = \underbrace{\| a - \hat{a} \|^2}_{\text{重建损失}} + \beta \cdot \underbrace{D_{KL}(q_\phi(z|o,a) \| \mathcal{N}(0, I))}_{\text{KL 散度}}
</code></pre>

<ul><li><strong>重建损失</strong>: 让解码器输出接近真实动作。</li>
<li><strong>KL 散度</strong>: 让隐变量分布接近标准正态分布，便于推理时采样。</li>
<li><strong>$\beta$</strong>: 权重系数 (ACT 中通常 $\beta = 10$)。</li>
</ul>
<strong>推理时 (无真实动作)</strong>:
<li>从标准正态分布采样 $z \sim \mathcal{N}(0, I)$。</li>
<li>解码器根据 $o$ 和 $z$ 生成动作序列。</li></p>

<pre><code class="python">z_mu, z_logvar = encoder(obs, gt_actions)  # 编码真实动作
z = reparameterize(z_mu, z_logvar)         # 重参数化采样
pred_actions = decoder(obs, z)             # 解码</p>

<p>recon_loss = mse(pred_actions, gt_actions)
kl_loss = -0.5 <em> (1 + z_logvar - z_mu</em>*2 - z_logvar.exp()).sum()
loss = recon_loss + beta * kl_loss</p>

<p>z = torch.randn(batch_size, z_dim)         # 直接从标准正态采样
pred_actions = decoder(obs, z)             # 解码
</code></pre>

<h3>3. 网络架构 (Network Architecture)</h3>

<p>ACT 使用 <strong>Transformer</strong> 作为骨干网络，具体包括:</p>

<h4>3.1 观测编码器 (Observation Encoder)</h4>

<pre><code class="">输入:
<ul><li>图像: [B, T, C, H, W] (可以是多相机)</li>
<li>本体感知: [B, D_p] (关节角度、末端位置等)</li>
</ul>
处理:
<ul><li>图像 → ResNet-18 / ViT → 特征向量 [B, T, D_v]</li>
<li>本体感知 → Linear → 特征向量 [B, D_p']</li>
<li>拼接 → [B, T+1, D]</li>
</ul></code></pre>

<h4>3.2 CVAE 编码器 (仅训练时)</h4>

<pre><code class="">输入:
<ul><li>观测特征: [B, T+1, D]</li>
<li>真实动作: [B, k, D_a] (经过 Linear 映射)</li>
</ul>
结构:
<ul><li>Transformer Encoder (4 层)</li>
<li>输出: z_mu, z_logvar ∈ R^{D_z} (通常 D_z = 32)</li>
</ul></code></pre>

<h4>3.3 动作解码器 (Action Decoder)</h4>

<pre><code class="">输入:
<ul><li>观测特征: [B, T+1, D] (作为 Cross-Attention 的 Key/Value)</li>
<li>隐变量 z: [B, D_z] (拼接到每个 Query Token)</li>
<li>可学习的 Action Query: [B, k, D] (k 个位置编码)</li>
</ul>
结构:
<ul><li>Transformer Decoder (7 层)</li>
<li>Cross-Attention: Query 关注 观测特征</li>
<li>输出: [B, k, D_a] (k 步动作)</li>
</ul></code></pre>

<h4>3.4 完整架构图</h4>

<pre><code class="">                    ┌─────────────────────────────────────────┐
                    │            CVAE Encoder                 │
                    │  (obs_feat, gt_actions) → z_mu, z_var   │
                    └──────────────────┬──────────────────────┘
                                       │ z (重参数化采样)
                                       ▼
┌──────────────┐    ┌─────────────────────────────────────────┐
│   Images     │───▶│                                         │
│  (多相机)    │    │         Observation Encoder             │
├──────────────┤    │  ResNet-18 / ViT + Positional Emb       │
│  Proprio     │───▶│                                         │
│  (本体感知)  │    └──────────────────┬──────────────────────┘
└──────────────┘                       │ obs_feat [B, T+1, D]
                                       ▼
                    ┌─────────────────────────────────────────┐
                    │          Action Decoder                 │
                    │  (Action Query + z) × obs_feat          │
                    │      Transformer Decoder                │
                    └──────────────────┬──────────────────────┘
                                       │
                                       ▼
                              Action Chunk [B, k, D_a]
</code></pre>

<h3>4. 与其他方法的对比 (Comparison)</h3>

<table>
<thead><tr>
<th>方法</th>
<th>预测方式</th>
<th>多模态处理</th>
<th>推理速度</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>单步 BC</strong></td>
<td>单帧</td>
<td>无 (MSE)</td>
<td>最快</td>
<td>简单任务</td>
</tr>
<tr>
<td><strong>ACT (CVAE)</strong></td>
<td>Chunk ($k$ 步)</td>
<td>CVAE 采样</td>
<td><strong>快</strong></td>
<td>高频、精细操作</td>
</tr>
<tr>
<td><strong>Diffusion Policy</strong></td>
<td>Chunk ($k$ 步)</td>
<td>扩散采样</td>
<td>慢 (多步去噪)</td>
<td>多模态复杂任务</td>
</tr>
<tr>
<td><strong>Flow Matching (π0)</strong></td>
<td>Chunk ($k$ 步)</td>
<td>ODE 采样</td>
<td>中等</td>
<td>通用基础模型</td>
</tr>
</tbody></table>

<p><strong>ACT 的优势</strong>:
<ul><li><strong>推理速度快</strong>: CVAE 只需一次前向传播，Diffusion 需要 10-100 步去噪。</li>
<li><strong>简单易实现</strong>: 标准 Transformer + VAE，无需复杂的噪声调度器。</li>
<li><strong>数据效率高</strong>: 在 ALOHA 项目中，仅 50 条演示就能学会双臂精细操作。</li>
</ul>
<strong>ACT 的劣势</strong>:
<ul><li><strong>分布覆盖有限</strong>: CVAE 的隐空间容量有限，可能无法覆盖所有动作模态。</li>
<li><strong>KL 坍塌风险</strong>: 如果 $\beta$ 设置不当，模型可能忽略隐变量 $z$。</li>
</ul>
<h3>5. 实战代码示例 (Code Example)</h3>

<pre><code class="python">import torch
import torch.nn as nn
from torch.nn import TransformerEncoder, TransformerDecoder</p>

<p>class ACTPolicy(nn.Module):
    def __init__(
        self,
        obs_dim: int,
        action_dim: int,
        chunk_size: int = 100,
        z_dim: int = 32,
        hidden_dim: int = 512,
        num_encoder_layers: int = 4,
        num_decoder_layers: int = 7,
    ):
        super().__init__()
        self.chunk_size = chunk_size
        self.z_dim = z_dim
        
        # Observation Encoder
        self.obs_encoder = nn.Linear(obs_dim, hidden_dim)
        
        # CVAE Encoder (用于训练)
        self.cvae_encoder = TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8),
            num_layers=num_encoder_layers
        )
        self.z_mu = nn.Linear(hidden_dim, z_dim)
        self.z_logvar = nn.Linear(hidden_dim, z_dim)
        
        # Action Decoder
        self.action_queries = nn.Parameter(torch.randn(chunk_size, hidden_dim))
        self.z_proj = nn.Linear(z_dim, hidden_dim)
        self.action_decoder = TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8),
            num_layers=num_decoder_layers
        )
        self.action_head = nn.Linear(hidden_dim, action_dim)
        
        # Action embedding for CVAE encoder
        self.action_embed = nn.Linear(action_dim, hidden_dim)
    
    def encode(self, obs_feat, actions):
        """CVAE 编码器: 编码观测和动作到隐空间"""
        # actions: [B, k, action_dim]
        action_feat = self.action_embed(actions)  # [B, k, hidden_dim]
        
        # 拼接观测和动作
        combined = torch.cat([obs_feat, action_feat], dim=1)  # [B, T+1+k, D]
        combined = combined.permute(1, 0, 2)  # [T+1+k, B, D]
        
        encoded = self.cvae_encoder(combined)
        pooled = encoded.mean(dim=0)  # [B, D]
        
        return self.z_mu(pooled), self.z_logvar(pooled)
    
    def reparameterize(self, mu, logvar):
        """重参数化技巧"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, obs_feat, z):
        """动作解码器: 从观测和隐变量生成动作序列"""
        batch_size = obs_feat.shape[0]
        
        # Action queries + z
        queries = self.action_queries.unsqueeze(0).expand(batch_size, -1, -1)
        z_expanded = self.z_proj(z).unsqueeze(1)  # [B, 1, D]
        queries = queries + z_expanded  # 广播加法
        
        queries = queries.permute(1, 0, 2)  # [k, B, D]
        obs_feat = obs_feat.permute(1, 0, 2)  # [T+1, B, D]
        
        decoded = self.action_decoder(queries, obs_feat)
        decoded = decoded.permute(1, 0, 2)  # [B, k, D]
        
        return self.action_head(decoded)
    
    def forward(self, obs, actions=None):
        """
        训练时: obs + actions → z → pred_actions
        推理时: obs → sample z → pred_actions
        """
        obs_feat = self.obs_encoder(obs).unsqueeze(1)  # [B, 1, D]
        
        if actions is not None:  # 训练模式
            z_mu, z_logvar = self.encode(obs_feat, actions)
            z = self.reparameterize(z_mu, z_logvar)
            pred_actions = self.decode(obs_feat, z)
            return pred_actions, z_mu, z_logvar
        else:  # 推理模式
            z = torch.randn(obs.shape[0], self.z_dim, device=obs.device)
            pred_actions = self.decode(obs_feat, z)
            return pred_actions</p>

<p>def compute_loss(pred_actions, gt_actions, z_mu, z_logvar, beta=10.0):
    """ACT 损失函数: 重建损失 + β * KL 散度"""
    recon_loss = nn.functional.mse_loss(pred_actions, gt_actions)
    kl_loss = -0.5 * torch.mean(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())
    return recon_loss + beta * kl_loss, recon_loss, kl_loss
</code></pre>

<h3>6. 时间集成的实现 (Temporal Ensemble)</h3>

<pre><code class="python">class TemporalEnsemble:
    def __init__(self, chunk_size, action_dim, decay=0.01):
        self.chunk_size = chunk_size
        self.decay = decay
        self.action_buffer = []  # 存储历史预测
        self.weights = []        # 存储对应权重
    
    def update(self, new_chunk):
        """添加新预测的 chunk"""
        # new_chunk: [k, action_dim]
        self.action_buffer.append(new_chunk)
        self.weights.append(1.0)
        
        # 衰减旧权重
        self.weights = [w * (1 - self.decay) for w in self.weights]
        
        # 移除过期的预测
        while len(self.action_buffer) &gt; self.chunk_size:
            self.action_buffer.pop(0)
            self.weights.pop(0)
    
    def get_action(self, t):
        """获取时刻 t 的集成动作"""
        weighted_sum = 0
        total_weight = 0
        
        for i, (chunk, w) in enumerate(zip(self.action_buffer, self.weights)):
            # 计算该 chunk 对时刻 t 的预测
            chunk_start_time = t - len(self.action_buffer) + i + 1
            local_idx = t - chunk_start_time
            
            if 0 &lt;= local_idx &lt; len(chunk):
                weighted_sum += w * chunk[local_idx]
                total_weight += w
        
        return weighted_sum / total_weight if total_weight &gt; 0 else None
</code></pre>

<h3>7. 面试常见问题 (Q&A)</h3>

<p><strong>Q1: ACT 和 Diffusion Policy 的核心区别是什么?</strong></p>

<p>A: 
<ul><li><strong>生成机制</strong>: ACT 使用 <strong>CVAE</strong> (一次前向传播)，Diffusion 使用<strong>迭代去噪</strong> (10-100 步)。</li>
<li><strong>速度</strong>: ACT 推理更快，适合高频控制 (50Hz)；Diffusion 慢但分布覆盖更全。</li>
<li><strong>实现复杂度</strong>: ACT 更简单 (标准 VAE)；Diffusion 需要噪声调度器。</li>
</ul>
<strong>Q2: 为什么 Action Chunking 能减少误差累积?</strong></p>

<p>A:
<ul><li>减少了<strong>决策点数量</strong>：chunk=100 时，原本 100 次决策变为 1 次。</li>
<li>模型需要<strong>规划整个子任务</strong>，而非盲目模仿，隐式学到了任务结构。</li>
<li>配合<strong>时间集成</strong>，单次预测误差被多次预测平均掉。</li>
</ul>
<strong>Q3: CVAE 中的 β 参数如何调整?</strong></p>

<p>A:
<ul><li><strong>β 过大</strong>: KL 散度被过度惩罚，隐变量 $z$ 趋近于标准正态，模型退化为确定性输出 (<strong>KL 坍塌</strong>)。</li>
<li><strong>β 过小</strong>: 隐空间不规整，推理时采样的 $z$ 可能落在"空白区"，生成不合理的动作。</li>
<li><strong>经验值</strong>: ALOHA 项目中 $\beta = 10$ 效果最佳；可以使用 <strong>β-VAE 退火</strong> (从小到大逐渐增加)。</li>
</ul>
<strong>Q4: 时间集成 (Temporal Ensemble) 的作用是什么?</strong></p>

<p>A:
<ul><li><strong>平滑轨迹</strong>: 消除相邻 chunk 之间的不连续。</li>
<li><strong>提高鲁棒性</strong>: 单次预测的错误被历史预测"稀释"。</li>
<li><strong>权衡延迟 vs 平滑</strong>: 衰减系数 $m$ 越大，响应越快但越不平滑。</li>
</ul>
<strong>Q5: ACT 为什么在 ALOHA 项目中表现出色?</strong></p>

<p>A:
<ul><li><strong>高频双臂协调</strong>: 双臂操作需要 50Hz 精细控制，ACT 的快速推理至关重要。</li>
<li><strong>数据效率</strong>: CVAE 的隐空间提供了良好的归纳偏置，50 条演示即可泛化。</li>
<li><strong>硬件友好</strong>: 简单架构易于部署到边缘设备。</li>
</ul>
<h3>8. 参考资源 (References)</h3>

<ul><li><strong>论文</strong>: <a href="https://arxiv.org/abs/2304.13705">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</a></li>
<li><strong>GitHub</strong>: <a href="https://github.com/tonyzhaozh/aloha">ALOHA</a></li>
<li><strong>项目主页</strong>: <a href="https://mobile-aloha.github.io/">Mobile ALOHA</a></li>
<li><strong>视频教程</strong>: <a href="https://www.bilibili.com/video/BV1xxx">ACT 论文精读</a> (B 站)</li>
</ul>
<hr></p>

<hr></p>

<h2>第8章 Flow Matching</h2>

<p>Physical Intelligence 的 π0 模型核心在于引入了 <strong>Flow Matching</strong> 来生成连续的动作序列，替代了传统的 Diffusion Policy 或离散 Tokenization。</p>

<blockquote><strong>注意</strong>: Pi0 已于 2025 年 2 月开源 (OpenPI / LeRobot)。以下代码基于 Flow Matching 原理和 VLA 架构通识进行的 <strong>核心逻辑解构</strong>，方便理解其数学过程。</blockquote>

<h3>1. 核心思想：从噪声流向动作 (The Math behind Flow)</h3>

<h4>1.1 什么是 Flow Matching?</h4>
不同于 Diffusion Model 学习去噪过程 (Denoising Score Matching)，Flow Matching 直接学习一个 <strong>确定性的常微分方程 (ODE)</strong>，定义了概率密度路径 $p_t(x)$ 如何随时间 $t$ 演变。</p>

<p>我们定义一个 <strong>向量场 (Vector Field)</strong> $v_t(x)$，它描述了样本在时间 $t$ 的移动速度和方向。</p>

<pre><code class="math">\frac{dx}{dt} = v_t(x)
</code></pre>

<ul><li>$x_0$: 真实数据分布 (Real Data, e.g., 机器人的正确动作)。</li>
<li>$x_1$: 标准高斯噪声分布 (Noise, $\mathcal{N}(0, I)$)。</li>
<li><strong>目标</strong>: 找到一个向量场 $v_t$，使得当我们从噪声 $x_1$ 出发，沿着这个场逆流而上 (或顺流而下，取决于定义) 积分到 $t=0$ 时，能够精确地变回 $x_0$。</li>
</ul>
<h4>1.2 为什么比 Diffusion 好?</h4>
<ul><li><strong>Diffusion</strong>: 轨迹是随机的 (Stochastic)，像布朗运动一样跌跌撞撞地去噪。推理步数多 (50-100步)。</li>
<li><strong>Flow Matching</strong>: 我们可以强制模型学习一条 <strong>"直的" (Straight)</strong> 轨迹。</li>
</ul>    - <strong>Optimal Transport (最优传输)</strong>: 点对点之间直线最短。Flow Matching 可以学习这种直线路径，使得推理极其高效 (10步以内)。
    - <strong>确定性与稳定性</strong>: 相比于随机采样，ODE 的确定性使得动作生成更加平滑，减少了高频抖动 (Jitter)，这对机械臂控制至关重要。</p>

<p>!<a href="../assets/flow_matching_vs_diffusion.png">Flow Matching vs Diffusion</a>
<em>图示: Diffusion 的随机轨迹 (左) vs Flow Matching 的直线轨迹 (右)</em></p>

<h3>2. 核心公式详解 (Key Formulas)</h3>

<h4>2.1 线性插值路径 (Conditional Flow)</h4>
为了训练模型，我们需要构造一个"正确答案"。假设我们已知一个真实样本 $x_0$ 和一个采样噪声 $x_1$，我们定义一条连接它们的直线路径：</p>

<pre><code class="math">x_t = (1 - t)x_0 + t x_1, \quad t \in [0, 1]
</code></pre>

<ul><li>当 $t=0$ 时， $x_t = x_0$ (数据)。</li>
<li>当 $t=1$ 时， $x_t = x_1$ (噪声)。</li>
</ul>
<h4>2.2 目标速度 (Target Velocity)</h4>
对上面的路径 $x_t$ 对时间 $t$ 求导，得到该路径上的理想速度 $u_t(x|x_1)$：</p>

<pre><code class="math">\frac{d}{dt} x_t = \frac{d}{dt} \left( (1 - t)x_0 + t x_1 \right) = x_1 - x_0
</code></pre>

<ul><li><strong>物理含义</strong>: 目标速度是一个恒定向量，方向从 $x_0$ 指向 $x_1$。这非常直观：要从数据变到噪声，就一直往噪声方向走；反之亦然。</li>
</ul>
<h4>2.3 损失函数 (Loss Function)</h4>
我们训练一个神经网络 $v_\theta(x_t, t, \text{cond})$ 来拟合这个目标速度。这就是 <strong>Conditional Flow Matching (CFM)</strong> loss：</p>

<pre><code class="math">\mathcal{L}(\theta) = \mathbb{E}_{t, x_0, x_1} \left[ \Vert v_\theta(x_t, t, \text{cond}) - (x_1 - x_0) \Vert^2 \right]
</code></pre>

<ul><li><strong>输入</strong>:</li>
</ul>    - $x_t$: 当前时刻的插值状态 (混合了数据和噪声)。
    - $t$: 当前时间步。
    - $\text{cond}$: 图像/语言特征 (VLM embedding)。
<ul><li><strong>标签 (Target)</strong>: $x_1 - x_0$ (常数向量)。</li>
<li><strong>直观解释</strong>: 无论你在路径的哪个位置，网络都应该告诉你："往那个方向走，就能到达终点"。</li>
</ul>
<h3>3. 模型架构 (Pseudo-Code)</h3>

<h4>2.1 VLM Backbone (Conditioning)</h4>
使用 PaliGemma 或类似 VLM 提取多模态特征。</p>

<pre><code class="python">class Pi0VLMBackbone(nn.Module):
    def __init__(self, base_vlm):
        super().__init__()
        self.vlm = base_vlm # e.g., PaliGemma-3B
        
    def forward(self, images, text):
        # 1. 提取图像和文本特征
        # output: [batch, seq_len, hidden_dim]
        features = self.vlm.extract_features(images, text)
        
        # 2. Pooling 或提取特定 Token 作为 Condition
        # 假设我们取最后一个 Token 的 embedding 作为全局上下文
        global_cond = features[:, -1, :] 
        return global_cond
</code></pre>

<h4>2.2 Flow Matching Policy Head</h4>
这是一个 MLP 或 Transformer，预测“速度场”。</p>

<pre><code class="python">class FlowMatchingPolicy(nn.Module):
    def __init__(self, action_dim, cond_dim, hidden_dim=1024):
        super().__init__()
        # Time Embedding: 将标量 t 映射为高维向量，捕捉细粒度的时间信息
        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(dim=256),
            nn.Linear(256, 256),
            nn.SiLU()
        )
        
        # 输入: 噪声动作(action_dim) + 时间embedding(256) + 条件(cond_dim)
        self.net = nn.Sequential(
            nn.Linear(action_dim + 256 + cond_dim, hidden_dim),
            nn.SiLU(), # Swish/SiLU 通常比 ReLU 效果更好
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, action_dim) # 输出 dx/dt
        )</p>

<p>    def forward(self, x_t, t, condition):
        # 1. 处理时间 t
        # t: [batch_size, 1] -&gt; t_emb: [batch_size, 256]
        t_emb = self.time_mlp(t)
        
        # 2. 拼接输入
        # 简单的 Concat 策略，更高级的可以用 AdaLN (Adaptive Layer Norm) 注入条件
        input_feat = torch.cat([x_t, t_emb, condition], dim=-1)
        
        # 3. 预测向量场 (Velocity)
        velocity = self.net(input_feat)
        return velocity
</code></pre>

<h3>3. 推理过程 (Inference / Sampling)</h3>
使用 ODE Solver (如 Euler 方法) 从噪声生成动作。</p>

<pre><code class="python">@torch.no_grad()
def generate_action(policy, vlm_cond, action_dim, steps=10, cfg_scale=1.0):
    """
    从高斯噪声生成动作，支持 Classifier-Free Guidance (CFG)
    """
    batch_size = vlm_cond.shape[0]
    
    # 1. 采样初始噪声 x_1 ~ N(0, I)
    x_t = torch.randn(batch_size, action_dim, device=device)
    
    # 2. 定义时间步 (从 1 到 0)
    dt = -1.0 / steps 
    times = torch.linspace(1.0, 0.0, steps + 1, device=device)
    
    # 3. ODE Solver 循环 (Euler Method)
    for i in range(steps):
        t_curr = times[i]
        
        # 预测当前位置的速度向量 v_t
        # CFG: 同时计算有条件和无条件的速度
        if cfg_scale &gt; 1.0:
            # 构造无条件输入 (空指令/空图像)
            null_cond = torch.zeros_like(vlm_cond) 
            # 批量预测
            input_cond = torch.cat([vlm_cond, null_cond])
            input_x = torch.cat([x_t, x_t])
            input_t = torch.cat([t_curr, t_curr])
            
            v_cond, v_uncond = policy(input_x, input_t, input_cond).chunk(2)
            
            # 组合速度向量
            velocity = v_uncond + cfg_scale * (v_cond - v_uncond)
        else:
            velocity = policy(x_t, t_curr, vlm_cond)
        
        # 更新位置: x_{t+dt} = x_t + v_t * dt
        x_t = x_t + velocity * dt
        
    return x_t
</code></pre>

<h3>4. 训练过程 (Training)</h3>
Flow Matching 的 Loss 非常直观：<strong>回归目标速度</strong>。
目标速度就是从噪声 $x_1$ 指向真实数据 $x_0$ 的方向。</p>

<pre><code class="python">def compute_loss(policy, vlm_cond, real_action):
    batch_size = real_action.shape[0]
    
    # 1. 采样时间 t ~ U[0, 1]
    t = torch.rand(batch_size, 1, device=device)
    
    # 2. 采样噪声 x_1 ~ N(0, I)
    noise = torch.randn_like(real_action)
    
    # 3. 构建中间状态 x_t (Linear Interpolation / Optimal Transport Path)
    # x_t = (1 - t) <em> x_0 + t </em> x_1
    # 注意: 这里定义 t=0 是数据, t=1 是噪声
    x_t = (1 - t) <em> real_action + t </em> noise
    
    # 4. 计算目标速度 (Target Velocity)
    # 也就是 x_1 - x_0 (指向噪声的方向? 或者反过来，取决于定义)
    # 在 Conditional Flow Matching (CFM) 中，通常 v_target = x_1 - x_0
    target_velocity = noise - real_action 
    
    # 5. 模型预测速度
    pred_velocity = policy(x_t, t, vlm_cond)
    
    # 6. MSE Loss
    loss = F.mse_loss(pred_velocity, target_velocity)
    return loss
</code></pre>

<h3>6. 为什么 Pi0 选择 Flow Matching? (Deep Dive)</h3>

<h4>6.1 连续性 vs 离散性 (Continuous vs Discrete)</h4>
<ul><li><strong>RT-1/RT-2 (Discrete)</strong>: 将动作空间切分为 256 个格子。</li>
</ul>    - <em>问题</em>: 丢失精度。对于灵巧手这种需要微米级控制的任务，离散化会导致动作"一卡一卡的" (Jitter)。
<ul><li><strong>Pi0 (Continuous)</strong>: 直接输出浮点数速度向量。</li>
</ul>    - <em>优势</em>: 理论上精度无限，动作平滑，更符合物理世界的本质。</p>

<h4>6.2 高频控制的数学基础</h4>
<ul><li>机器人控制回路通常是 500Hz。如果模型推理需要 100ms (10Hz)，中间 490ms 都在"盲跑"。</li>
<li>Flow Matching 的 <strong>ODE 求解器</strong> 特性允许我们在推理时进行 <strong>时间步缩放 (Time-step Scaling)</strong>。</li>
</ul>    - 我们可以只跑 ODE 的 1 步 (Euler Step)，虽然精度略低，但速度极快，可以实现高频响应。
    - 也可以跑 10 步，获得高精度动作。
    - 这种 <strong>Compute-Accuracy Trade-off</strong> 是 Transformer 做不到的。</p>

<h4>6.3 为什么是直线? (Optimal Transport)</h4>
<ul><li><strong>Wasserstein Distance</strong>: 在概率分布空间中，将一个分布搬运到另一个分布的"最小代价"路径就是直线 (Geodesic)。</li>
<li><strong>OT-CFM</strong>: 我们构造的 $x_t = (1-t)x_0 + t x_1$ 正是 Optimal Transport 的位移插值 (Displacement Interpolation)。</li>
<li><strong>Rectified Flow</strong>: 这与 Stable Diffusion 3 使用的 Rectified Flow 思想一致，旨在将弯曲的扩散路径"拉直"，从而允许极少步数的推理 (1-step generation)。</li>
</ul>
<h4>6.4 ODE Solver 的选择</h4>
<ul><li><strong>Euler (1st order)</strong>: 最简单，一步走到底。$x_{t+dt} = x_t + v_t dt$。速度最快，但误差最大。</li>
<li><strong>Midpoint / Heun (2nd order)</strong>: 先试探性走半步，看斜率，再修正。精度更高，但需要 2 倍的 NFE (Number of Function Evaluations)。</li>
<li><strong>RK4 (4th order)</strong>: 经典的高精度求解器，需要 4 倍 NFE。</li>
<li><strong>结论</strong>: 在机器人控制中，通常使用 <strong>Euler</strong> (追求极速) 或 <strong>Heun</strong> (追求平衡)。由于 Flow Matching 训练出的向量场非常平滑 (直线)，Euler 方法通常已经足够好用。</li>
</ul>

<hr></p>

<hr></p>

<h2>第9章 FAST 动作序列编码</h2>

<blockquote>[!IMPORTANT]</blockquote>
<blockquote><strong>FAST</strong> (Frequency-space Action Sequence Tokenization，频域动作序列 Token 化) 是 <strong>Physical Intelligence</strong> 开发的一种高效动作 Token 化方法，专为解决 VLA 模型中连续动作转换为离散 token 的难题而设计。</blockquote>

<h3>1. 概述</h3>
在 VLA 模型中，动作的表示方式至关重要。传统的离散化方法（如简单分桶）在处理高频、灵巧的机器人操作时效果不佳。FAST 通过<strong>离散余弦变换 (DCT)</strong> 和<strong>字节对编码 (BPE)</strong> 的组合，实现了高效的动作压缩和 token 化。</p>

<ul><li>  <strong>开发者</strong>: Physical Intelligence</li>
<li>  <strong>论文</strong>: <a href="https://arxiv.org/abs/2501.09747">FAST: Efficient Action Tokenization for VLA Models (arXiv:2501.09747)</a></li>
<li>  <strong>核心目标</strong>: 将连续的机器人动作序列压缩为紧凑的离散 token，同时保持高频动作的精度。</li>
<li>  <strong>应用</strong>: 已成功集成到 <strong>OpenVLA</strong> 中，显著提升训练速度（<strong>最高 5 倍加速</strong>）。</li>
</ul>
<h3>2. 核心问题：为什么需要 FAST？</h3>
传统 VLA 模型中的动作 Token 化面临几个挑战：</p>

<h4>2.1. 简单分桶的局限性</h4>
<ul><li>  <strong>Token 数量爆炸</strong>: 对于 7-DoF 机械臂，如果每个关节分成 256 个 bin，总 token 数可达 256^7，导致难以学习。</li>
<li>  <strong>高频动作丢失</strong>: 简单分桶无法捕捉平滑、高频的轨迹变化（如快速折叠衣物、精细抓取）。</li>
</ul>
<h4>2.2. 连续动作的自相关性</h4>
<ul><li>  机器人动作在时间上高度自相关（t 和 t+1 的动作非常相似）。</li>
<li>  自回归模型（如 Transformer）在处理这种高相关性数据时效率低下。</li>
</ul>
FAST 通过<strong>频域变换</strong>解决了这些问题。</p>

<h3>3. FAST 的核心技术</h3>

<h4>3.1. 离散余弦变换 (DCT)</h4>
FAST 借鉴了 <strong>JPEG 图像压缩</strong>的思想，使用 DCT 将时域的动作序列转换到频域。</p>

<h4>3.1.1. 为什么需要 DCT？</h4>
机器人动作序列有两个关键特性：
<li> <strong>时间平滑性</strong>: 相邻时刻的关节角度变化很小（高自相关性）。</li>
<li> <strong>能量集中</strong>: 大部分"信息"集中在低频变化（缓慢移动），高频抖动是噪声。</li></p>

<p><strong>直接 Token 化的问题</strong>:
<ul><li>  如果直接对每个时间步的每个关节分桶，会产生大量<strong>冗余 token</strong>（因为连续时刻非常相似）。</li>
<li>  自回归模型（Transformer）需要逐个预测这些高度相关的 token，<strong>极其低效</strong>。</li>
</ul>
<strong>DCT 的解决方案</strong>:
<ul><li>  将时间序列从<strong>时域</strong>转换到<strong>频域</strong>，分离出低频（主要信息）和高频（噪声）。</li>
<li>  只保留<strong>低频系数</strong>，大幅减少数据量，同时保留动作的本质特征。</li>
</ul>
<h4>3.1.2. DCT 工作原理</h4>
<strong>输入</strong>: 一段动作序列，例如 10 个时间步的 7-DoF 关节角度：
<pre><code class="">[ [θ1_t0, θ2_t0, ..., θ7_t0],
  [θ1_t1, θ2_t1, ..., θ7_t1],
  ...
  [θ1_t9, θ2_t9, ..., θ7_t9] ]  # 形状: [10, 7]
</code></pre>

<p><strong>步骤</strong>:
<li> <strong>对每个关节维度独立应用 DCT</strong>:</li>
    -   DCT 将 10 个时间步的关节角度分解为 10 个频率分量。
    -   公式: <code>X_k = Σ(x_n <em> cos(π/10 </em> (n + 0.5) * k))</code>, k=0,1,...,9
    -   <code>X_0</code> 是 DC 分量（平均值），<code>X_1, X_2</code> 是低频，<code>X_8, X_9</code> 是高频。</p>

<p><li> <strong>低频保留</strong>:</li>
    -   由于动作平滑，能量主要在前 K 个系数（例如 K=4）。
    -   丢弃后 6 个高频系数，<strong>压缩比 2.5:1</strong>（10 → 4）。</p>

<p><li> <strong>量化</strong>:</li>
    -   将浮点 DCT 系数量化为整数（例如 0-255），方便后续 token 化。</p>

<p><strong>类比 JPEG</strong>:
<ul><li>  <strong>JPEG 压缩图像</strong>: 对 8×8 像素块做 2D DCT，保留低频，丢弃高频（人眼不敏感的细节）。</li>
<li>  <strong>FAST 压缩动作</strong>: 对时间序列做 1D DCT，保留低频（缓慢移动），丢弃高频（抖动噪声）。</li>
<li>  <strong>本质</strong>: 都是利用<strong>信号的频域稀疏性</strong>进行压缩。</li>
</ul>
<h4>3.1.3. 为什么 DCT 而不是 FFT？</h4>
<ul><li>  <strong>DCT 只有实数</strong>: 更适合实值信号（关节角度），FFT 会产生复数。</li>
<li>  <strong>边界友好</strong>: DCT 假设信号对称延拓，避免边界不连续导致的高频伪影。</li>
<li>  <strong>压缩效率</strong>: DCT 的能量集中性比 FFT 更好（这也是 JPEG 选择 DCT 的原因）。</li>
</ul>
<h4>3.2. 字节对编码 (BPE)</h4>
经过 DCT 量化后，我们得到一个整数序列，例如：
<pre><code class="">[42, 15, 3, 1, 0, 0, 0]  # 7-DoF，每个关节的前 K=4 个 DCT 系数
</code></pre>
对于 10 个时间步，会有 <code>10 × 7 × 4 = 280</code> 个整数值。这仍然太多！</p>

<h4>3.2.1. 为什么需要 BPE？</h4>
<ul><li>  <strong>统计模式</strong>: DCT 系数的分布不是均匀的，某些<strong>系数组合</strong>会频繁出现。</li>
<li>  <strong>减少 token 数</strong>: BPE 能将常见的系数对合并为单个 token，大幅减少序列长度。</li>
</ul>
<h4>3.2.2. BPE 工作原理</h4>
<strong>初始化</strong>:
<ul><li>  词汇表 = <code>{0, 1, 2, ..., 255}</code> （所有可能的量化 DCT 系数）。</li>
</ul>
<strong>迭代合并</strong>:
<li> <strong>统计</strong>: 在训练数据中，统计所有相邻系数对的频率。</li>
    -   例如，<code>[42, 15]</code> 这个组合出现了 10000 次（最高频）。
<li> <strong>合并</strong>: 将 <code>[42, 15]</code> 合并为一个新 token <code>token_256</code>。</li>
    -   词汇表更新: <code>{0, 1, ..., 255, 256=[42,15]}</code>
<li> <strong>重复</strong>: 不断找最高频的对，合并，直到词汇表达到目标大小（例如 8000）。</li></p>

<p><strong>效果</strong>:
<ul><li>  原始序列: <code>[42, 15, 3, 1, 0, 0, 0]</code> (7 个 token)</li>
<li>  BPE 后: <code>[token_256, token_512, 0]</code> (3 个 token)</li>
<li>  <strong>压缩比</strong>: 2.3:1</li>
</ul>
<strong>类比 GPT 的 BPE</strong>:
<ul><li>  <strong>GPT 文本 BPE</strong>: 将常见的字母组合（如 "ing", "the"）合并为单个 token，减少序列长度。</li>
</ul>    -   例: "running" → <code>["run", "ning"]</code> 而不是 7 个字母。
<ul><li>  <strong>FAST 动作 BPE</strong>: 将常见的 DCT 系数组合合并为单个 token，减少动作序列长度。</li>
</ul>    -   例: <code>[42, 15]</code> → <code>token_256</code>
<ul><li>  <strong>本质</strong>: 都是基于<strong>统计频率</strong>的数据压缩，利用数据的<strong>局部相关性</strong>。</li>
</ul>
<h4>3.2.3. 为什么 BPE 有效？</h4>
<ul><li>  <strong>机器人动作的模式</strong>: 某些动作模式（如 "抓取+提起"）的 DCT 系数模式会重复出现。</li>
<li>  <strong>减少冗余</strong>: BPE 能自动发现这些模式，并用单个 token 表示，<strong>避免重复编码</strong>。</li>
<li>  <strong>保持语义</strong>: 高频的系数组合通常对应有意义的动作片段，BPE 保留了这种语义结构。</li>
</ul>
<h4>3.3. FAST+ (Universal Tokenizer)</h4>
FAST+ 是在 <strong>100 万+真实机器人动作序列</strong>上预训练的通用 token 化器。
<ul><li>  <strong>跨平台</strong>: 适用于不同的机器人（机械臂、人形机器人、移动机器人）。</li>
<li>  <strong>跨频率</strong>: 适应不同的控制频率（10Hz 到 100Hz）。</li>
<li>  <strong>开箱即用</strong>: 无需为每个新任务重新训练 tokenizer。</li>
</ul>
<h3>4. FAST 的优势</h3>
<table>
<thead><tr>
<th>特性</th>
<th>简单分桶</th>
<th>FAST (DCT + BPE)</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Token 数量</strong></td>
<td>高（256^7）</td>
<td><strong>低（2-3 个 token/序列）</strong></td>
</tr>
<tr>
<td><strong>高频精度</strong></td>
<td>差（抖动）</td>
<td><strong>强（平滑）</strong></td>
</tr>
<tr>
<td><strong>训练速度</strong></td>
<td>慢</td>
<td><strong>快（5 倍加速）</strong></td>
</tr>
<tr>
<td><strong>泛化能力</strong></td>
<td>弱</td>
<td><strong>强（FAST+ 跨任务泛化）</strong></td>
</tr>
</tbody></table>

<h3>5. 在 OpenVLA 中的应用</h3>
FAST 已被集成到 <strong>OpenVLA</strong> 框架中：
<ul><li>  <strong>训练</strong>: 使用 FAST tokenizer 将 Open X-Embodiment 数据集中的动作序列 token 化。</li>
<li>  <strong>推理</strong>: 生成的 token 通过逆 DCT 转换回连续动作。</li>
<li>  <strong>效果</strong>: 在折叠衣物、清理桌子等高频任务上，成功率提升 <strong>30-50%</strong>。</li>
</ul>
<h3>6. 与其他动作表示的对比</h3>
<table>
<thead><tr>
<th>方法</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
</tr></thead>
<tbody>
<tr>
<td><strong>分桶 (Binning)</strong></td>
<td>将连续值分成离散区间</td>
<td>简单</td>
<td>Token 爆炸，精度差</td>
</tr>
<tr>
<td><strong>扩散 (Diffusion)</strong></td>
<td>通过去噪生成动作</td>
<td>平滑，高精度</td>
<td>推理慢（多步去噪）</td>
</tr>
<tr>
<td><strong>流匹配 (Flow Matching)</strong></td>
<td>ODE 求解器生成轨迹</td>
<td>快速，高质量</td>
<td>需要额外训练头</td>
</tr>
<tr>
<td><strong>FAST (DCT + BPE)</strong></td>
<td>频域压缩 + Token 化</td>
<td><strong>快速，兼容自回归模型</strong></td>
<td>需要预训练 tokenizer</td>
</tr>
</tbody></table>

<h3>7. 面试要点</h3>
<ul><li>  <strong>DCT 是核心</strong>: 记住 "像 JPEG 压缩图片一样压缩动作轨迹"。</li>
<li>  <strong>BPE 进一步压缩</strong>: 类似 GPT 的 token 化，将 DCT 系数压缩为少量 token。</li>
<li>  <strong>5 倍加速</strong>: FAST 使 OpenVLA 的训练速度提升 5 倍。</li>
<li>  <strong>FAST+ 是通用 tokenizer</strong>: 在 100 万+真实机器人数据上预训练，跨平台泛化。</li>
<li>  <strong>适合自回归模型</strong>: FAST 的 token 输出可以直接喂给 Transformer，无需修改架构。</li>
</ul>
<h3>8. 参考资源</h3>
<ul><li>  <strong>论文</strong>: <a href="https://arxiv.org/abs/2501.09747">FAST: Efficient Action Tokenization for VLA Models (arXiv:2501.09747)</a></li>
<li>  <strong>官方博客</strong>: <a href="https://physicalintelligence.company/blog/fast">Physical Intelligence - FAST</a></li>
<li>  <strong>GitHub</strong>: <a href="https://github.com/openvla/openvla">OpenVLA</a></li>
<li>  <strong>Hugging Face</strong>: <a href="https://huggingface.co/pi0/FAST-plus">FAST+ Tokenizer</a></li>
</ul>

<hr></p>

<h1>第三部分：训练技术与优化</h1>

<hr></p>

<h2>第10章 参数高效微调 (PEFT/LoRA)</h2>

<p>在 VLA 时代，我们通常基于 7B+ 的大模型进行微调。全量微调 (Full Fine-tuning) 极其昂贵，因此参数高效微调 (PEFT) 成为了必修课。</p>

<h3>1. LoRA (Low-Rank Adaptation)</h3>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    LoRA 架构示意图                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│         输入 x                                                  │
│           │                                                     │
│     ┌─────┴─────┐                                               │
│     │           │                                               │
│     ▼           ▼                                               │
│  ┌──────┐   ┌──────┐                                            │
│  │  W₀  │   │  A   │  ← 低秩矩阵 (r &lt;&lt; d)                       │
│  │      │   │ r×k  │    可训练                                  │
│  │ d×k  │   └──┬───┘                                            │
│  │      │      │                                                │
│  │ 冻结 │      ▼                                                │
│  └──┬───┘   ┌──────┐                                            │
│     │       │  B   │  ← 低秩矩阵                                │
│     │       │ d×r  │    可训练                                  │
│     │       └──┬───┘                                            │
│     │          │                                                │
│     │    ΔW = B·A                                               │
│     │          │                                                │
│     └────┬─────┘                                                │
│          │  W = W₀ + α·BA                                       │
│          ▼                                                      │
│        输出 h                                                   │
│                                                                 │
│  参数量: d×k (冻结) + r×(d+k) (训练) ≈ 0.1% ~ 1%                │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>1.1. 核心思想</h4>
大模型的权重矩阵 $W \in \mathbb{R}^{d \times k}$ 虽然参数很多，但在特定任务 (如机器人控制) 上，其<strong>内在维度 (Intrinsic Dimension)</strong> 其实很低。
我们不需要更新整个 $W$，只需要学习一个低秩的增量矩阵 $\Delta W$。</p>

<h4>1.2. 数学原理</h4>
假设预训练权重为 $W_0$，微调后的权重为 $W_0 + \Delta W$。
我们将 $\Delta W$ 分解为两个低秩矩阵 $A$ 和 $B$ 的乘积：</p>

<pre><code class="math">W = W_0 + \Delta W = W_0 + B A
</code></pre>
其中：
<ul><li>$B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$</li>
<li>$r \ll \min(d, k)$ 是秩 (Rank)，通常取 8, 16, 32。</li>
<li><strong>参数量对比</strong>: $d \times k$ (全量) vs $r \times (d + k)$ (LoRA)。对于 7B 模型，LoRA 参数量通常不到 1%。</li>
</ul>
<h4>1.3. 训练与推理</h4>
<ul><li><strong>初始化</strong>: $A$ 使用高斯初始化，$B$ 初始化为 0。这样初始状态下 $\Delta W = 0$，模型输出与预训练模型一致。</li>
<li><strong>训练</strong>: 冻结 $W_0$，只更新 $A$ 和 $B$。</li>
<li><strong>推理</strong>: 可以将 $BA$ 加回到 $W_0$ 中 (Merge)，推理速度与原模型完全一致，无额外延迟。</li>
</ul>
<pre><code class="math">  W_{merged} = W_0 + \alpha \cdot BA
</code></pre>

<p>  ($\alpha$ 是缩放系数，通常 $\alpha/r$ 用于归一化)。</p>

<hr></p>

<h3>2. QLoRA (Quantized LoRA)</h3>

<h4>2.1. 痛点</h4>
LoRA 虽然减少了可训练参数，但<strong>基础模型 $W_0$ 依然需要以 FP16 加载到显存中</strong>。对于 65B 的模型，光加载就需要 130GB 显存，单卡 4090 根本跑不动。</p>

<h4>2.2. 核心创新</h4>
QLoRA 结合了 <strong>4-bit 量化</strong> 和 <strong>LoRA</strong>，使得 65B 模型可以在 48GB 显存上微调。</p>

<p><li> <strong>4-bit NormalFloat (NF4)</strong>: 一种理论最优的 4-bit 量化数据类型，专门针对正态分布的权重设计。</li>
<li> <strong>Double Quantization</strong>: 对量化常数 (Quantization Constants) 再进行一次量化，进一步节省显存 (每参数平均节省 0.37 bit)。</li>
<li> <strong>Paged Optimizers</strong>: 利用 CPU 内存来缓存优化器状态 (Optimizer States)，防止显存 OOM。</li></p>

<h4>2.3. 显存计算 (7B Model)</h4>
<ul><li><strong>Full Fine-tuning (Adam)</strong>: ~112 GB (权重+梯度+优化器状态)</li>
<li><strong>LoRA (FP16)</strong>: ~16 GB (权重) + ~1 GB (LoRA) = ~17 GB</li>
<li><strong>QLoRA (4-bit)</strong>: ~4 GB (权重) + ~1 GB (LoRA) = ~5 GB (可以在 RTX 3060 上跑！)</li>
</ul>
<hr></p>

<h3>3. 其他 PEFT 方法</h3>

<h4>3.1. Adapters (Houlsby et al.)</h4>
<ul><li>在 Transformer 的每一层中插入小的全连接网络 (Adapter Layers)。</li>
<li><strong>缺点</strong>: 增加了推理延迟 (因为是串行的层)，无法像 LoRA 那样合并权重。</li>
</ul>
<h4>3.2. Prefix Tuning / P-Tuning</h4>
<ul><li>在 Input Token 前面拼接一组可学习的 Virtual Tokens。</li>
<li><strong>缺点</strong>: 占用了宝贵的 Context Window 长度。</li>
</ul>
<h4>3.3. P-Tuning v2</h4>
<ul><li>在每一层都加入可学习的 Prefix，而不仅仅是输入层。</li>
<li><strong>优点</strong>: 比 P-Tuning v1 效果更好，接近全量微调。</li>
<li><strong>缺点</strong>: 仍然占用 Context Window。</li>
</ul>
<hr></p>

<h3>4. PEFT 方法对比 (Comparison)</h3>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    PEFT 方法对比一览                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   方法          原理              推理延迟   可合并   参数量     │
│   ─────────────────────────────────────────────────────────     │
│   LoRA          低秩分解 ΔW=BA    无额外     ✅       ~0.1-1%   │
│   Adapter       串行插入 MLP      有额外     ❌       ~1-5%     │
│   P-Tuning      可学习 Prompt     占 Context ❌       ~0.01%    │
│   Prefix-Tuning 可学习 KV 前缀    占 Context ❌       ~0.1%     │
│   P-Tuning v2   每层 Prefix       占 Context ❌       ~0.1-1%   │
│                                                                 │
│   💡 VLA 首选: LoRA (可合并，无推理延迟)                         │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<table>
<thead><tr>
<th>方法</th>
<th>原理</th>
<th>推理延迟</th>
<th>可合并权重</th>
<th>参数量</th>
<th>效果</th>
</tr></thead>
<tbody>
<tr>
<td><strong>LoRA</strong></td>
<td>低秩分解 $\Delta W = BA$</td>
<td>无</td>
<td>✅</td>
<td>~0.1-1%</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong>Adapter</strong></td>
<td>串行插入 MLP</td>
<td>有</td>
<td>❌</td>
<td>~1-5%</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong>P-Tuning</strong></td>
<td>可学习 Soft Prompt</td>
<td>占 Context</td>
<td>❌</td>
<td>~0.01%</td>
<td>⭐⭐</td>
</tr>
<tr>
<td><strong>Prefix-Tuning</strong></td>
<td>可学习 KV 前缀</td>
<td>占 Context</td>
<td>❌</td>
<td>~0.1%</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong>P-Tuning v2</strong></td>
<td>每层 Prefix</td>
<td>占 Context</td>
<td>❌</td>
<td>~0.1-1%</td>
<td>⭐⭐⭐⭐</td>
</tr>
</tbody></table>

<h4>LoRA vs P-Tuning vs Adapter 核心差异</h4>

<table>
<thead><tr>
<th>对比维度</th>
<th>LoRA</th>
<th>P-Tuning</th>
<th>Adapter</th>
</tr></thead>
<tbody>
<tr>
<td><strong>修改位置</strong></td>
<td>权重矩阵 (W)</td>
<td>输入层 / 每层</td>
<td>层间插入</td>
</tr>
<tr>
<td><strong>推理时</strong></td>
<td>可合并，无延迟</td>
<td>需保留 Prompt</td>
<td>需保留 Adapter</td>
</tr>
<tr>
<td><strong>Context 占用</strong></td>
<td>无</td>
<td>有 (几十~几百 Token)</td>
<td>无</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>通用首选</td>
<td>小模型 / 特定任务</td>
<td>多任务切换</td>
</tr>
</tbody></table>

<hr></p>

<h3>5. LoRA 参数选择指南 (Hyperparameter Guide)</h3>

<h4>5.1 Rank (秩 $r$) 的影响</h4>

<table>
<thead><tr>
<th>$r$ 值</th>
<th>参数量</th>
<th>适用场景</th>
<th>风险</th>
</tr></thead>
<tbody>
<tr>
<td><strong>4-8</strong></td>
<td>最少</td>
<td>简单任务 (指令跟随、风格迁移)</td>
<td>欠拟合</td>
</tr>
<tr>
<td><strong>16-32</strong></td>
<td>适中</td>
<td>通用微调 (VLA 推荐)</td>
<td>平衡</td>
</tr>
<tr>
<td><strong>64-128</strong></td>
<td>较多</td>
<td>复杂推理、知识注入</td>
<td>过拟合</td>
</tr>
<tr>
<td><strong>256+</strong></td>
<td>接近全量</td>
<td>几乎等价于全量微调</td>
<td>失去 LoRA 优势</td>
</tr>
</tbody></table>

<h4>5.2 Alpha ($\alpha$) 的影响</h4>

<pre><code class="math">W = W_0 + \frac{\alpha}{r} \cdot BA
</code></pre>

<ul><li><strong>$\alpha / r$ 比值</strong>: 控制 LoRA 更新的"强度"</li>
<li><strong>常见设置</strong>: $\alpha = 2r$ (即 $\alpha/r = 2$)</li>
<li><strong>$\alpha$ 大</strong>: 更激进的更新，收敛快但可能不稳定</li>
<li><strong>$\alpha$ 小</strong>: 更保守的更新，稳定但收敛慢</li>
</ul>
<h4>5.3 目标层选择</h4>

<table>
<thead><tr>
<th>配置</th>
<th>目标层</th>
<th>参数量</th>
<th>效果</th>
</tr></thead>
<tbody>
<tr>
<td><strong>最小配置</strong></td>
<td><code>q_proj, v_proj</code></td>
<td>~0.1%</td>
<td>基础效果</td>
</tr>
<tr>
<td><strong>推荐配置</strong></td>
<td><code>q_proj, k_proj, v_proj, o_proj</code></td>
<td>~0.2%</td>
<td>较好效果</td>
</tr>
<tr>
<td><strong>完整配置</strong></td>
<td>Attention + MLP 全部</td>
<td>~1%</td>
<td>最佳效果</td>
</tr>
</tbody></table>

<pre><code class="python">from peft import LoraConfig</p>

<p>lora_config = LoraConfig(
    r=16,                    # 秩
    lora_alpha=32,           # alpha = 2r
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"       # MLP
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
</code></pre>

<hr></p>

<h3>6. 面试高频考点</h3>

<p><strong>Q: LoRA 的原理是什么？与 P-Tuning、Adapter 的异同点？</strong>
A: 
<ul><li><strong>LoRA</strong>: 将权重增量 $\Delta W$ 分解为低秩矩阵 $BA$，冻结原始权重只训练 $A, B$。推理时可合并回原权重，无额外延迟。</li>
<li><strong>P-Tuning</strong>: 在输入前加可学习的 Soft Prompt，占用 Context Window。</li>
<li><strong>Adapter</strong>: 在 Transformer 层间插入小型 MLP，推理时有额外延迟。</li>
<li><strong>核心区别</strong>: LoRA 修改权重本身，P-Tuning 修改输入，Adapter 修改结构。LoRA 是唯一可以"无痕合并"的方法。</li>
</ul>
<strong>Q: LoRA 的参数选择对模型性能有何影响？</strong>
A:
<ul><li><strong>Rank $r$</strong>: 控制表达能力。$r$ 小 (4-8) 适合简单任务，$r$ 大 (64+) 适合复杂任务但易过拟合。</li>
<li><strong>Alpha $\alpha$</strong>: 控制更新强度。通常 $\alpha = 2r$。</li>
<li><strong>目标层</strong>: 只训练 <code>q_proj, v_proj</code> 是最小配置，加上 MLP 效果更好。</li>
</ul>
<strong>Q: LoRA 的秩 $r$ 越大越好吗？</strong>
A: 不一定。对于简单的任务 (如指令跟随)，$r=8$ 足矣。对于复杂的逻辑推理或知识注入，$r$ 可能需要设大一点 (64 或 128)。但过大容易过拟合，且失去参数高效的优势。</p>

<p><strong>Q: 为什么 QLoRA 比 LoRA 慢？</strong>
A: 因为 QLoRA 在计算梯度时，需要将 4-bit 权重<strong>反量化 (Dequantize)</strong> 回 FP16/BF16 进行计算。这个反量化过程增加了计算开销。</p>

<p><strong>Q: VLA 模型微调应该微调哪些层？</strong>
A: 通常微调 <strong>Attention (q_proj, v_proj)</strong> 和 <strong>MLP (gate_proj, up_proj, down_proj)</strong> 效果最好。只微调 Attention 有时不够。</p>

<hr></p>

<h2>第11章 强化学习基础与 RLHF</h2>

<blockquote><strong>核心概念</strong>: 强化学习 (Reinforcement Learning, RL) 是一种通过与环境交互来学习最优策略的方法。智能体通过<strong>试错</strong> (Trial-and-Error) 和<strong>奖励反馈</strong> (Reward Feedback) 不断改进行为。</blockquote>

<h3>1. 为什么 VLA 需要强化学习? (Why RL for VLA?)</h3>

<h4>1.1 行为克隆 (BC) 的局限</h4>

<p>传统 VLA 主要依赖 <strong>行为克隆 (Behavior Cloning)</strong>：模仿人类演示。</p>

<table>
<thead><tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
</tr></thead>
<tbody>
<tr>
<td><strong>BC</strong></td>
<td>简单，数据效率高</td>
<td>上限是人类水平，无法超越演示者</td>
</tr>
<tr>
<td><strong>RL</strong></td>
<td>可以超越人类，自我进化</td>
<td>需要大量交互，稀疏奖励难优化</td>
</tr>
</tbody></table>

<h4>1.2 RL 在 VLA 中的价值</h4>

<ul><li><strong>超越人类示教</strong>: 通过自我博弈/探索发现更优策略</li>
<li><strong>长序列优化</strong>: BC 只模仿每步动作，RL 优化整个轨迹的累积回报</li>
<li><strong>适应性学习</strong>: 在真机部署时持续自我改进</li>
<li><strong>稀疏奖励任务</strong>: 只有任务成功时给奖励的场景（如组装）</li>
</ul>
<h4>1.3 VLA 中的 RL 应用案例</h4>

<ul><li><strong>π*0.6 (Pi-Star)</strong>: 使用 Recap 算法（Offline RL）超越人类示教</li>
<li><strong>RT-2</strong>: 使用 RL from Human Feedback (RLHF) 改进语义推理</li>
<li><strong>RoboCasa</strong>: 使用 PPO 训练家庭操作策略</li>
</ul>
<h3>2. RL 基础概念 (RL Fundamentals)</h3>

<h4>2.1 马尔可夫决策过程 (MDP)</h4>

<pre><code class="">MDP = (S, A, P, R, γ)
</code></pre>

<ul><li><strong>S</strong>: 状态空间 (State Space) - 机器人观测到的一切</li>
<li><strong>A</strong>: 动作空间 (Action Space) - 可执行的动作</li>
<li><strong>P(s'|s,a)</strong>: 状态转移概率 - 环境动力学</li>
<li><strong>R(s,a)</strong>: 奖励函数 - 行为好坏的反馈</li>
<li><strong>γ</strong>: 折扣因子 - 未来奖励的权重 (通常 0.99)</li>
</ul>
<h4>2.2 马尔可夫性 (Markov Property)</h4>

<p><strong>定义</strong>: 下一状态只依赖于当前状态和动作，与历史无关。</p>

<pre><code class="">P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)
</code></pre>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    马尔可夫性图解                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   非马尔可夫:  s₀ → s₁ → s₂ → s₃ → s₄                           │
│               └────────────────────┘                            │
│                  所有历史都影响未来                               │
│                                                                 │
│   马尔可夫:    s₀ → s₁ → s₂ → s₃ → s₄                           │
│                          └───┘                                  │
│                    只有 s₃ 影响 s₄                               │
│                                                                 │
│   💡 "当前状态是对历史的充分统计量"                              │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>为什么重要</strong>:
<ul><li><strong>简化计算</strong>: 不需要记忆整个历史，只需当前状态</li>
<li><strong>Bellman 方程成立</strong>: 价值函数可以递归定义</li>
<li><strong>实际应用</strong>: 机器人状态通常包含位置+速度，满足马尔可夫性；若只有位置则不满足</li>
</ul>
<h4>2.3 核心目标</h4>

<p>最大化<strong>累积折扣回报 (Cumulative Discounted Return)</strong>:</p>

<pre><code class="">G_t = Σ_{k=0}^{∞} γ^k × R_{t+k+1}
</code></pre>

<h4>2.4 价值函数 (Value Functions)</h4>

<p><strong>状态价值函数 (State Value)</strong>:</p>

<pre><code class="">V^π(s) = E_π[ G_t | S_t = s ]
</code></pre>

<p><strong>动作价值函数 (Action Value / Q-Function)</strong>:</p>

<pre><code class="">Q^π(s, a) = E_π[ G_t | S_t = s, A_t = a ]
</code></pre>

<p><strong>Bellman 方程</strong>:</p>

<pre><code class="">Q^π(s, a) = R(s, a) + γ × E_{s' ~ P}[ V^π(s') ]
</code></pre>

<h4>2.5 最优价值函数与最优策略</h4>

<p><strong>最优价值函数</strong>:</p>

<pre><code class="">V*(s) = max_π V^π(s)
Q*(s, a) = max_π Q^π(s, a)
</code></pre>

<p><strong>最优策略</strong>:</p>

<pre><code class="">π<em>(s) = argmax_a Q</em>(s, a)
</code></pre>

<p><strong>为什么最优价值函数就是最优策略？</strong></p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                最优价值函数 ↔ 最优策略                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   核心定理: 给定 Q*(s, a)，最优策略可直接导出                    │
│                                                                 │
│   π<em>(s) = argmax_a Q</em>(s, a)                                     │
│                                                                 │
│   证明思路:                                                     │
│   1. Q*(s,a) 表示在状态 s 执行动作 a 后，按最优策略行动的期望回报 │
│   2. 要最大化回报，只需在每个状态选择 Q* 最大的动作              │
│   3. 这正是贪心策略，而对于 Q* 贪心就是最优的                    │
│                                                                 │
│   反过来:                                                       │
│   给定最优策略 π<em>，可以计算出 Q</em>(s,a) = Q^{π*}(s,a)             │
│                                                                 │
│   💡 最优价值函数和最优策略是"一体两面"                          │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>数学推导</strong>:
<li><strong>Bellman 最优方程</strong>: <code>V<em>(s) = max_a [ R(s,a) + γ × Σ_{s'} P(s'|s,a) × V</em>(s') ]</code></li>
<li>如果我们知道 <code>V*</code>，则最优动作是使上式取最大值的 <code>a</code></li>
<li>这等价于 <code>π<em>(s) = argmax_a Q</em>(s,a)</code></li></p>

<h4>2.6 策略 (Policy)</h4>

<p>策略 <code>π(a|s)</code> 定义了在状态 <code>s</code> 下采取动作 <code>a</code> 的概率。</p>

<ul><li><strong>确定性策略</strong>: <code>a = π(s)</code></li>
<li><strong>随机策略</strong>: <code>a ~ π(·|s)</code></li>
</ul>
<h3>3. RL 算法分类 (RL Algorithm Taxonomy)</h3>

<pre><code class="">                        RL 算法
                           │
           ┌───────────────┼───────────────┐
           │               │               │
      Model-Free      Model-Based    Offline RL
           │               │               │
     ┌─────┴─────┐         │          ┌────┴────┐
     │           │         │          │         │
 Value-Based  Policy-Based │       CQL/IQL   Recap
     │           │         │
   DQN/SAC   PPO/TRPO   Dreamer/MBPO
</code></pre>

<h4>3.1 Model-Free vs Model-Based</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                Model-Free vs Model-Based                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Model-Free (无模型):                                          │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  Agent ←──── 交互 ────→ Environment                     │   │
│   │    │                         │                          │   │
│   │    └── 直接学习 π 或 Q ──────┘                          │   │
│   │        (不关心环境如何工作)                              │   │
│   └─────────────────────────────────────────────────────────┘   │
│   代表: DQN, PPO, SAC                                           │
│   特点: 简单直接，但需要大量交互数据                            │
│                                                                 │
│   Model-Based (基于模型):                                       │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  Agent ←──── 交互 ────→ Environment                     │   │
│   │    │                         │                          │   │
│   │    ├── 学习环境模型 P(s'|s,a) ┘                         │   │
│   │    │                                                    │   │
│   │    └── 在模型中规划/模拟 ──→ 策略                       │   │
│   └─────────────────────────────────────────────────────────┘   │
│   代表: Dreamer, MBPO, MuZero                                   │
│   特点: 样本高效，但模型误差会累积                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<table>
<thead><tr>
<th>类型</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Model-Free</strong></td>
<td>直接学习策略/价值</td>
<td>简单，无模型偏差</td>
<td>样本效率低</td>
<td>真机交互成本低</td>
</tr>
<tr>
<td><strong>Model-Based</strong></td>
<td>学习环境动力学 $P(s'</td>
<td>s,a)$</td>
<td>样本效率高</td>
<td>模型误差累积</td>
<td>仿真环境、Sim-to-Real</td>
</tr>
</tbody></table>

<p><strong>关键区别</strong>:
<ul><li><strong>Model-Free</strong>: 不尝试理解环境如何工作，只关心"什么动作能获得高回报"</li>
<li><strong>Model-Based</strong>: 先学习环境的"规则"（状态转移），再利用规则进行规划</li>
</ul>
<strong>VLA 中的应用</strong>:
<ul><li><strong>Model-Free</strong>: π0.6 的 Recap 算法（直接从数据学习策略）</li>
<li><strong>Model-Based</strong>: 世界模型 (World Model) 用于预测未来状态，辅助规划</li>
</ul>
<h4>3.2 策略迭代 vs 值迭代 (Policy Iteration vs Value Iteration)</h4>

<p>两种经典的动态规划算法，用于求解 MDP。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│              策略迭代 vs 值迭代                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   策略迭代 (Policy Iteration):                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  初始化 π₀                                               │   │
│   │      │                                                   │   │
│   │      ▼                                                   │   │
│   │  ┌──────────────────┐                                    │   │
│   │  │ 策略评估 (PE)    │ ← 计算 V^π (迭代至收敛)            │   │
│   │  │ V^π(s) = E[...]  │                                    │   │
│   │  └────────┬─────────┘                                    │   │
│   │           │                                              │   │
│   │           ▼                                              │   │
│   │  ┌──────────────────┐                                    │   │
│   │  │ 策略改进 (PI)    │ ← π(s) = argmax_a Q(s,a)           │   │
│   │  │ π ← greedy(V)    │                                    │   │
│   │  └────────┬─────────┘                                    │   │
│   │           │                                              │   │
│   │           └──── 重复直到 π 不再变化 ────┘                │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│   值迭代 (Value Iteration):                                     │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  初始化 V₀                                               │   │
│   │      │                                                   │   │
│   │      ▼                                                   │   │
│   │  ┌──────────────────────────────────────────────────┐   │   │
│   │  │ V(s) ← max_a [R(s,a) + γ Σ P(s'|s,a) V(s')]      │   │   │
│   │  │       (Bellman 最优方程的迭代更新)                │   │   │
│   │  └──────────────────────────────────────────────────┘   │   │
│   │           │                                              │   │
│   │           └──── 重复直到 V 收敛 ────┘                    │   │
│   │                                                          │   │
│   │  最后: π(s) = argmax_a Q(s,a)                            │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<table>
<thead><tr>
<th>对比</th>
<th>策略迭代</th>
<th>值迭代</th>
</tr></thead>
<tbody>
<tr>
<td><strong>核心操作</strong></td>
<td>评估 + 改进交替</td>
<td>直接迭代 Bellman 最优方程</td>
</tr>
<tr>
<td><strong>每轮计算</strong></td>
<td>策略评估需迭代至收敛</td>
<td>只做一次 Bellman 更新</td>
</tr>
<tr>
<td><strong>收敛速度</strong></td>
<td>迭代次数少</td>
<td>每轮计算量小</td>
</tr>
<tr>
<td><strong>总体效率</strong></td>
<td>大状态空间更快</td>
<td>小状态空间更快</td>
</tr>
<tr>
<td><strong>策略输出</strong></td>
<td>每轮都有显式策略</td>
<td>最后才提取策略</td>
</tr>
</tbody></table>

<p><strong>直觉理解</strong>:
<ul><li><strong>策略迭代</strong>: "先完整评估当前策略有多好，再改进"</li>
<li><strong>值迭代</strong>: "直接朝最优价值函数迭代，最后再提取策略"</li>
</ul>
<h4>3.3 On-Policy vs Off-Policy</h4>

<table>
<thead><tr>
<th>类型</th>
<th>代表算法</th>
<th>特点</th>
</tr></thead>
<tbody>
<tr>
<td><strong>On-Policy</strong></td>
<td>PPO, TRPO</td>
<td>只用当前策略的数据，稳定但低效</td>
</tr>
<tr>
<td><strong>Off-Policy</strong></td>
<td>SAC, TD3</td>
<td>可复用历史数据，高效但不稳定</td>
</tr>
</tbody></table>

<h3>4. VLA 常用 RL 算法 (RL Algorithms for VLA)</h3>

<h4>4.1 PPO (Proximal Policy Optimization)</h4>

<p><strong>核心思想</strong>: 限制策略更新幅度，防止训练崩溃。</p>

<p><strong>目标函数</strong>:</p>

<pre><code class="">L_CLIP(θ) = E_t[ min( r_t(θ) × Â_t, clip(r_t(θ), 1-ε, 1+ε) × Â_t ) ]
</code></pre>

<p>其中 <code>r_t(θ) = π_θ(a_t|s_t) / π_{θ_old}(a_t|s_t)</code> 是重要性采样比率。</p>

<pre><code class="python">import torch
import torch.nn as nn
import torch.nn.functional as F</p>

<p>class PPO:
    def __init__(self, policy, value_net, clip_epsilon=0.2, lr=3e-4):
        self.policy = policy
        self.value_net = value_net
        self.clip_epsilon = clip_epsilon
        self.optimizer = torch.optim.Adam(
            list(policy.parameters()) + list(value_net.parameters()), lr=lr
        )
    
    def compute_gae(self, rewards, values, dones, gamma=0.99, lam=0.95):
        """Generalized Advantage Estimation"""
        advantages = []
        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma <em> next_value </em> (1 - dones[t]) - values[t]
            gae = delta + gamma <em> lam </em> (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        return torch.tensor(advantages)
    
    def update(self, states, actions, old_log_probs, rewards, dones):
        # 计算价值和优势
        values = self.value_net(states).squeeze()
        advantages = self.compute_gae(rewards, values.detach(), dones)
        returns = advantages + values.detach()
        
        # 归一化优势
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO 更新
        for _ in range(10):  # 多轮更新
            # 计算新的 log_prob
            dist = self.policy(states)
            new_log_probs = dist.log_prob(actions)
            
            # 重要性采样比率
            ratio = torch.exp(new_log_probs - old_log_probs)
            
            # Clipped 目标
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # 价值损失
            value_loss = F.mse_loss(self.value_net(states).squeeze(), returns)
            
            # 熵正则化 (鼓励探索)
            entropy = dist.entropy().mean()
            
            # 总损失
            loss = policy_loss + 0.5 <em> value_loss - 0.01 </em> entropy
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
</code></pre>

<h4>4.2 SAC (Soft Actor-Critic)</h4>

<p><strong>核心思想</strong>: 最大化奖励的同时最大化策略熵（鼓励探索）。</p>

<p><strong>目标</strong>:</p>

<pre><code class="">J(π) = E_{τ ~ π}[ Σ_t R(s_t, a_t) + α × H(π(·|s_t)) ]
</code></pre>

<pre><code class="python">class SAC:
    def __init__(self, actor, critic1, critic2, target_critic1, target_critic2, 
                 alpha=0.2, gamma=0.99, tau=0.005):
        self.actor = actor
        self.critic1 = critic1
        self.critic2 = critic2
        self.target_critic1 = target_critic1
        self.target_critic2 = target_critic2
        self.alpha = alpha  # 熵系数
        self.gamma = gamma
        self.tau = tau
    
    def update(self, batch):
        states, actions, rewards, next_states, dones = batch
        
        # ===== Critic 更新 =====
        with torch.no_grad():
            # 从当前策略采样下一个动作
            next_actions, next_log_probs = self.actor.sample(next_states)
            
            # 目标 Q 值 (取两个 critic 的最小值)
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            
            # TD 目标
            target = rewards + self.gamma <em> (1 - dones) </em> target_q
        
        # 当前 Q 值
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic_loss = F.mse_loss(current_q1, target) + F.mse_loss(current_q2, target)
        
        # ===== Actor 更新 =====
        new_actions, log_probs = self.actor.sample(states)
        q1 = self.critic1(states, new_actions)
        q2 = self.critic2(states, new_actions)
        q = torch.min(q1, q2)
        
        # 最大化 Q - α * log_prob
        actor_loss = (self.alpha * log_probs - q).mean()
        
        # ===== 软更新目标网络 =====
        self.soft_update(self.critic1, self.target_critic1)
        self.soft_update(self.critic2, self.target_critic2)
        
        return critic_loss, actor_loss
    
    def soft_update(self, source, target):
        for src_param, tgt_param in zip(source.parameters(), target.parameters()):
            tgt_param.data.copy_(self.tau <em> src_param.data + (1 - self.tau) </em> tgt_param.data)
</code></pre>

<h4>4.3 Offline RL (离线强化学习)</h4>

<p><strong>核心问题</strong>: 只有固定的历史数据集，无法与环境交互。</p>

<p><strong>挑战</strong>: 分布偏移 (Distribution Shift) - 策略可能选择数据集中没见过的动作。</p>

<h4>4.3.1 CQL (Conservative Q-Learning)</h4>

<p><strong>思想</strong>: 保守估计 Q 值，惩罚数据集外的动作。</p>

<pre><code class="">L_CQL = α × E_{s ~ D}[ log Σ_a exp(Q(s,a)) - E_{a ~ D}[Q(s,a)] ] + L_TD
</code></pre>

<h4>4.3.2 IQL (Implicit Q-Learning)</h4>

<p><strong>思想</strong>: 使用分位数回归避免显式最大化 Q。</p>

<h4>4.3.3 Recap (π*0.6 的核心算法)</h4>

<p>+<strong>思想</strong>: 从成功和失败的轨迹中学习（详细机制可参考 <a href="./pi0_6_dissection.md">pi0_6_dissection.md</a> 中的 Recap 解析）。</p>

<pre><code class="python">class RecapAlgorithm:
    """π*0.6 的 Recap 离线 RL 算法"""
    def __init__(self, policy, value_net):
        self.policy = policy
        self.value_net = value_net
    
    def label_trajectories(self, trajectories):
        """标注轨迹: 成功 vs 失败"""
        labeled = []
        for traj in trajectories:
            success = traj['final_reward'] &gt; 0  # 任务是否成功
            for t, transition in enumerate(traj['transitions']):
                # 关键: 找到失败轨迹中"开始出错"的时刻
                if not success and self.is_critical_failure(traj, t):
                    transition['label'] = 'negative'  # 负样本
                elif success:
                    transition['label'] = 'positive'  # 正样本
                labeled.append(transition)
        return labeled
    
    def is_critical_failure(self, traj, t):
        """判断是否是导致失败的关键时刻"""
        # 使用价值函数估计: 如果 V 骤降，说明这步出错了
        v_t = self.value_net(traj['states'][t])
        v_t1 = self.value_net(traj['states'][t+1])
        return (v_t - v_t1) &gt; threshold
    
    def update(self, labeled_data):
        """对比学习: 提升正样本概率，降低负样本概率"""
        loss = 0
        for sample in labeled_data:
            state, action = sample['state'], sample['action']
            log_prob = self.policy.log_prob(state, action)
            
            if sample['label'] == 'positive':
                loss -= log_prob  # 提升正样本概率
            else:
                loss += log_prob  # 降低负样本概率
        
        return loss / len(labeled_data)
</code></pre>

<h3>5. 奖励设计 (Reward Engineering)</h3>

<h4>5.1 稀疏奖励 vs 稠密奖励</h4>

<table>
<thead><tr>
<th>类型</th>
<th>示例</th>
<th>优点</th>
<th>缺点</th>
</tr></thead>
<tbody>
<tr>
<td><strong>稀疏</strong></td>
<td>任务成功 +1，否则 0</td>
<td>不需要人工设计</td>
<td>难以学习</td>
</tr>
<tr>
<td><strong>稠密</strong></td>
<td>距离目标越近奖励越高</td>
<td>学习容易</td>
<td>可能导致局部最优</td>
</tr>
</tbody></table>

<h4>5.2 奖励设计示例</h4>

<pre><code class="python">def compute_reward(state, action, next_state, task_type="pick_and_place"):
    """机器人操作任务的奖励函数"""
    
    if task_type == "pick_and_place":
        gripper_pos = state['gripper_position']
        object_pos = state['object_position']
        target_pos = state['target_position']
        
        # 阶段 1: 接近物体
        dist_to_object = np.linalg.norm(gripper_pos - object_pos)
        
        # 阶段 2: 抓取物体
        is_grasping = state['gripper_closed'] and dist_to_object &lt; 0.02
        
        # 阶段 3: 移动到目标
        if is_grasping:
            dist_to_target = np.linalg.norm(object_pos - target_pos)
        else:
            dist_to_target = 1.0  # 惩罚没抓到物体
        
        # 组合奖励
        reward = -0.1 * dist_to_object  # 接近物体
        reward += 0.5 * is_grasping     # 抓取奖励
        reward -= 0.1 * dist_to_target  # 接近目标
        
        # 稀疏成功奖励
        if dist_to_target &lt; 0.05 and is_grasping:
            reward += 10.0  # 任务成功
        
        return reward
</code></pre>

<h4>5.3 奖励塑形 (Reward Shaping)</h4>

<pre><code class="python">def shaped_reward(state, next_state, potential_func, gamma=0.99):
    """
    基于势函数的奖励塑形 (不改变最优策略)
    F(s, s') = γ * Φ(s') - Φ(s)
    """
    phi_s = potential_func(state)
    phi_s_next = potential_func(next_state)
    shaping = gamma * phi_s_next - phi_s
    return shaping
</code></pre>

<h3>6. RL + VLA 的结合 (RL + VLA Integration)</h3>

<h4>6.1 RLHF 完整流程 (RLHF Pipeline)</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    RLHF 三阶段流程                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   阶段 1: SFT (Supervised Fine-Tuning)                          │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  预训练模型 + 高质量数据 → 基础策略 π_SFT                 │   │
│   └─────────────────────────────────────────────────────────┘   │
│                          │                                      │
│                          ▼                                      │
│   阶段 2: Reward Model Training                                 │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  收集人类偏好 (A &gt; B) → 训练奖励模型 R(s, a)              │   │
│   │  Loss: -log σ(R(y_w) - R(y_l))  (Bradley-Terry)          │   │
│   └─────────────────────────────────────────────────────────┘   │
│                          │                                      │
│                          ▼                                      │
│   阶段 3: RL Fine-tuning (PPO)                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  使用 R(s, a) 作为奖励，PPO 优化策略                      │   │
│   │  + KL 惩罚: 防止偏离 π_SFT 太远                          │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│   需要同时加载: π_SFT (参考), π_θ (训练), R (奖励), V (价值)    │
│   显存需求: 4 个模型 → 非常昂贵!                                │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<pre><code class="python">class RLHF_VLA:
    """使用人类反馈强化学习改进 VLA"""
    def __init__(self, vla_model, reward_model):
        self.vla = vla_model
        self.reward_model = reward_model  # 从人类偏好训练
    
    def collect_comparisons(self, states, num_samples=2):
        """收集人类偏好比较"""
        actions = [self.vla.sample(states) for _ in range(num_samples)]
        # 人类选择更好的动作
        human_preference = get_human_preference(states, actions)
        return actions, human_preference
    
    def train_reward_model(self, comparisons):
        """从偏好数据训练奖励模型"""
        for (action_win, action_lose, state) in comparisons:
            r_win = self.reward_model(state, action_win)
            r_lose = self.reward_model(state, action_lose)
            
            # Bradley-Terry 模型
            loss = -torch.log(torch.sigmoid(r_win - r_lose))
            loss.backward()
    
    def rl_finetune(self, states):
        """使用学到的奖励进行 RL 微调"""
        actions = self.vla.sample(states)
        rewards = self.reward_model(states, actions)
        
        # PPO 更新
        self.ppo_update(states, actions, rewards)
</code></pre>

<h4>6.2 从演示初始化 RL (Demo-Guided RL)</h4>

<pre><code class="python">class DemoGuidedRL:
    """结合 BC 预训练和 RL 微调"""
    def __init__(self, policy):
        self.policy = policy
    
    def phase1_bc_pretrain(self, demonstrations):
        """Phase 1: 行为克隆预训练"""
        for state, action in demonstrations:
            pred_action = self.policy(state)
            loss = F.mse_loss(pred_action, action)
            loss.backward()
    
    def phase2_rl_finetune(self, env, num_episodes=1000):
        """Phase 2: RL 微调超越演示"""
        for episode in range(num_episodes):
            state = env.reset()
            while not done:
                action = self.policy.sample(state)
                next_state, reward, done, _ = env.step(action)
                
                # 存储经验
                self.buffer.add(state, action, reward, next_state, done)
                state = next_state
            
            # SAC/PPO 更新
            self.rl_update()
</code></pre>

<h3>7. 面试高频问题 (Q&A)</h3>

<p><strong>Q1: VLA 中 BC 和 RL 如何取舍?</strong></p>

<p>A:
<ul><li><strong>优先 BC</strong>: 数据充足、任务简单、需要快速迭代</li>
<li><strong>引入 RL</strong>: 需要超越人类、长序列优化、稀疏奖励任务</li>
<li><strong>最佳实践</strong>: BC 预训练 + RL 微调 (如 π*0.6)</li>
</ul>
<strong>Q2: Offline RL 和 Online RL 的核心区别?</strong></p>

<p>A:
<ul><li><strong>Online RL</strong>: 可以与环境交互，探索新状态</li>
<li><strong>Offline RL</strong>: 只有固定数据集，需要处理分布偏移</li>
<li><strong>VLA 现状</strong>: 因为真机交互成本高，Offline RL 更实用</li>
</ul>
<strong>Q3: SAC 中温度参数 α 的作用?</strong></p>

<p>A:
<ul><li><strong>α 大</strong>: 更重视熵 → 更多探索 → 策略更随机</li>
<li><strong>α 小</strong>: 更重视奖励 → 更多利用 → 策略更确定</li>
<li><strong>自动调节</strong>: 可以将 α 设为可学习参数</li>
</ul>
<strong>Q4: 为什么 PPO 比 TRPO 更流行?</strong></p>

<p>A:
<ul><li><strong>简单</strong>: PPO 只需 Clip，TRPO 需要计算 Fisher 矩阵</li>
<li><strong>高效</strong>: PPO 可以用 SGD，TRPO 需要共轭梯度</li>
<li><strong>效果</strong>: 在大多数任务上性能相当</li>
</ul>
<strong>Q5: Recap 算法相比传统 Offline RL 的优势?</strong></p>

<p>A:
<ul><li><strong>利用失败数据</strong>: 传统方法只模仿成功轨迹，Recap 从失败中学习</li>
<li><strong>关键时刻识别</strong>: 通过价值函数定位"出错点"</li>
<li><strong>简单高效</strong>: 不需要复杂的约束优化</li>
</ul>
<strong>Q6: RLHF 的基本流程是什么？与 DPO 的差异是什么？</strong></p>

<p>A: <strong>RLHF 三阶段流程</strong>:
<li><strong>SFT</strong>: 在高质量数据上监督微调，得到基础策略 $\pi_{SFT}$</li>
<li><strong>Reward Model</strong>: 从人类偏好数据训练奖励模型 $R(s, a)$</li>
<li><strong>RL (PPO)</strong>: 使用 $R$ 作为奖励，PPO 优化策略，加 KL 惩罚防止偏离 $\pi_{SFT}$</li></p>

<p><strong>DPO (Direct Preference Optimization)</strong> 的差异:
<ul><li><strong>跳过 Reward Model</strong>: 直接从偏好数据优化策略</li>
<li><strong>数学推导</strong>: 将 RL 目标重参数化为分类问题</li>
</ul>
<pre><code class="">L_DPO = -E[ log σ( β × log(π_θ(y_w|x) / π_ref(y_w|x)) - β × log(π_θ(y_l|x) / π_ref(y_l|x)) ) ]
</code></pre>

<table>
<thead><tr>
<th>对比</th>
<th>RLHF</th>
<th>DPO</th>
</tr></thead>
<tbody>
<tr>
<td><strong>阶段数</strong></td>
<td>3 阶段</td>
<td>1 阶段</td>
</tr>
<tr>
<td><strong>模型数</strong></td>
<td>4 个 (π, π_ref, R, V)</td>
<td>2 个 (π, π_ref)</td>
</tr>
<tr>
<td><strong>稳定性</strong></td>
<td>PPO 易崩</td>
<td>更稳定</td>
</tr>
<tr>
<td><strong>计算量</strong></td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td><strong>效果</strong></td>
<td>略优 (理论上)</td>
<td>接近 RLHF</td>
</tr>
</tbody></table>

<h3>8. 主流 RL 框架 (RL Frameworks)</h3>

<h4>8.1 框架对比</h4>

<table>
<thead><tr>
<th>框架</th>
<th>定位</th>
<th>优势</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Stable Baselines3</strong></td>
<td>易用、稳定</td>
<td>文档完善，API 简洁</td>
<td>快速实验、教学</td>
</tr>
<tr>
<td><strong>RLlib</strong></td>
<td>分布式、可扩展</td>
<td>Ray 生态，多 GPU/节点</td>
<td>大规模训练</td>
</tr>
<tr>
<td><strong>CleanRL</strong></td>
<td>单文件实现</td>
<td>代码清晰，易于修改</td>
<td>学习、研究</td>
</tr>
<tr>
<td><strong>TorchRL</strong></td>
<td>PyTorch 官方</td>
<td>与 PyTorch 深度集成</td>
<td>生产级应用</td>
</tr>
<tr>
<td><strong>SKRL</strong></td>
<td>Isaac Lab 集成</td>
<td>GPU 并行，机器人专用</td>
<td>机器人 RL</td>
</tr>
</tbody></table>

<h4>8.2 Stable Baselines3 (SB3)</h4>

<p><strong>特点</strong>: 最易上手的 RL 库，API 设计优雅。</p>

<pre><code class="python">from stable_baselines3 import PPO, SAC, TD3
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback</p>

<p>env = make_vec_env("Pendulum-v1", n_envs=4)</p>

<p>model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    verbose=1,
    tensorboard_log="./logs/"
)</p>

<p>model.learn(total_timesteps=100_000, callback=EvalCallback(env))</p>

<p>model.save("ppo_pendulum")
model = PPO.load("ppo_pendulum")</p>

<p>obs = env.reset()
for _ in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
</code></pre>

<p><strong>自定义网络</strong>:</p>

<pre><code class="python">from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import torch.nn as nn</p>

<p>class CustomCNN(BaseFeaturesExtractor):
    """自定义 CNN 特征提取器 (用于图像输入)"""
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        n_input_channels = observation_space.shape[0]
        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
        )
        # 计算输出维度
        with torch.no_grad():
            n_flatten = self.cnn(torch.zeros(1, *observation_space.shape)).shape[1]
        self.linear = nn.Linear(n_flatten, features_dim)
    
    def forward(self, observations):
        return self.linear(self.cnn(observations))</p>

<p>policy_kwargs = dict(
    features_extractor_class=CustomCNN,
    features_extractor_kwargs=dict(features_dim=256),
)
model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs)
</code></pre>

<h4>8.3 RLlib (Ray)</h4>

<p><strong>特点</strong>: 分布式训练首选，支持多 GPU/多节点。</p>

<pre><code class="python">from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig</p>

<p>config = (
    PPOConfig()
    .environment("CartPole-v1")
    .framework("torch")
    .rollouts(
        num_rollout_workers=4,      # 并行 worker 数
        num_envs_per_worker=2,      # 每个 worker 的环境数
    )
    .training(
        lr=5e-5,
        gamma=0.99,
        train_batch_size=4000,
        sgd_minibatch_size=128,
        num_sgd_iter=30,
        model={"fcnet_hiddens": [256, 256]},
    )
    .resources(
        num_gpus=1,                  # 训练用 GPU
        num_cpus_per_worker=1,
    )
)</p>

<p>algo = config.build()
for i in range(100):
    result = algo.train()
    print(f"Iter {i}: reward = {result['episode_reward_mean']:.2f}")</p>

<p>tune.run(
    "PPO",
    config=config.to_dict(),
    stop={"episode_reward_mean": 200},
    num_samples=4,  # 并行搜索 4 组超参
)
</code></pre>

<p><strong>多智能体 RL</strong>:</p>

<pre><code class="python">from ray.rllib.algorithms.ppo import PPOConfig</p>

<p>config = (
    PPOConfig()
    .environment("MultiAgentCartPole")
    .multi_agent(
        policies={"policy_0", "policy_1"},
        policy_mapping_fn=lambda agent_id, episode, **kwargs: f"policy_{agent_id}",
    )
)
</code></pre>

<h4>8.4 SKRL (Isaac Lab 集成)</h4>

<p><strong>特点</strong>: 专为 Isaac Lab 设计，GPU 并行训练。</p>

<pre><code class="python">from skrl.agents.torch.ppo import PPO, PPO_DEFAULT_CONFIG
from skrl.trainers.torch import SequentialTrainer
from skrl.envs.wrappers.torch import wrap_env</p>

<p>env = wrap_env(isaac_lab_env)</p>

<p>cfg = PPO_DEFAULT_CONFIG.copy()
cfg["rollouts"] = 16
cfg["learning_epochs"] = 8
cfg["mini_batches"] = 4
cfg["discount_factor"] = 0.99
cfg["lambda"] = 0.95
cfg["learning_rate"] = 3e-4</p>

<p>agent = PPO(
    models={"policy": policy_net, "value": value_net},
    memory=memory,
    cfg=cfg,
    observation_space=env.observation_space,
    action_space=env.action_space,
)</p>

<p>trainer = SequentialTrainer(cfg=trainer_cfg, env=env, agents=agent)
trainer.train()
</code></pre>

<h4>8.5 框架选择指南</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    RL 框架选择决策树                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Q1: 是否需要分布式训练 (多 GPU/多节点)?                        │
│       │                                                         │
│       ├── 是 → RLlib (Ray 生态，分布式首选)                     │
│       │                                                         │
│       └── 否 → Q2: 是否使用 Isaac Lab?                          │
│                    │                                            │
│                    ├── 是 → SKRL (官方推荐)                     │
│                    │                                            │
│                    └── 否 → Q3: 目标是什么?                     │
│                                 │                               │
│                                 ├── 快速实验 → SB3              │
│                                 ├── 学习研究 → CleanRL          │
│                                 └── 生产部署 → TorchRL          │
│                                                                 │
│   💡 VLA 常用组合:                                              │
│   • 仿真训练: Isaac Lab + SKRL/RSL-RL                           │
│   • 大规模: RLlib + Ray Cluster                                 │
│   • 快速验证: SB3 + Gymnasium                                   │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>8.6 TORCS 与自动驾驶 RL</h4>

<p><strong>TORCS</strong> (The Open Racing Car Simulator) 是经典的自动驾驶 RL 测试平台。</p>

<pre><code class="python">import gym
import gym_torcs</p>

<p>env = gym.make("Torcs-v0", vision=True, throttle=True)</p>

<p>obs = env.reset()
for _ in range(1000):
    action = agent.act(obs)  # 你的策略
    obs, reward, done, info = env.step(action)
    if done:
        obs = env.reset()
</code></pre>

<p><strong>注</strong>: TORCS 主要用于自动驾驶研究，VLA 机器人领域更常用 Isaac Lab/MuJoCo。</p>

<h3>9. LIBERO 终身学习基准 (Lifelong Learning Benchmark)</h3>

<blockquote><strong>LIBERO</strong>: Benchmarking Knowledge Transfer for Lifelong Robot Learning <a href="https://github.com/Lifelong-Robot-Learning/LIBERO">[GitHub</a>] <a href="https://arxiv.org/abs/2306.03310">[Paper</a>]</blockquote>

<h4>9.1 为什么关注 LIBERO?</h4>

<ul><li><strong>主流基准</strong>: 多任务 / 终身学习 / 知识迁移研究默认引用的数据与任务集</li>
<li><strong>任务覆盖全面</strong>: 提供受控分布偏移 (Spatial / Object / Goal) 与 entangled 任务 (LIBERO-100)</li>
<li><strong>开箱即用</strong>: 打包示范数据、评测脚本、策略网络 (BC-RNN / BC-Transformer / BC-ViLT) 与算法 (base、ER、EWC、PackNet、Multitask)</li>
<li><strong>可扩展</strong>: Procedural generation pipeline 支持生成更多 manipulation 任务，方便自定义研究</li>
</ul>
<h4>9.2 任务套件总览</h4>

<table>
<thead><tr>
<th>套件</th>
<th>任务数</th>
<th>迁移挑战</th>
<th>说明</th>
</tr></thead>
<tbody>
<tr>
<td><strong>LIBERO-Spatial</strong></td>
<td>30</td>
<td>空间关系</td>
<td>相同物体，不同空间布置</td>
</tr>
<tr>
<td><strong>LIBERO-Object</strong></td>
<td>30</td>
<td>物体类型</td>
<td>不同物体属性，考验语义+抓取泛化</td>
</tr>
<tr>
<td><strong>LIBERO-Goal</strong></td>
<td>30</td>
<td>目标变化</td>
<td>同场景，多目标组合</td>
</tr>
<tr>
<td><strong>LIBERO-100</strong></td>
<td>100</td>
<td>混合 (Entangled)</td>
<td>拆成 <strong>LIBERO-90</strong> (预训练) + <strong>LIBERO-10</strong> (终身学习测试)</td>
</tr>
</tbody></table>

<h4>9.3 数据与环境</h4>

<pre><code class="bash">conda create -n libero python=3.8.13
conda activate libero
git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git
cd LIBERO
pip install -r requirements.txt
pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 \
            torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113
pip install -e .</p>

<p>python benchmark_scripts/download_libero_datasets.py --use-huggingface
</code></pre>

<h4>9.4 训练 / 评测入口</h4>

<pre><code class="bash">export CUDA_VISIBLE_DEVICES=0
python libero/lifelong/main.py \
    seed=0 \
    benchmark_name=LIBERO_90 \
    policy=bc_transformer_policy \
    lifelong=ewc</p>

<p>python libero/lifelong/evaluate.py \
    --benchmark LIBERO_10 \
    --task_id 0 \
    --algo ewc \
    --policy bc_transformer_policy \
    --ep 50 \
    --device_id 0
</code></pre>

<h4>9.5 与 VLA 的结合方式</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                LIBERO + VLA 结合流程                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   1. 数据层: 复用 LIBERO 示范训练 / 微调 VLA 的 Action Head     │
│                                                                 │
│   2. 评测层: 作为"终身学习回归测试集"，检查 Catastrophic        │
│              Forgetting                                         │
│                                                                 │
│   3. 跨域迁移: 先用 LIBERO-90 预训练，再在 LIBERO-10 /          │
│               自定义任务 → 真机验证 Sim-to-Real                  │
│                                                                 │
│   4. 算法组合: 将 RLHF / DPO 等上层优化与 EWC / PackNet 等      │
│               底层正则结合，形成混合 pipeline                    │
│                                                                 │
│   💡 面试常问: "如何评估 VLA 的终身学习能力？"                   │
│      → 使用 LIBERO-90 预训练 + LIBERO-10 测试遗忘程度            │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>9.6 面试 Q&A</h4>

<p><strong>Q: 如何使用 LIBERO 评估 VLA 的终身学习能力？</strong></p>

<p>A:
<li><strong>预训练</strong>: 在 LIBERO-90 上训练 VLA 策略</li>
<li><strong>终身学习</strong>: 在 LIBERO-10 的 10 个任务上顺序微调</li>
<li><strong>评估遗忘</strong>: 每学完一个新任务后，测试所有旧任务的成功率</li>
<li><strong>对比算法</strong>: 比较 Sequential Fine-tuning (baseline) vs EWC / PackNet / ER</li>
<li><strong>关键指标</strong>: Forward Transfer (新任务学习速度) + Backward Transfer (旧任务遗忘程度)</li></p>

<h3>10. 参考资源 (References)</h3>

<ul><li><strong>PPO</strong>: <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a></li>
<li><strong>SAC</strong>: <a href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL</a></li>
<li><strong>CQL</strong>: <a href="https://arxiv.org/abs/2006.04779">Conservative Q-Learning for Offline RL</a></li>
<li><strong>IQL</strong>: <a href="https://arxiv.org/abs/2110.06169">Offline RL with Implicit Q-Learning</a></li>
<li><strong>Spinning Up</strong>: <a href="https://spinningup.openai.com/">OpenAI Spinning Up in Deep RL</a></li>
<li><strong>Stable Baselines3</strong>: <a href="https://stable-baselines3.readthedocs.io/">Docs</a></li>
<li><strong>RLlib</strong>: <a href="https://docs.ray.io/en/latest/rllib/">Docs</a></li>
<li><strong>SKRL</strong>: <a href="https://skrl.readthedocs.io/">Docs</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第12章 知识蒸馏</h2>

<blockquote><strong>核心概念</strong>: 知识蒸馏 (Knowledge Distillation, KD) 是一种模型压缩技术，通过让小模型（学生）模仿大模型（教师）的行为，实现知识的迁移。在 VLA 领域，知识蒸馏是实现边缘端部署的关键技术。</blockquote>

<h3>1. 为什么 VLA 需要知识蒸馏? (Why KD for VLA?)</h3>

<h4>1.1 VLA 部署挑战</h4>

<table>
<thead><tr>
<th>模型</th>
<th>参数量</th>
<th>推理延迟</th>
<th>显存需求</th>
</tr></thead>
<tbody>
<tr>
<td><strong>RT-2 (PaLI-X)</strong></td>
<td>55B</td>
<td>~5s</td>
<td>100+ GB</td>
</tr>
<tr>
<td><strong>OpenVLA</strong></td>
<td>7B</td>
<td>~200ms</td>
<td>16 GB</td>
</tr>
<tr>
<td><strong>目标 (边缘)</strong></td>
<td>< 1B</td>
<td>< 50ms</td>
<td>< 4 GB</td>
</tr>
</tbody></table>

<p><strong>现实约束</strong>:
<ul><li><strong>实时性</strong>: 机器人控制需要 20-50Hz (20-50ms)</li>
<li><strong>硬件限制</strong>: 机载计算通常只有 Jetson Orin (8-32GB)</li>
<li><strong>功耗</strong>: 移动机器人对功耗敏感</li>
</ul>
<h4>1.2 知识蒸馏的价值</h4>

<pre><code class="math">\text{小模型性能} + \text{大模型知识} \approx \text{大模型性能}
</code></pre>

<ul><li><strong>模型压缩</strong>: 10x 参数减少，性能损失 < 10%</li>
<li><strong>推理加速</strong>: 降低延迟，满足实时控制需求</li>
<li><strong>降低成本</strong>: 减少部署硬件需求</li>
</ul>
<h3>2. 知识蒸馏基础 (KD Fundamentals)</h3>

<h4>2.1 基本框架</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────┐
│                    Teacher Model (大模型)                    │
│                    - 预训练好的 VLA                          │
│                    - 参数冻结                               │
└──────────────────────────┬──────────────────────────────────┘
                           │ Soft Labels / Logits
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                     Distillation Loss                       │
│              L = α <em> L_hard + (1-α) </em> L_soft                │
└──────────────────────────┬──────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                   Student Model (小模型)                     │
│                    - 目标部署模型                            │
│                    - 可训练                                 │
└─────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2.2 软标签 (Soft Labels)</h4>

<p><strong>硬标签 (Hard Labels)</strong>: One-hot 向量，只有正确类别为 1。</p>

<p><strong>软标签 (Soft Labels)</strong>: 教师模型的 softmax 概率分布。</p>

<pre><code class="math">p_i^{(T)} = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
</code></pre>

<p>其中 $T$ 是<strong>温度参数</strong> (Temperature)：
<ul><li><strong>T = 1</strong>: 正常 softmax</li>
<li><strong>T > 1</strong>: 分布更平滑，保留更多"暗知识"</li>
<li><strong>T → ∞</strong>: 均匀分布</li>
</ul>
<h4>2.3 蒸馏损失 (Distillation Loss)</h4>

<pre><code class="python">import torch
import torch.nn as nn
import torch.nn.functional as F</p>

<p>class DistillationLoss(nn.Module):
    def __init__(self, temperature=4.0, alpha=0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
    
    def forward(self, student_logits, teacher_logits, labels):
        """
        student_logits: 学生模型输出
        teacher_logits: 教师模型输出
        labels: 真实标签 (用于硬标签损失)
        """
        # 硬标签损失 (与真实标签)
        hard_loss = F.cross_entropy(student_logits, labels)
        
        # 软标签损失 (与教师)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=-1)
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=-1)
        soft_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
        soft_loss = soft_loss <em> (self.temperature </em>* 2)  # 温度缩放
        
        # 组合损失
        total_loss = self.alpha <em> hard_loss + (1 - self.alpha) </em> soft_loss
        return total_loss
</code></pre>

<h3>3. VLA 中的知识蒸馏策略 (KD Strategies for VLA)</h3>

<h4>3.1 特征蒸馏 (Feature Distillation)</h4>

<p>除了输出层，还蒸馏中间特征。</p>

<pre><code class="python">class FeatureDistillation(nn.Module):
    def __init__(self, teacher, student, feature_layers):
        super().__init__()
        self.teacher = teacher
        self.student = student
        self.feature_layers = feature_layers
        
        # 特征投影层 (对齐维度)
        self.projectors = nn.ModuleDict()
        for name in feature_layers:
            t_dim = teacher.get_feature_dim(name)
            s_dim = student.get_feature_dim(name)
            if t_dim != s_dim:
                self.projectors[name] = nn.Linear(s_dim, t_dim)
    
    def forward(self, inputs):
        # 获取教师特征
        with torch.no_grad():
            teacher_features = self.teacher.get_features(inputs, self.feature_layers)
        
        # 获取学生特征
        student_features = self.student.get_features(inputs, self.feature_layers)
        
        # 特征对齐损失
        feature_loss = 0
        for name in self.feature_layers:
            t_feat = teacher_features[name]
            s_feat = student_features[name]
            
            if name in self.projectors:
                s_feat = self.projectors<a href="s_feat">name</a>
            
            feature_loss += F.mse_loss(s_feat, t_feat)
        
        return feature_loss / len(self.feature_layers)
</code></pre>

<h4>3.2 注意力蒸馏 (Attention Distillation)</h4>

<p>让学生模型学习教师的注意力模式。</p>

<pre><code class="python">class AttentionDistillation(nn.Module):
    def __init__(self, teacher, student):
        super().__init__()
        self.teacher = teacher
        self.student = student
    
    def forward(self, inputs):
        # 获取注意力权重
        with torch.no_grad():
            _, teacher_attns = self.teacher(inputs, output_attentions=True)
        
        _, student_attns = self.student(inputs, output_attentions=True)
        
        # 注意力对齐损失
        attn_loss = 0
        for t_attn, s_attn in zip(teacher_attns, student_attns):
            # t_attn, s_attn: [B, num_heads, seq_len, seq_len]
            attn_loss += F.mse_loss(s_attn, t_attn)
        
        return attn_loss / len(teacher_attns)
</code></pre>

<h4>3.3 动作轨迹蒸馏 (Action Trajectory Distillation)</h4>

<p>VLA 特有：蒸馏整个动作序列。</p>

<pre><code class="python">class ActionTrajectoryDistillation(nn.Module):
    """VLA 专用: 蒸馏动作轨迹"""
    def __init__(self, teacher_vla, student_vla, chunk_size=16):
        super().__init__()
        self.teacher = teacher_vla
        self.student = student_vla
        self.chunk_size = chunk_size
    
    def forward(self, obs, instruction):
        """
        obs: 观测 [B, C, H, W]
        instruction: 语言指令
        """
        # 教师生成动作轨迹
        with torch.no_grad():
            teacher_actions = self.teacher.generate_actions(obs, instruction)
            # teacher_actions: [B, chunk_size, action_dim]
        
        # 学生生成动作轨迹
        student_actions = self.student.generate_actions(obs, instruction)
        
        # 轨迹级别的蒸馏损失
        trajectory_loss = F.mse_loss(student_actions, teacher_actions)
        
        # 可选: 时间加权 (更重视近期动作)
        time_weights = torch.exp(-0.1 * torch.arange(self.chunk_size))
        weighted_loss = (trajectory_loss * time_weights.unsqueeze(-1)).mean()
        
        return weighted_loss
</code></pre>

<h4>3.4 分布蒸馏 (Distribution Distillation)</h4>

<p>对于 Diffusion/Flow Matching 策略，蒸馏动作分布。</p>

<pre><code class="python">class DiffusionDistillation(nn.Module):
    """Diffusion Policy 的蒸馏"""
    def __init__(self, teacher_diffusion, student_diffusion, num_steps=10):
        super().__init__()
        self.teacher = teacher_diffusion
        self.student = student_diffusion
        self.num_steps = num_steps
    
    def forward(self, obs, condition):
        # 教师预测的噪声
        with torch.no_grad():
            x_t = torch.randn(obs.shape[0], self.action_dim)
            for t in reversed(range(self.num_steps)):
                teacher_eps = self.teacher.predict_noise(x_t, t, condition)
                x_t = self.ddim_step(x_t, teacher_eps, t)
            teacher_trajectory = x_t
        
        # 学生预测 (直接预测最终轨迹，一步到位)
        student_trajectory = self.student(obs, condition)
        
        # 蒸馏损失
        loss = F.mse_loss(student_trajectory, teacher_trajectory)
        return loss
</code></pre>

<h3>4. VLA 蒸馏的特殊考虑 (Special Considerations)</h3>

<h4>4.1 视觉编码器蒸馏</h4>

<pre><code class="python">class VisionEncoderDistillation:
    """蒸馏视觉编码器: SigLIP (L) → MobileViT (S)"""
    
    def __init__(self, teacher_vision, student_vision):
        self.teacher = teacher_vision  # SigLIP-L (400M)
        self.student = student_vision  # MobileViT (6M)
        
        # CLS token 对齐
        self.cls_projector = nn.Linear(student.embed_dim, teacher.embed_dim)
        
        # Patch token 对齐
        self.patch_projector = nn.Conv1d(student.embed_dim, teacher.embed_dim, 1)
    
    def distill(self, images):
        with torch.no_grad():
            t_cls, t_patches = self.teacher(images)
        
        s_cls, s_patches = self.student(images)
        
        # CLS 损失
        cls_loss = F.mse_loss(self.cls_projector(s_cls), t_cls)
        
        # Patch 损失 (需要处理分辨率不匹配)
        t_patches_interp = F.interpolate(t_patches, size=s_patches.shape[-1])
        patch_loss = F.mse_loss(self.patch_projector(s_patches), t_patches_interp)
        
        return cls_loss + 0.5 * patch_loss
</code></pre>

<h4>4.2 语言模型蒸馏</h4>

<pre><code class="python">class LLMDistillation:
    """蒸馏语言模型: Llama-7B → TinyLlama-1B"""
    
    def __init__(self, teacher_llm, student_llm):
        self.teacher = teacher_llm
        self.student = student_llm
    
    def distill(self, input_ids, attention_mask):
        # 教师输出
        with torch.no_grad():
            teacher_outputs = self.teacher(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )
        
        # 学生输出
        student_outputs = self.student(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # Logit 蒸馏
        logit_loss = self.soft_cross_entropy(
            student_outputs.logits,
            teacher_outputs.logits,
            temperature=2.0
        )
        
        # 隐藏层蒸馏 (跳层对齐)
        # Teacher 32 层 → Student 12 层, 每隔 ~2.7 层对齐
        layer_mapping = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 31]
        hidden_loss = 0
        for s_idx, t_idx in enumerate(layer_mapping):
            t_hidden = teacher_outputs.hidden_states[t_idx]
            s_hidden = student_outputs.hidden_states[s_idx]
            hidden_loss += F.mse_loss(s_hidden, t_hidden)
        
        return logit_loss + 0.1 * hidden_loss
</code></pre>

<h4>4.3 多阶段蒸馏 (Progressive Distillation)</h4>

<pre><code class="python">class ProgressiveDistillation:
    """渐进式蒸馏: 逐步缩小模型"""
    
    def __init__(self, teacher, intermediate_sizes=[4B, 2B, 1B]):
        self.teacher = teacher
        self.stages = []
        
        prev_model = teacher
        for size in intermediate_sizes:
            student = create_model(size)
            self.stages.append((prev_model, student))
            prev_model = student
    
    def train(self, data):
        for stage_idx, (teacher, student) in enumerate(self.stages):
            print(f"Stage {stage_idx + 1}: {teacher.num_params}B → {student.num_params}B")
            
            for epoch in range(num_epochs):
                for batch in data:
                    loss = self.distill_step(teacher, student, batch)
                    loss.backward()
                    optimizer.step()
            
            # 冻结当前学生，作为下一阶段的教师
            for param in student.parameters():
                param.requires_grad = False
</code></pre>

<h3>5. 蒸馏 + 量化联合优化 (KD + Quantization)</h3>

<h4>5.1 量化感知蒸馏 (Quantization-Aware Distillation)</h4>

<pre><code class="python">class QADistillation:
    """量化感知蒸馏"""
    
    def __init__(self, teacher, student, quant_bits=4):
        self.teacher = teacher
        self.student = student
        self.quant_bits = quant_bits
    
    def forward(self, inputs):
        # 教师 (FP16)
        with torch.no_grad():
            teacher_out = self.teacher(inputs)
        
        # 学生 (模拟量化)
        student_out = self.quantized_forward(self.student, inputs)
        
        # 蒸馏损失
        loss = F.mse_loss(student_out, teacher_out)
        return loss
    
    def quantized_forward(self, model, inputs):
        """模拟 INT4 量化的前向传播"""
        # 伪量化: 模拟量化误差，但保持梯度可传播
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # 量化权重
                w = module.weight
                scale = w.abs().max() / (2 ** (self.quant_bits - 1) - 1)
                w_quant = torch.round(w / scale) * scale
                
                # Straight-Through Estimator
                module.weight.data = w + (w_quant - w).detach()
        
        return model(inputs)
</code></pre>

<h4>5.2 蒸馏后量化 (Post-Distillation Quantization)</h4>

<pre><code class="python">def post_distillation_quantize(distilled_model, calibration_data, bits=4):
    """蒸馏后进行 PTQ"""
    from transformers import BitsAndBytesConfig
    
    # 收集激活统计
    activations = collect_activations(distilled_model, calibration_data)
    
    # 计算量化参数
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",  # NormalFloat4
    )
    
    # 量化模型
    quantized_model = quantize_model(distilled_model, quant_config)
    
    return quantized_model
</code></pre>

<h3>6. 实战：OpenVLA 蒸馏到 1B 模型</h3>

<pre><code class="python">class OpenVLADistillation:
    """将 OpenVLA (7B) 蒸馏到 1B 模型"""
    
    def __init__(self):
        # 教师: OpenVLA 7B
        self.teacher = AutoModelForVision2Seq.from_pretrained("openvla/openvla-7b")
        
        # 学生: TinyLlama 1B + MobileViT
        self.student = TinyVLA(
            vision_encoder="mobilevit_small",
            language_model="TinyLlama/TinyLlama-1.1B",
            action_head_dim=7
        )
        
        # 蒸馏配置
        self.temperature = 4.0
        self.alpha = 0.7  # 软标签权重
    
    def train_step(self, batch):
        images = batch['images']
        instructions = batch['instructions']
        gt_actions = batch['actions']
        
        # 教师预测
        with torch.no_grad():
            teacher_actions, teacher_logits = self.teacher(
                images, instructions, return_logits=True
            )
        
        # 学生预测
        student_actions, student_logits = self.student(
            images, instructions, return_logits=True
        )
        
        # 多任务蒸馏损失
        
        # 1. 动作蒸馏
        action_distill = F.mse_loss(student_actions, teacher_actions)
        
        # 2. Logit 蒸馏 (如果有语言输出)
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(teacher_logits / self.temperature, dim=-1),
            reduction='batchmean'
        ) <em> (self.temperature </em>* 2)
        
        # 3. 硬标签损失 (与真实动作)
        hard_loss = F.mse_loss(student_actions, gt_actions)
        
        # 组合
        total_loss = (
            self.alpha * soft_loss + 
            (1 - self.alpha) * hard_loss +
            0.5 * action_distill
        )
        
        return total_loss</p>

<p>distiller = OpenVLADistillation()
optimizer = torch.optim.AdamW(distiller.student.parameters(), lr=1e-4)</p>

<p>for epoch in range(10):
    for batch in dataloader:
        loss = distiller.train_step(batch)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</p>

<p>distiller.student.save_pretrained("./openvla-1b-distilled")
</code></pre>

<h3>7. 效果对比 (Performance Comparison)</h3>

<table>
<thead><tr>
<th>模型</th>
<th>参数量</th>
<th>延迟 (Jetson)</th>
<th>CALVIN 成功率</th>
<th>显存</th>
</tr></thead>
<tbody>
<tr>
<td><strong>OpenVLA (Teacher)</strong></td>
<td>7B</td>
<td>500ms</td>
<td>78%</td>
<td>16GB</td>
</tr>
<tr>
<td><strong>OpenVLA-1B (Distilled)</strong></td>
<td>1B</td>
<td>80ms</td>
<td>72%</td>
<td>4GB</td>
</tr>
<tr>
<td><strong>OpenVLA-1B + INT4</strong></td>
<td>1B</td>
<td>50ms</td>
<td>70%</td>
<td>2GB</td>
</tr>
<tr>
<td><strong>从头训练 1B</strong></td>
<td>1B</td>
<td>80ms</td>
<td>58%</td>
<td>4GB</td>
</tr>
</tbody></table>

<p><strong>结论</strong>: 蒸馏后的 1B 模型比从头训练提升 14%！</p>

<h3>8. 面试高频问题 (Q&A)</h3>

<p><strong>Q1: 温度参数 T 的作用是什么? 如何选择?</strong></p>

<p>A:
<ul><li><strong>T 的作用</strong>: 软化 softmax 分布，保留更多"暗知识"（如相似类的关系）</li>
<li><strong>T 小 (1-2)</strong>: 分布尖锐，信息主要来自 top-1 预测</li>
<li><strong>T 大 (4-10)</strong>: 分布平滑，保留更多类间关系</li>
<li><strong>经验值</strong>: VLA 中通常 T=4-6</li>
</ul>
<strong>Q2: 为什么蒸馏比从头训练小模型效果好?</strong></p>

<p>A:
<ul><li><strong>软标签信息</strong>: 包含类间关系（如"红色苹果"和"绿色苹果"更相似）</li>
<li><strong>正则化效果</strong>: 软标签比 one-hot 更平滑，防止过拟合</li>
<li><strong>数据效率</strong>: 小模型难以从有限数据学到复杂模式，教师"总结"了知识</li>
</ul>
<strong>Q3: 蒸馏 VLA 时应该蒸馏哪些组件?</strong></p>

<p>A:
<ul><li><strong>必须蒸馏</strong>: Logits (输出分布)、动作轨迹</li>
<li><strong>推荐蒸馏</strong>: 注意力权重、中间特征</li>
<li><strong>可选蒸馏</strong>: 视觉编码器特征（如果学生编码器不同）</li>
</ul>
<strong>Q4: 蒸馏和量化的顺序?</strong></p>

<p>A:
<ul><li><strong>推荐</strong>: 先蒸馏，再量化</li>
<li><strong>原因</strong>: 蒸馏保留知识，量化压缩表示；先量化会丢失太多信息</li>
<li><strong>高级</strong>: 量化感知蒸馏（QAD），在蒸馏时模拟量化误差</li>
</ul>
<strong>Q5: Self-Distillation 是什么?</strong></p>

<p>A:
<ul><li><strong>定义</strong>: 模型自己蒸馏自己（教师和学生是同一模型）</li>
<li><strong>方法</strong>: 用模型的历史 checkpoint 作为教师</li>
<li><strong>效果</strong>: 提升模型的校准性 (Calibration)，常用于大模型训练后期</li>
</ul>
<h3>9. 参考资源 (References)</h3>

<ul><li><strong>Original KD</strong>: <a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a></li>
<li><strong>Feature Distillation</strong>: <a href="https://arxiv.org/abs/1412.6550">FitNets: Hints for Thin Deep Nets</a></li>
<li><strong>Attention Transfer</strong>: <a href="https://arxiv.org/abs/1612.03928">Paying More Attention to Attention</a></li>
<li><strong>Progressive Distillation</strong>: <a href="https://arxiv.org/abs/1910.01348">On the Efficacy of Knowledge Distillation</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第13章 自监督学习</h2>

<blockquote><strong>核心概念</strong>: 自监督学习 (Self-Supervised Learning, SSL) 是一种从无标签数据中学习有意义表示的方法。通过设计 <strong>Pretext Task (代理任务)</strong>，让模型从数据本身的结构中学习。</blockquote>

<h3>1. 为什么 VLA 需要自监督学习? (Why SSL for VLA?)</h3>

<h4>1.1 机器人数据的困境</h4>

<table>
<thead><tr>
<th>数据类型</th>
<th>规模</th>
<th>标注成本</th>
</tr></thead>
<tbody>
<tr>
<td>互联网图文</td>
<td>数十亿</td>
<td>低（网页自动爬取）</td>
</tr>
<tr>
<td>视频数据</td>
<td>数亿小时</td>
<td>极低（无需标注）</td>
</tr>
<tr>
<td><strong>机器人操作数据</strong></td>
<td><strong>数十万</strong></td>
<td><strong>极高（需真机遥操）</strong></td>
</tr>
</tbody></table>

<p><strong>问题</strong>: 有标签的机器人数据稀缺，无法支撑大模型训练。</p>

<h4>1.2 SSL 的价值</h4>

<pre><code class="math">\text{大量无标签数据} \xrightarrow{\text{SSL 预训练}} \text{通用表示} \xrightarrow{\text{少量标签微调}} \text{高性能策略}
</code></pre>

<ul><li><strong>视觉表示</strong>: 从海量图像/视频中学习通用视觉特征</li>
<li><strong>动作表示</strong>: 从人类视频中学习动作先验</li>
<li><strong>世界模型</strong>: 从视频中学习物理规律</li>
</ul>
<h3>2. 自监督学习范式 (SSL Paradigms)</h3>

<h4>2.1 对比学习 (Contrastive Learning)</h4>

<p><strong>核心思想</strong>: 拉近相似样本，推远不同样本。</p>

<h4>2.1.1 InfoNCE 损失</h4>

<pre><code class="math">\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbf{1}_{k \neq i} \exp(\text{sim}(z_i, z_k) / \tau)}
</code></pre>

<p>其中:
<ul><li>$z_i, z_j$: 同一样本的两个增强视图的表示</li>
<li>$\tau$: 温度系数 (通常 0.07)</li>
<li>$\text{sim}(\cdot)$: 余弦相似度</li>
</ul>
<h4>2.1.2 SimCLR 框架</h4>

<pre><code class="">         ┌──────────────────────────────────────┐
         │           原始图像 x                  │
         └───────────────┬──────────────────────┘
                         │
         ┌───────────────┼───────────────┐
         │ 数据增强 T    │               │ 数据增强 T'
         ▼               │               ▼
      ┌─────┐            │            ┌─────┐
      │ x_i │            │            │ x_j │
      └──┬──┘            │            └──┬──┘
         │               │               │
         ▼               │               ▼
      Encoder f          │            Encoder f
         │               │               │
         ▼               │               ▼
      ┌─────┐            │            ┌─────┐
      │ h_i │            │            │ h_j │
      └──┬──┘            │            └──┬──┘
         │               │               │
         ▼               │               ▼
      Projector g        │            Projector g
         │               │               │
         ▼               │               ▼
      ┌─────┐            │            ┌─────┐
      │ z_i │◀───────────┴───────────▶│ z_j │
      └─────┘       对比损失           └─────┘
                   (最大化相似度)
</code></pre>

<p><strong>数据增强策略</strong>:
<ul><li>随机裁剪 + 缩放</li>
<li>颜色抖动 (Color Jittering)</li>
<li>高斯模糊</li>
<li>水平翻转</li>
</ul>
<pre><code class="python">import torch
import torch.nn as nn
import torch.nn.functional as F</p>

<p>class SimCLR(nn.Module):
    def __init__(self, encoder, proj_dim=128, temperature=0.07):
        super().__init__()
        self.encoder = encoder
        self.projector = nn.Sequential(
            nn.Linear(encoder.output_dim, 512),
            nn.ReLU(),
            nn.Linear(512, proj_dim)
        )
        self.temperature = temperature
    
    def forward(self, x_i, x_j):
        # 编码
        h_i = self.encoder(x_i)  # [B, D]
        h_j = self.encoder(x_j)
        
        # 投影
        z_i = F.normalize(self.projector(h_i), dim=1)
        z_j = F.normalize(self.projector(h_j), dim=1)
        
        # 计算相似度矩阵
        batch_size = z_i.shape[0]
        z = torch.cat([z_i, z_j], dim=0)  # [2B, D]
        sim_matrix = torch.mm(z, z.t()) / self.temperature  # [2B, 2B]
        
        # 构建标签: 正样本对在对角线上
        labels = torch.arange(batch_size, device=z.device)
        labels = torch.cat([labels + batch_size, labels])  # [2B]
        
        # 移除自身相似度
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        sim_matrix.masked_fill_(mask, -float('inf'))
        
        # InfoNCE 损失
        loss = F.cross_entropy(sim_matrix, labels)
        return loss
</code></pre>

<h4>2.1.3 CLIP: 视觉-语言对比学习</h4>

<pre><code class="python">def clip_loss(image_features, text_features, temperature=0.07):
    """
    image_features: [B, D] - 图像编码
    text_features: [B, D] - 文本编码
    """
    # 归一化
    image_features = F.normalize(image_features, dim=1)
    text_features = F.normalize(text_features, dim=1)
    
    # 相似度矩阵
    logits = torch.mm(image_features, text_features.t()) / temperature
    
    # 对称损失
    labels = torch.arange(len(logits), device=logits.device)
    loss_i2t = F.cross_entropy(logits, labels)      # 图像→文本
    loss_t2i = F.cross_entropy(logits.t(), labels)  # 文本→图像
    
    return (loss_i2t + loss_t2i) / 2
</code></pre>

<h4>2.2 掩码预测 (Masked Prediction)</h4>

<p><strong>核心思想</strong>: 遮住部分输入，让模型预测被遮住的部分。</p>

<h4>2.2.1 MAE (Masked Autoencoder)</h4>

<pre><code class="">原始图像 (224x224)
    │
    ▼ Patch 分割 (16x16)
196 个 Patches
    │
    ▼ 随机 Mask (75%)
49 个可见 Patches + 147 个 Mask Tokens
    │
    ▼ ViT Encoder (只处理可见 Patches)
    │
    ▼ 添加 Mask Tokens + 位置编码
    │
    ▼ 轻量 Decoder (重建全部 Patches)
    │
    ▼ MSE Loss (只计算被 Mask 的 Patches)
</code></pre>

<p><strong>关键设计</strong>:
<ul><li><strong>高 Mask 比例 (75%)</strong>: 任务足够难，强迫学习语义</li>
<li><strong>非对称架构</strong>: 重编码器 (ViT-L)，轻解码器 (小 Transformer)</li>
<li><strong>只编码可见 Patches</strong>: 大幅减少计算量</li>
</ul>
<pre><code class="python">class MAE(nn.Module):
    def __init__(self, encoder, decoder, mask_ratio=0.75):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.mask_ratio = mask_ratio
        
        # Mask token
        self.mask_token = nn.Parameter(torch.zeros(1, 1, encoder.embed_dim))
    
    def random_masking(self, x, mask_ratio):
        """随机 Mask 策略"""
        B, N, D = x.shape  # [Batch, N_patches, Dim]
        num_keep = int(N * (1 - mask_ratio))
        
        # 随机打乱
        noise = torch.rand(B, N, device=x.device)
        ids_shuffle = torch.argsort(noise, dim=1)
        ids_restore = torch.argsort(ids_shuffle, dim=1)
        
        # 保留前 num_keep 个
        ids_keep = ids_shuffle[:, :num_keep]
        x_masked = torch.gather(x, 1, ids_keep.unsqueeze(-1).expand(-1, -1, D))
        
        # 生成 mask (1 表示被移除)
        mask = torch.ones(B, N, device=x.device)
        mask[:, :num_keep] = 0
        mask = torch.gather(mask, 1, ids_restore)
        
        return x_masked, mask, ids_restore
    
    def forward(self, images):
        # Patch embedding
        patches = self.encoder.patch_embed(images)  # [B, N, D]
        
        # Random masking
        patches_masked, mask, ids_restore = self.random_masking(patches, self.mask_ratio)
        
        # Encode (只处理可见 patches)
        latent = self.encoder.forward_encoder(patches_masked)
        
        # Decode (补充 mask tokens)
        B, N_vis, D = latent.shape
        mask_tokens = self.mask_token.expand(B, ids_restore.shape[1] - N_vis, -1)
        latent_full = torch.cat([latent, mask_tokens], dim=1)
        latent_full = torch.gather(latent_full, 1, ids_restore.unsqueeze(-1).expand(-1, -1, D))
        
        pred = self.decoder(latent_full)  # [B, N, patch_size^2 * 3]
        
        # Loss (只计算 masked patches)
        target = self.patchify(images)
        loss = (pred - target) ** 2
        loss = (loss.mean(dim=-1) * mask).sum() / mask.sum()
        
        return loss
</code></pre>

<h4>2.2.2 VideoMAE: 视频版 MAE</h4>

<p><strong>关键改进</strong>:
<ul><li><strong>时空 Masking</strong>: 对视频的时间和空间维度同时 Mask</li>
<li><strong>Tube Masking</strong>: 在时间上连续 Mask 同一位置（更难）</li>
<li><strong>运动先验</strong>: 学习时序动态</li>
</ul>
<strong>对 VLA 的价值</strong>: 从人类视频中学习动作的时序模式</p>

<h4>2.3 预测学习 (Predictive Learning)</h4>

<h4>2.3.1 时间对比学习</h4>

<pre><code class="python">class TemporalContrastive(nn.Module):
    """学习视频帧的时序关系"""
    def __init__(self, encoder, pred_steps=3):
        super().__init__()
        self.encoder = encoder
        self.predictor = nn.GRU(hidden_size, hidden_size)
        self.pred_steps = pred_steps
    
    def forward(self, video_frames):
        """
        video_frames: [B, T, C, H, W]
        """
        B, T, C, H, W = video_frames.shape
        
        # 编码每帧
        features = []
        for t in range(T):
            feat = self.encoder(video_frames[:, t])
            features.append(feat)
        features = torch.stack(features, dim=1)  # [B, T, D]
        
        # 预测未来帧特征
        loss = 0
        for t in range(T - self.pred_steps):
            context = features[:, :t+1]  # 历史帧
            pred, _ = self.predictor(context)
            pred = pred[:, -1]  # 最后一步的预测
            
            # 对比学习: 正样本是未来帧
            target = features[:, t + self.pred_steps]
            loss += contrastive_loss(pred, target)
        
        return loss / (T - self.pred_steps)
</code></pre>

<h4>2.3.2 R3M: 从人类视频学习机器人表示</h4>

<p><strong>核心思想</strong>: 人类的手部动作与机器人 gripper 类似，可以迁移。</p>

<pre><code class="">人类视频 (Ego4D 数据集)
    │
    ├─▶ 时间对比学习 (Time Contrastive)
    │   - 相邻帧特征应该相似
    │
    ├─▶ 视频-语言对齐 (Video-Language Alignment)
    │   - 视频与文字描述对齐
    │
    └─▶ 语言条件时间对比 (L3)
        - 语言引导的时序预测</p>

<p>    │
    ▼
通用视觉表示 (适用于机器人)
</code></pre>

<h3>3. VLA 中的 SSL 应用 (SSL in VLA)</h3>

<h4>3.1 视觉编码器预训练</h4>

<table>
<thead><tr>
<th>方法</th>
<th>数据</th>
<th>用于 VLA</th>
</tr></thead>
<tbody>
<tr>
<td><strong>ImageNet 监督</strong></td>
<td>1M 图像 + 标签</td>
<td>基础特征</td>
</tr>
<tr>
<td><strong>CLIP</strong></td>
<td>400M 图文对</td>
<td>语义对齐</td>
</tr>
<tr>
<td><strong>DINOv2</strong></td>
<td>142M 无标签图像</td>
<td>空间特征</td>
</tr>
<tr>
<td><strong>R3M</strong></td>
<td>Ego4D 人类视频</td>
<td>操作相关特征</td>
</tr>
</tbody></table>

<h4>3.2 世界模型预训练</h4>

<pre><code class="python">class WorldModelSSL(nn.Module):
    """从视频预测未来帧，学习物理规律"""
    def __init__(self):
        self.encoder = ViTEncoder()
        self.dynamics = TransformerDynamics()
        self.decoder = ViTDecoder()
    
    def forward(self, video, actions=None):
        """
        video: [B, T, C, H, W]
        actions: [B, T-1, A] (可选，动作条件)
        """
        # 编码历史帧
        B, T, C, H, W = video.shape
        latents = []
        for t in range(T):
            z_t = self.encoder(video[:, t])
            latents.append(z_t)
        latents = torch.stack(latents, dim=1)  # [B, T, D]
        
        # 预测下一帧
        pred_latents = self.dynamics(latents[:, :-1], actions)
        
        # 重建
        pred_frames = self.decoder(pred_latents)
        
        # 损失: 预测与真实未来帧的差异
        loss = F.mse_loss(pred_frames, video[:, 1:])
        return loss
</code></pre>

<h4>3.3 动作表示学习</h4>

<pre><code class="python">class ActionSSL(nn.Module):
    """从视频中学习隐式动作表示"""
    def __init__(self):
        self.encoder = VideoEncoder()
        self.action_predictor = MLP()
    
    def forward(self, frame_t, frame_t1):
        """预测两帧之间的隐式动作"""
        z_t = self.encoder(frame_t)
        z_t1 = self.encoder(frame_t1)
        
        # 预测"动作"使得 z_t → z_t1
        pred_action = self.action_predictor(torch.cat([z_t, z_t1], dim=-1))
        
        # 自监督目标: 动作应该能够预测状态变化
        pred_z_t1 = self.dynamics(z_t, pred_action)
        loss = F.mse_loss(pred_z_t1, z_t1.detach())
        
        return loss, pred_action
</code></pre>

<h3>4. 数据增强策略 (Data Augmentation)</h3>

<h4>4.1 图像增强</h4>

<pre><code class="python">import torchvision.transforms as T</p>

<p>train_transform = T.Compose([
    T.RandomResizedCrop(224, scale=(0.8, 1.0)),
    T.RandomHorizontalFlip(p=0.5),  # 注意: 机器人任务可能需要禁用
    T.ColorJitter(0.4, 0.4, 0.4, 0.1),
    T.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
</code></pre>

<h4>4.2 机器人特定增强</h4>

<pre><code class="python">class RobotAugmentation:
    """机器人场景的数据增强"""
    
    @staticmethod
    def camera_shift(image, proprio, max_shift=10):
        """模拟相机位置轻微偏移"""
        shift_x = random.randint(-max_shift, max_shift)
        shift_y = random.randint(-max_shift, max_shift)
        image = T.functional.affine(image, angle=0, translate=(shift_x, shift_y), scale=1, shear=0)
        return image, proprio
    
    @staticmethod
    def action_noise(action, noise_std=0.01):
        """给动作添加噪声（用于 BC）"""
        noise = torch.randn_like(action) * noise_std
        return action + noise
    
    @staticmethod
    def temporal_crop(video, crop_ratio=0.8):
        """时间维度裁剪"""
        T = video.shape[0]
        crop_len = int(T * crop_ratio)
        start = random.randint(0, T - crop_len)
        return video[start:start + crop_len]
</code></pre>

<h3>5. 对比学习 vs 掩码预测 (Comparison)</h3>

<table>
<thead><tr>
<th>特性</th>
<th>对比学习 (Contrastive)</th>
<th>掩码预测 (Masked)</th>
</tr></thead>
<tbody>
<tr>
<td><strong>代表</strong></td>
<td>SimCLR, CLIP, MoCo</td>
<td>MAE, BEiT</td>
</tr>
<tr>
<td><strong>目标</strong></td>
<td>学习不变性表示</td>
<td>学习重建能力</td>
</tr>
<tr>
<td><strong>数据增强</strong></td>
<td>强依赖</td>
<td>不依赖</td>
</tr>
<tr>
<td><strong>负样本</strong></td>
<td>需要 (大 batch)</td>
<td>不需要</td>
</tr>
<tr>
<td><strong>计算效率</strong></td>
<td>低 (大 batch)</td>
<td>高 (只编码 25%)</td>
</tr>
<tr>
<td><strong>下游任务</strong></td>
<td>分类、检索</td>
<td>检测、分割</td>
</tr>
<tr>
<td><strong>VLA 适用</strong></td>
<td>语义理解</td>
<td>空间精细任务</td>
</tr>
</tbody></table>

<h3>6. 面试高频问题 (Q&A)</h3>

<p><strong>Q1: 对比学习中温度系数 τ 的作用是什么?</strong></p>

<p>A:
<ul><li><strong>τ 小</strong>: 分布更 sharp，对负样本区分更严格，但容易过拟合噪声</li>
<li><strong>τ 大</strong>: 分布更 uniform，对比更"软"，但难以学习细粒度区分</li>
<li><strong>经验值</strong>: 0.07 (CLIP), 0.1 (SimCLR)</li>
</ul>
<strong>Q2: MAE 为什么要 Mask 75% 这么高的比例?</strong></p>

<p>A:
<ul><li><strong>任务难度</strong>: 比例低时任务太简单，通过插值就能重建</li>
<li><strong>语义学习</strong>: 高比例强迫模型理解图像的整体语义结构</li>
<li><strong>效率</strong>: 只编码 25% patches，计算量减少 4 倍</li>
<li><strong>对比</strong>: NLP 的 BERT 只 Mask 15%，因为语言冗余更低</li>
</ul>
<strong>Q3: R3M 如何从人类视频迁移到机器人?</strong></p>

<p>A:
<ul><li><strong>假设</strong>: 人类手部操作的视觉特征与机器人 gripper 类似</li>
<li><strong>数据</strong>: Ego4D (第一人称视频) 含大量手-物交互</li>
<li><strong>方法</strong>: 时间对比 + 语言对齐，学习"动作相关"的视觉表示</li>
<li><strong>迁移</strong>: 冻结编码器，只训练策略头</li>
</ul>
<strong>Q4: 自监督学习在 VLA 中最大的价值是什么?</strong></p>

<p>A:
<ul><li><strong>数据效率</strong>: 利用海量无标签数据预训练，减少对稀缺机器人数据的依赖</li>
<li><strong>泛化能力</strong>: 预训练特征具有更好的跨域泛化能力</li>
<li><strong>世界模型</strong>: 从视频学习物理规律，支持模型预测控制</li>
</ul>
<strong>Q5: 如何选择 SSL 方法进行 VLA 预训练?</strong></p>

<p>A:
<ul><li><strong>语义理解任务</strong> (如指令跟随): CLIP / 对比学习</li>
<li><strong>空间精细任务</strong> (如精密装配): MAE / DINOv2</li>
<li><strong>动作预测任务</strong>: R3M / VideoMAE</li>
</ul>
<h3>7. 参考资源 (References)</h3>

<ul><li><strong>SimCLR</strong>: <a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning</a></li>
<li><strong>MAE</strong>: <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a></li>
<li><strong>CLIP</strong>: <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models</a></li>
<li><strong>R3M</strong>: <a href="https://arxiv.org/abs/2203.12601">R3M: A Universal Visual Representation for Robot Manipulation</a></li>
<li><strong>DINOv2</strong>: <a href="https://arxiv.org/abs/2304.07193">DINOv2: Learning Robust Visual Features</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第14章 迁移学习与 Co-training</h2>

<blockquote><strong>核心概念</strong>: 迁移学习 (Transfer Learning) 是将在<strong>源域 (Source Domain)</strong> 学到的知识应用到<strong>目标域 (Target Domain)</strong> 的技术。在 VLA 领域，迁移学习是实现跨机器人、跨场景泛化的关键。</blockquote>

<h3>1. 为什么 VLA 需要迁移学习? (Why Transfer Learning?)</h3>

<h4>1.1 机器人学习的迁移挑战</h4>

<table>
<thead><tr>
<th>迁移类型</th>
<th>源域</th>
<th>目标域</th>
<th>挑战</th>
</tr></thead>
<tbody>
<tr>
<td><strong>跨形态 (Cross-Embodiment)</strong></td>
<td>单臂机器人</td>
<td>双臂机器人</td>
<td>动作空间不同</td>
</tr>
<tr>
<td><strong>跨场景 (Cross-Environment)</strong></td>
<td>实验室</td>
<td>真实家庭</td>
<td>视觉分布偏移</td>
</tr>
<tr>
<td><strong>跨任务 (Cross-Task)</strong></td>
<td>抓取物体</td>
<td>折叠衣物</td>
<td>技能差异大</td>
</tr>
<tr>
<td><strong>仿真到真实 (Sim-to-Real)</strong></td>
<td>仿真环境</td>
<td>真机</td>
<td>物理差异</td>
</tr>
</tbody></table>

<h4>1.2 迁移学习的价值</h4>

<pre><code class="math">\text{数据收集成本} = \frac{\text{所需数据量}}{\text{数据收集效率}} \propto \frac{1}{\text{迁移能力}}
</code></pre>

<ul><li><strong>减少数据需求</strong>: 预训练模型只需少量目标域数据微调</li>
<li><strong>提高泛化能力</strong>: 学习跨域不变的特征表示</li>
<li><strong>加速部署</strong>: 新机器人/场景无需从头训练</li>
</ul>
<h3>2. 迁移学习范式 (Transfer Learning Paradigms)</h3>

<h4>2.1 预训练-微调 (Pre-training + Fine-tuning)</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────┐
│                    Phase 1: Pre-training                    │
│                                                             │
│   大规模数据 (ImageNet/CLIP/OXE)                            │
│              │                                              │
│              ▼                                              │
│        预训练模型 (通用特征)                                  │
└─────────────────────────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────┐
│                    Phase 2: Fine-tuning                     │
│                                                             │
│   目标域少量数据 (50-500 episodes)                           │
│              │                                              │
│              ▼                                              │
│        微调后模型 (目标任务特定)                              │
└─────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2.2 冻结特征提取 (Feature Extraction)</h4>

<p>只训练任务头，冻结预训练的 backbone。</p>

<pre><code class="python">class FeatureExtraction(nn.Module):
    def __init__(self, pretrained_encoder, action_dim):
        super().__init__()
        self.encoder = pretrained_encoder
        
        # 冻结编码器
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        # 只训练策略头
        self.policy_head = nn.Sequential(
            nn.Linear(encoder.output_dim, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, obs):
        with torch.no_grad():
            features = self.encoder(obs)
        action = self.policy_head(features)
        return action
</code></pre>

<p><strong>适用</strong>: 目标域数据极少 (< 50 episodes)</p>

<h4>2.3 全量微调 (Full Fine-tuning)</h4>

<p>解冻所有参数进行训练。</p>

<pre><code class="python">class FullFineTuning(nn.Module):
    def __init__(self, pretrained_model):
        super().__init__()
        self.model = pretrained_model
        
        # 所有参数可训练
        for param in self.model.parameters():
            param.requires_grad = True
    
    def forward(self, obs):
        return self.model(obs)
</code></pre>

<p><strong>适用</strong>: 目标域数据充足，且与源域差异较大</p>

<h4>2.4 参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)</h4>

<p>只训练少量新增参数即可适配新任务，常见工具包括 LoRA、Prompt Tuning、Adapter 等。</p>

<p>LoRA 通过在投影层中串接低秩增量（$W' = W_0 + BA$）来模拟微调效果，大幅减少可训练参数、减轻显存压力，还可以为不同任务保存多个适配器。</p>

<p>详细的 LoRA 数学推导与实践示例请参考 <strong><a href="./peft_lora.md">theory/peft_lora.md</a></strong>（该文档也是 VLA 中 PEFT 的权威参考），这里只保留高层总结和经验值。</p>

<h3>3. 跨形态迁移 (Cross-Embodiment Transfer)</h3>

<h4>3.1 动作空间对齐</h4>

<p>不同机器人的动作空间差异是跨形态迁移的核心挑战。</p>

<pre><code class="python">class ActionSpaceAdapter(nn.Module):
    """将源域动作映射到目标域"""
    def __init__(self, source_action_dim, target_action_dim):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(source_action_dim, 128),
            nn.ReLU(),
            nn.Linear(128, target_action_dim)
        )
    
    def forward(self, source_action):
        return self.adapter(source_action)
</code></pre>

<h4>3.2 统一动作表示</h4>

<p><strong>RDT/OpenVLA 的方案</strong>: 统一填充到最大维度</p>

<pre><code class="python">def unify_action_space(action, embodiment_type, max_dim=32):
    """统一不同机器人的动作空间"""
    action_mapping = {
        "franka_7dof": 7,
        "ur5_6dof": 6,
        "bimanual_14dof": 14,
        "mobile_3dof": 3
    }
    
    original_dim = action_mapping[embodiment_type]
    
    # 填充到统一维度
    padded_action = F.pad(action, (0, max_dim - original_dim))
    
    # 创建有效维度 mask
    mask = torch.zeros(max_dim)
    mask[:original_dim] = 1
    
    return padded_action, mask
</code></pre>

<h4>3.3 形态无关特征学习</h4>

<pre><code class="python">class EmbodimentInvariantEncoder(nn.Module):
    """学习与机器人形态无关的特征"""
    def __init__(self, obs_encoder, action_encoder):
        super().__init__()
        self.obs_encoder = obs_encoder
        self.action_encoder = action_encoder
        
        # 对抗学习: 分类器试图区分形态
        self.embodiment_classifier = nn.Linear(hidden_dim, num_embodiments)
        self.gradient_reversal = GradientReversalLayer()
    
    def forward(self, obs, action, embodiment_label):
        # 编码观测
        obs_feat = self.obs_encoder(obs)
        
        # 对抗训练: 让特征无法区分形态
        reversed_feat = self.gradient_reversal(obs_feat)
        embodiment_pred = self.embodiment_classifier(reversed_feat)
        adversarial_loss = F.cross_entropy(embodiment_pred, embodiment_label)
        
        return obs_feat, adversarial_loss
</code></pre>

<h3>4. 仿真到真实迁移 (Sim-to-Real Transfer)</h3>

<h4>4.1 Domain Randomization (域随机化)</h4>

<p>在仿真中随机化各种参数，让模型对变化鲁棒。</p>

<pre><code class="python">class DomainRandomization:
    """仿真环境的域随机化"""
    
    @staticmethod
    def visual_randomization(image):
        """视觉随机化"""
        transforms = [
            T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
            T.GaussianBlur(kernel_size=5),
            T.RandomAdjustSharpness(sharpness_factor=2),
        ]
        for t in transforms:
            if random.random() &gt; 0.5:
                image = t(image)
        return image
    
    @staticmethod
    def physics_randomization(env):
        """物理参数随机化"""
        # 摩擦系数
        env.set_friction(random.uniform(0.5, 1.5))
        # 物体质量
        env.set_object_mass(random.uniform(0.8, 1.2) * default_mass)
        # 电机延迟
        env.set_actuator_delay(random.uniform(0, 0.05))
        return env
    
    @staticmethod
    def camera_randomization(env):
        """相机参数随机化"""
        # 相机位置扰动
        env.camera_pos += np.random.uniform(-0.02, 0.02, size=3)
        # FOV 变化
        env.camera_fov = random.uniform(55, 65)
        return env
</code></pre>

<h4>4.2 System Identification (系统辨识)</h4>

<pre><code class="python">class SystemIdentification(nn.Module):
    """从真实数据推断仿真参数"""
    def __init__(self):
        self.encoder = nn.LSTM(obs_dim, hidden_dim)
        self.param_predictor = nn.Linear(hidden_dim, physics_param_dim)
    
    def forward(self, trajectory):
        """
        trajectory: [T, obs_dim] - 真实轨迹
        """
        _, (h_n, _) = self.encoder(trajectory)
        predicted_params = self.param_predictor(h_n.squeeze())
        return predicted_params  # e.g., 摩擦系数、质量等
</code></pre>

<h4>4.3 Real-to-Sim-to-Real</h4>

<pre><code class="">真实数据 (少量)
    │
    ▼ System Identification
仿真参数校准
    │
    ▼ 大量仿真数据生成
    │
    ▼ 仿真训练
    │
    ▼ Sim-to-Real Fine-tuning
真机部署
</code></pre>

<h3>5. 域适应 (Domain Adaptation)</h3>

<h4>5.1 对抗域适应 (Adversarial Domain Adaptation)</h4>

<pre><code class="python">class DANN(nn.Module):
    """Domain-Adversarial Neural Network"""
    def __init__(self):
        self.feature_extractor = FeatureExtractor()
        self.task_classifier = TaskClassifier()
        self.domain_classifier = DomainClassifier()
    
    def forward(self, source_data, target_data, alpha=1.0):
        # 源域特征
        source_feat = self.feature_extractor(source_data)
        source_task_pred = self.task_classifier(source_feat)
        
        # 目标域特征
        target_feat = self.feature_extractor(target_data)
        
        # 域分类器 (通过梯度反转对抗训练)
        source_domain = self.domain_classifier(
            GradientReversal.apply(source_feat, alpha)
        )
        target_domain = self.domain_classifier(
            GradientReversal.apply(target_feat, alpha)
        )
        
        # 域分类损失: 让特征无法区分来自哪个域
        domain_loss = F.binary_cross_entropy(
            torch.cat([source_domain, target_domain]),
            torch.cat([torch.zeros_like(source_domain), 
                       torch.ones_like(target_domain)])
        )
        
        return source_task_pred, domain_loss</p>

<p>class GradientReversal(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        # 反转梯度
        return -ctx.alpha * grad_output, None
</code></pre>

<h4>5.2 最大均值差异 (MMD)</h4>

<pre><code class="python">def mmd_loss(source_features, target_features, kernel='rbf'):
    """Maximum Mean Discrepancy 损失"""
    def rbf_kernel(x, y, sigma=1.0):
        dist = torch.cdist(x, y) ** 2
        return torch.exp(-dist / (2 <em> sigma </em>* 2))
    
    K_ss = rbf_kernel(source_features, source_features)
    K_tt = rbf_kernel(target_features, target_features)
    K_st = rbf_kernel(source_features, target_features)
    
    mmd = K_ss.mean() + K_tt.mean() - 2 * K_st.mean()
    return mmd
</code></pre>

<h3>6. 零样本/少样本迁移 (Zero/Few-Shot Transfer)</h3>

<h4>6.1 语言引导的零样本迁移</h4>

<pre><code class="python">class LanguageGuidedTransfer(nn.Module):
    """通过语言描述实现零样本迁移"""
    def __init__(self, vlm_backbone):
        super().__init__()
        self.vlm = vlm_backbone
        
    def forward(self, image, task_description):
        """
        task_description: "pick up the red cup and place it on the tray"
        无需该任务的训练数据，依靠 VLM 的语义理解
        """
        # VLM 直接理解新任务
        action = self.vlm.generate_action(image, task_description)
        return action
</code></pre>

<h4>6.2 少样本学习策略</h4>

<pre><code class="python">class FewShotPolicy(nn.Module):
    """少样本学习策略"""
    def __init__(self, base_policy, adaptation_steps=5):
        super().__init__()
        self.base_policy = base_policy
        self.adaptation_steps = adaptation_steps
    
    def adapt(self, support_set, lr=0.01):
        """在少量支持样本上快速适应"""
        # 复制参数
        adapted_params = {k: v.clone() for k, v in self.base_policy.named_parameters()}
        
        for _ in range(self.adaptation_steps):
            # 计算支持集上的损失
            loss = 0
            for obs, action in support_set:
                pred_action = self.base_policy(obs)
                loss += F.mse_loss(pred_action, action)
            loss /= len(support_set)
            
            # 梯度更新
            grads = torch.autograd.grad(loss, adapted_params.values())
            adapted_params = {
                k: v - lr * g 
                for (k, v), g in zip(adapted_params.items(), grads)
            }
        
        return adapted_params
</code></pre>

<h3>7. VLA 中的迁移学习实践 (Practical Transfer in VLA)</h3>

<h4>7.1 OpenVLA 的迁移流程</h4>

<pre><code class="python">from transformers import AutoModelForVision2Seq
model = AutoModelForVision2Seq.from_pretrained("openvla/openvla-7b")</p>

<p>from peft import LoraConfig, get_peft_model</p>

<p>lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
)
model = get_peft_model(model, lora_config)</p>

<p>for batch in target_dataloader:
    loss = model.compute_loss(**batch)
    loss.backward()
    optimizer.step()</p>

<p>model.save_pretrained("./openvla-adapted-myrobot")
</code></pre>

<h4>7.2 迁移效果对比</h4>

<table>
<thead><tr>
<th>方法</th>
<th>目标域数据量</th>
<th>成功率</th>
<th>训练时间</th>
</tr></thead>
<tbody>
<tr>
<td>从头训练</td>
<td>1000 episodes</td>
<td>65%</td>
<td>10 hours</td>
</tr>
<tr>
<td>全量微调</td>
<td>100 episodes</td>
<td>72%</td>
<td>2 hours</td>
</tr>
<tr>
<td>LoRA 微调</td>
<td>50 episodes</td>
<td><strong>78%</strong></td>
<td><strong>30 min</strong></td>
</tr>
<tr>
<td>冻结+策略头</td>
<td>20 episodes</td>
<td>60%</td>
<td>10 min</td>
</tr>
</tbody></table>

<h3>8. 面试高频问题 (Q&A)</h3>

<p><strong>Q1: 迁移学习和域适应的区别是什么?</strong></p>

<p>A:
<ul><li><strong>迁移学习</strong>: 广义概念，任何利用源域知识帮助目标域学习的方法</li>
<li><strong>域适应</strong>: 迁移学习的子类，专注于解决源域和目标域<strong>分布不同</strong>的问题</li>
<li><strong>关系</strong>: 域适应是实现迁移学习的一种具体技术</li>
</ul>
<strong>Q2: 为什么 LoRA 在 VLA 中效果好?</strong></p>

<p>A:
<ul><li><strong>数据效率</strong>: 机器人数据稀缺，少量参数更不易过拟合</li>
<li><strong>知识保留</strong>: VLM 的预训练知识通过冻结主干被保护</li>
<li><strong>多任务适配</strong>: 可以为不同任务/机器人保存独立的 LoRA 适配器</li>
<li><strong>部署高效</strong>: 推理时可以合并 LoRA 权重，无额外开销</li>
</ul>
<strong>Q3: Sim-to-Real 中 Domain Randomization 的局限性?</strong></p>

<p>A:
<ul><li><strong>参数敏感</strong>: 随机化范围需要精心调整，过大会降低性能</li>
<li><strong>无法覆盖所有差异</strong>: 有些真实世界的复杂性难以在仿真中建模</li>
<li><strong>训练效率</strong>: 极端随机化可能导致学习困难</li>
<li><strong>改进方案</strong>: 结合 System Identification 或 Real-to-Sim</li>
</ul>
<strong>Q4: 跨形态迁移的核心挑战是什么?</strong></p>

<p>A:
<ul><li><strong>动作空间不一致</strong>: 不同机器人 DoF 不同</li>
<li><strong>观测空间差异</strong>: 相机位置、分辨率不同</li>
<li><strong>动力学差异</strong>: 不同机器人的运动特性不同</li>
<li><strong>解决方案</strong>: 统一动作表示 + 形态无关特征学习 + 对抗训练</li>
</ul>
<strong>Q5: 如何判断是否需要全量微调 vs LoRA?</strong></p>

<p>A:
<ul><li><strong>LoRA 适用</strong>: 目标域与源域相似，数据量少 (< 500 episodes)</li>
<li><strong>全量微调适用</strong>: 目标域差异大，有足够数据 (> 1000 episodes)</li>
<li><strong>经验法则</strong>: 先尝试 LoRA，效果不佳再考虑全量微调</li>
</ul>
<h3>9. 参考资源 (References)</h3>

<ul><li><strong>LoRA</strong>: <a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation of Large Language Models</a></li>
<li><strong>Domain Randomization</strong>: <a href="https://arxiv.org/abs/1703.06907">Domain Randomization for Transferring Deep Neural Networks</a></li>
<li><strong>DANN</strong>: <a href="https://arxiv.org/abs/1505.07818">Domain-Adversarial Training of Neural Networks</a></li>
<li><strong>Open X-Embodiment</strong>: <a href="https://arxiv.org/abs/2310.08864">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第14章附 Co-training 详解</h2>

<blockquote><strong>定义</strong>: 在训练 VLA 模型时，同时混合 <strong>机器人动作数据 (Robot Action Data)</strong> 和 <strong>互联网视觉语言数据 (Internet Vision-Language Data)</strong>。</blockquote>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                   Co-training 数据混合策略                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────┐       ┌──────────────────┐               │
│  │   机器人数据      │       │   互联网数据      │               │
│  │   (Robot Data)   │       │   (Web Data)     │               │
│  │                  │       │                  │               │
│  │  📷 + 🎯 + 🦾    │       │  📷 + 📝         │               │
│  │  图像 指令 动作   │       │  图像 文本        │               │
│  └────────┬─────────┘       └────────┬─────────┘               │
│           │                          │                          │
│           │    混合比例 1:1          │                          │
│           └──────────┬───────────────┘                          │
│                      ▼                                          │
│           ┌──────────────────────┐                              │
│           │      VLA 模型        │                              │
│           │   ┌──────┬──────┐   │                              │
│           │   │Action│ Text │   │                              │
│           │   │ Head │ Head │   │                              │
│           │   └──┬───┴──┬───┘   │                              │
│           └──────┼──────┼───────┘                              │
│                  │      │                                       │
│           ┌──────┴──┐ ┌─┴──────┐                               │
│           │ Action  │ │  Text  │                               │
│           │  Loss   │ │  Loss  │                               │
│           │ (机器人)│ │ (互联网)│                               │
│           └─────────┘ └────────┘                               │
│                                                                 │
│   效果: 保持语义能力 ✓  学习动作控制 ✓  防止灾难性遗忘 ✓         │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. 为什么需要 Co-training?</h3>

<h4>1.1. 防止灾难性遗忘 (Catastrophic Forgetting)</h4>
VLA 模型通常基于预训练的 VLM (如 LLaVA, PaLI) 微调。如果只用机器人数据 (通常只有简单的指令如 "Pick apple") 训练，模型会迅速忘记通用的视觉语义知识。
<ul><li><strong>现象</strong>: 模型能抓苹果，但认不出"苹果"是"水果"，或者认不出未见过的物体。</li>
<li><strong>后果</strong>: 丧失了 VLM 最宝贵的通用常识 (Common Sense)。</li>
</ul>
<h4>1.2. 保持通用泛化能力 (Generalization)</h4>
互联网数据包含了丰富的物体、场景和概念，Co-training 能让机器人利用这些知识处理未见过的指令 (Zero-shot)。
<ul><li><strong>举例</strong>: 训练数据里只有 "Pick up the apple"，但用户指令是 "Pick up the red fruit"。如果模型保留了 VLM 的知识，它就能理解 "red fruit" 指的是 apple。</li>
</ul>
<h3>2. 实施策略 (Implementation)</h3>

<h4>2.1. 数据配比 (Mixing Ratio)</h4>
通常采用 <strong>1:1</strong> 或 <strong>1:X</strong> 的比例混合。
<ul><li><strong>RT-2</strong>: 机器人数据 : 互联网数据 = <strong>1 : 1</strong> (Batch 内部混合)。</li>
<li><strong>OpenVLA</strong>: 机器人数据 (Bridge/DROID) : LLaVA Instruct Data = <strong>50% : 50%</strong>。</li>
</ul>
<h4>2.2. Loss 计算 (Loss Masking)</h4>
由于两种数据的标签不同，计算 Loss 时需要进行 Masking：</p>

<table>
<thead><tr>
<th>数据类型</th>
<th>输入 (Input)</th>
<th>输出 (Output)</th>
<th>Loss 计算</th>
</tr></thead>
<tbody>
<tr>
<td><strong>机器人数据</strong></td>
<td>Image + Instruction</td>
<td>Action + (Optional) Text</td>
<td><strong>Action Head Loss</strong> (MSE/CE) + Text Loss</td>
</tr>
<tr>
<td><strong>互联网数据</strong></td>
<td>Image + Text</td>
<td>Text (Caption/VQA)</td>
<td><strong>Text Token Loss</strong> (Next Token Prediction)</td>
</tr>
</tbody></table>

<blockquote><strong>注意</strong>: 对于互联网数据，Action Head 的输出被 Mask 掉，不产生梯度，因为这些数据没有动作标签。</blockquote>

<h4>2.3. 代码逻辑 (Pseudo-code)</h4>

<pre><code class="python">batch_robot = get_robot_batch() # {image, text, action}
batch_web = get_web_batch()     # {image, text, action=None}</p>

<p>out_robot = model(batch_robot.image, batch_robot.text)
loss_action = mse_loss(out_robot.pred_action, batch_robot.gt_action)</p>

<p>out_web = model(batch_web.image, batch_web.text)
loss_text = cross_entropy(out_web.logits, batch_web.gt_text)</p>

<p>total_loss = loss_action + lambda * loss_text</p>

<p>total_loss.backward()
</code></pre>

<h3>3. 案例分析 (Case Studies)</h3>

<h4>3.1. RT-2 (Google DeepMind)</h4>
<ul><li><strong>发现</strong>: Co-training 对于保持模型的逻辑推理能力至关重要。</li>
<li><strong>实验</strong>: 如果不加 Web Data，模型在"将可乐罐放到泰勒斯威夫特照片上"这种需要语义理解的任务上成功率会暴跌。因为模型忘记了"泰勒斯威夫特"是谁。</li>
</ul>
<h4>3.2. OpenVLA (Stanford/Berkeley)</h4>
<ul><li><strong>策略</strong>: 使用 LLaVA 的微调数据 (COCO, GQA, ScienceQA) 进行 Co-training。</li>
<li><strong>效果</strong>: 确保了模型在微调动作控制的同时，依然是一个合格的 VLM (能聊天，能描述图像)。这使得 OpenVLA 既能控制机器人，也能当 VLM 用。</li>
</ul>
<h3>4. 面试高频问题</h3>

<p><strong>Q: 为什么 Co-training 能提高 Zero-shot 能力？</strong>
A: 机器人数据通常是 Narrow Domain 的（特定场景、特定物体）。通过混合 Web Data，模型保持了对 Wide Domain（通用物体、复杂语义）的理解。当遇到未见过的物体（如"恐龙玩具"）时，模型可以利用 VLM 的知识识别它，并结合学到的抓取动作进行操作。</p>

<p><strong>Q: 混合比例对性能有什么影响？</strong>
A: 
<ul><li><strong>Web Data 过少</strong>: 灾难性遗忘，通用能力下降。</li>
<li><strong>Web Data 过多</strong>: 机器人动作学习变慢，因为 Action Loss 的权重被稀释。</li>
<li><strong>经验值</strong>: 1:1 是一个稳健的起点。</li>
</ul>

<hr></p>

<h2>第15章 量化技术</h2>

<p>量化 (Quantization) 是将高精度浮点数 (FP32/FP16) 映射到低精度整数 (INT8/INT4) 的过程。它是 VLA 模型边缘部署的核心技术。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    量化流程 (Quantization Flow)                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   FP32 权重                    INT8/INT4 权重                   │
│   ┌─────────┐                  ┌─────────┐                      │
│   │ 0.0234  │                  │   3     │                      │
│   │ 0.1567  │   ──────────▶    │   20    │                      │
│   │-0.0891  │    量化 (Q)      │  -11    │                      │
│   │ 0.2103  │                  │   27    │                      │
│   └─────────┘                  └─────────┘                      │
│       ▲                             │                           │
│       │                             │                           │
│       └─────────────────────────────┘                           │
│              反量化 (DeQ)                                        │
│                                                                 │
│   存储: 32-bit ────────▶ 4-bit (8x 压缩)                        │
│   精度: 高 ────────────▶ 低 (有损失)                            │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. 基础原理</h3>

<h4>1.1. 映射公式</h4>
将浮点数 $x$ 映射到整数 $q$：</p>

<pre><code class="math">q = \text{round}\left( \frac{x}{S} + Z \right)
</code></pre>

<p>反量化 (Dequantization)：</p>

<pre><code class="math">\hat{x} = S(q - Z)
</code></pre>
其中：
<ul><li>$S$ (Scale): 缩放因子，决定了量化的粒度。</li>
<li>$Z$ (Zero-point): 零点偏移，用于对齐零点。</li>
</ul>
<h4>1.2. 对称 vs 非对称 (Symmetric vs Asymmetric)</h4>

<pre><code class="">对称量化 (Symmetric)              非对称量化 (Asymmetric)
        Z = 0                           Z ≠ 0</p>

<p>    -127 ◀──────▶ +127              0 ◀──────────▶ 255
      │     0     │                 │      Z       │
      ├─────┼─────┤                 ├──────┼───────┤
      │     │     │                 │      │       │
   ───┴─────┴─────┴───           ──┴──────┴───────┴──
     min   0    max               min    0       max
      └─────┬─────┘                └───────┬───────┘
            │                              │
       数据分布对称                    数据分布偏移
     (适合权重 weights)           (适合激活值 activations)
</code></pre>

<h4>对称量化 (Symmetric)</h4>
<ul><li><strong>特点</strong>: $Z=0$。映射范围是对称的 (e.g., INT8: $[-127, 127]$)。</li>
<li><strong>公式</strong>: $q = \text{round}(x / S)$。</li>
<li><strong>优点</strong>: 计算快 (不需要加减 Zero-point)。</li>
<li><strong>缺点</strong>: 如果数据分布严重不对称 (e.g., ReLU 后的激活值全是正的)，会浪费一半的量化范围。</li>
</ul>
<h4>非对称量化 (Asymmetric)</h4>
<ul><li><strong>特点</strong>: $Z \neq 0$。映射范围可以适配数据分布 (e.g., $[min, max]$ 映射到 $[0, 255]$)。</li>
<li><strong>优点</strong>: 精度更高，充分利用 bit 位。</li>
<li><strong>缺点</strong>: 计算稍慢。</li>
</ul>
<hr></p>

<h3>2. 粒度 (Granularity)</h3>

<p>量化参数 $S$ 和 $Z$ 是怎么算的？取决于粒度。</p>

<h4>2.1. Per-Tensor (Layer-wise)</h4>
<ul><li>整个 Tensor 共享一组 $(S, Z)$。</li>
<li><strong>优点</strong>: 硬件实现简单。</li>
<li><strong>缺点</strong>: 精度最差。如果 Tensor 里有一个极大的离群值 (Outlier)，整个 Tensor 的 Scale 都会被拉大，导致小数值全部变成 0。</li>
</ul>
<h4>2.2. Per-Channel (Channel-wise)</h4>
<ul><li>每一行 (或每一列) 有一组 $(S, Z)$。</li>
<li><strong>优点</strong>: 精度显著提升，是 CNN/LLM 权重量化的标准做法。</li>
<li><strong>缺点</strong>: 存储 Scale 需要额外空间 (但在大模型中可忽略)。</li>
</ul>
<h4>2.3. Per-Token / Per-Group</h4>
<ul><li><strong>Per-Token</strong>: 针对激活值 (Activation)，每个 Token 动态计算 Scale。</li>
<li><strong>Per-Group</strong>: 将权重每 128 个数分为一组 (Group)，每组一个 Scale。这是 <strong>4-bit 量化 (如 AWQ, GPTQ)</strong> 的标配，因为 4-bit 精度太低，必须用细粒度 Scale 来补。</li>
</ul>
<hr></p>

<h3>3. 进阶难题：离群值 (Outliers)</h3>

<p>LLM / VLA 模型有一个特性：<strong>激活值中存在极端的离群值</strong> (Outliers)。这些值虽然很少，但对模型性能至关重要。</p>

<h4>3.1. 为什么直接量化会失败？</h4>
如果直接做 INT8 量化，为了包住那个巨大的 Outlier，Scale 会变得很大，导致其他 99.9% 的正常数值都被压缩到了 0，模型直接傻掉。</p>

<h4>3.2. 解决方案：SmoothQuant / AWQ</h4>
<ul><li><strong>SmoothQuant</strong>: 数学等价变换。</li>
</ul>  $$ Y = X W = (X \cdot s^{-1}) \cdot (s \cdot W) $$
  把激活值 $X$ 里的 Outlier "平摊" (Smooth) 到权重 $W$ 上。让 $X$ 变小，让 $W$ 变大。因为权重通常比较均匀，更容易量化。
<ul><li><strong>AWQ (Activation-aware Weight Quantization)</strong>:</li>
</ul>  不量化那些对应重要激活值 (Salient Weights) 的权重，或者保留更高的精度。</p>

<hr></p>

<h3>4. 面试高频考点</h3>

<p><strong>Q: 为什么 4-bit 量化通常比 8-bit 量化更难？</strong>
A: 4-bit 只有 16 个格子。如果 Scale 稍微没选好，误差就会巨大。所以 4-bit 通常需要 <strong>Per-Group</strong> 量化 (Group Size=128) 和更复杂的校准算法 (如 GPTQ)。</p>

<p><strong>Q: Post-Training Quantization (PTQ) vs Quantization-Aware Training (QAT)</strong>
<ul><li><strong>PTQ</strong>: 拿训练好的模型直接量化 (加少量校准数据)。简单，主流。</li>
<li><strong>QAT</strong>: 训练时就模拟量化误差 (Fake Quantization)。精度最高，但训练成本高，VLA 领域较少用。</li>
</ul>
<strong>Q: 权重量化 (Weight-only) vs 激活量化 (Activation Quantization)</strong>
<ul><li><strong>W4A16</strong>: 权重 4-bit，激活 FP16。省显存，推理速度受限于反量化带宽。</li>
<li><strong>W8A8</strong>: 权重 8-bit，激活 8-bit。可以使用 INT8 Tensor Core 加速计算，真正的推理加速。</li>
</ul>

<hr></p>

<h1>第四部分：感知与空间智能</h1>

<hr></p>

<h2>第16章 空间数学基础</h2>

<p>对于 AI 背景的同学来说，机器人学最令人头大的往往不是深度学习模型，而是<strong>坐标变换 (Coordinate Transformations)</strong>。理解空间关系是训练 VLA 模型的基础。</p>

<h3>1. 核心坐标系 (Core Frames)</h3>

<p>在机器人操作中，我们必须时刻清楚数据是在哪个坐标系下定义的。</p>

<h4>1.1. World Frame (世界坐标系) $\{W\}$</h4>
<ul><li><strong>定义</strong>: 全局固定的参考系，通常是机器人底座固定的桌子角，或者房间的某个角落。</li>
<li><strong>作用</strong>: 多机器人协作时的统一基准。</li>
</ul>
<h4>1.2. Base Frame (基座坐标系) $\{B\}$</h4>
<ul><li><strong>定义</strong>: 机器人底座 (Base Link) 的中心。</li>
<li><strong>作用</strong>: <strong>绝大多数单臂机器人的 Action 都是相对于 Base Frame 定义的</strong>。</li>
<li><strong>注意</strong>: 如果机器人是移动的 (Mobile Manipulator)，Base Frame 本身是在 World Frame 中移动的。</li>
</ul>
<h4>1.3. End-effector Frame (末端坐标系 / TCP) $\{E\}$</h4>
<ul><li><strong>定义</strong>: 机械臂末端执行器 (Gripper) 的中心点 (Tool Center Point, TCP)。通常 $Z$ 轴指向夹爪延伸方向，$X$ 轴指向夹爪闭合方向。</li>
<li><strong>作用</strong>: 描述“手”的位置和朝向。</li>
<li><strong>Delta Action</strong>: 很多时候模型预测的是 TCP 相对于<strong>当前时刻 TCP</strong> 的移动量 (即在自己的坐标系下往前走 1cm)。</li>
</ul>
<h4>1.4. Camera Frame (相机坐标系) $\{C\}$</h4>
<ul><li><strong>定义</strong>: 以相机光心为原点，$Z$ 轴通常指向前方，$X$ 轴向右，$Y$ 轴向下 (OpenCV 标准)。</li>
<li><strong>作用</strong>: 视觉输入 (RGB/Depth) 最初都是在 Camera Frame 下的。</li>
</ul>
<hr></p>

<h3>2. 齐次变换矩阵 (Homogeneous Transformation Matrix)</h3>

<p>为了统一描述旋转 (Rotation) 和平移 (Translation)，我们使用 $4 \times 4$ 的齐次变换矩阵 $T$。</p>

<pre><code class="math">T_{A}^{B} = \begin{bmatrix} R_{3\times3} &amp; t_{3\times1} \\ 0_{1\times3} &amp; 1 \end{bmatrix} \in SE(3)
</code></pre>

<ul><li><strong>物理含义</strong>: 描述了坐标系 $\{A\}$ 相对于坐标系 $\{B\}$ 的位姿。</li>
<li><strong>点的变换</strong>: 如果点 $P$ 在 $\{A\}$ 中的坐标是 $P_A = [x, y, z, 1]^T$，那么它在 $\{B\}$ 中的坐标是：</li>
</ul>  $$ P_B = T_{A}^{B} \times P_A $$</p>

<h4>2.1. 逆变换 (Inverse)</h4>
求逆矩阵对应于反向变换 $T_{B}^{A} = (T_{A}^{B})^{-1}$：
<pre><code class="math">(T_{A}^{B})^{-1} = \begin{bmatrix} R^T &amp; -R^T t \\ 0 &amp; 1 \end{bmatrix}
</code></pre>
<blockquote><strong>注意</strong>: 旋转矩阵是正交矩阵，所以 $R^{-1} = R^T$，计算非常快。</blockquote>

<h4>2.2. 链式法则 (Chain Rule)</h4>
这是机器人学中最强大的工具。例如，已知相机相对于世界的外参 $T_{C}^{W}$，以及物体相对于相机的位姿 $T_{O}^{C}$，求物体在世界系下的位姿：
<pre><code class="math">T_{O}^{W} = T_{C}^{W} \times T_{O}^{C}
</code></pre>

<hr></p>

<h3>3. 旋转表示深度解析 (Rotation Representations Deep Dive)</h3>

<p>位置 $t \in \mathbb{R}^3$ 很好表示，但旋转 $R \in SO(3)$ 是非线性的流形结构，神经网络很难直接预测。</p>

<h4>3.1. 欧拉角 (Euler Angles)</h4>
<ul><li><strong>表示</strong>: $(roll, pitch, yaw)$ 或 $(\alpha, \beta, \gamma)$。</li>
<li><strong>致命缺陷</strong>: <strong>万向节死锁 (Gimbal Lock)</strong>。当中间轴旋转 90 度时，第一轴和第三轴重合，丢失一个自由度。</li>
<li><strong>不连续性</strong>: $\pi$ 和 $-\pi$ 是同一个角度，但数值相差巨大。这会导致 Loss 爆炸。</li>
<li><strong>结论</strong>: ❌ <strong>VLA 模型严禁直接预测欧拉角</strong>。</li>
</ul>
<h4>3.2. 四元数 (Quaternion)</h4>
<ul><li><strong>表示</strong>: $q = w + xi + yj + zk$，通常写为向量 $[w, x, y, z]$。</li>
<li><strong>性质</strong>: 必须满足单位模约束 $\|q\|_2 = 1$。</li>
<li><strong>双倍覆盖 (Double Cover)</strong>: $q$ 和 $-q$ 表示完全相同的旋转。</li>
</ul>    - <strong>训练坑点</strong>: 如果真值是 $q$，模型预测的是 $-q$ (实际上是对的)，但 MSE Loss 会很大。
    - <strong>解决方案</strong>: 在计算 Loss 前，如果 $\langle q_{pred}, q_{gt} \rangle < 0$，则将 $q_{gt}$ 翻转为 $-q_{gt}$。
<ul><li><strong>结论</strong>: ✅ <strong>主流选择</strong> (如 RT-1)，但需要处理归一化和双倍覆盖。</li>
</ul>
<h4>3.3. 6D 旋转表示 (6D Rotation Representation) [SOTA]</h4>
<ul><li><strong>来源</strong>: <em>On the Continuity of Rotation Representations in Neural Networks</em> (Zhou et al., CVPR 2019).</li>
<li><strong>核心思想</strong>: 神经网络预测 $3 \times 3$ 旋转矩阵的前两列 $r_1, r_2$ (共 6 个数)。</li>
</ul>    - $r_1$ 是 $X$ 轴方向 (未归一化)。
    - $r_2$ 是 $Y$ 轴方向 (未归一化，且不一定垂直于 $X$)。
<ul><li><strong>还原步骤 (Gram-Schmidt 正交化)</strong>:</li>
</ul>    1. 归一化 $x = \frac{r_1}{\|r_1\|}$
    2. 计算 $z = x \times r_2$ (叉乘得到垂直于 $x, r_2$ 平面的轴)
    3. 归一化 $z = \frac{z}{\|z\|}$
    4. 计算 $y = z \times x$ (得到完美的正交系)
    5. 组装 $R = [x, y, z]$
<ul><li><strong>优点</strong>: <strong>连续性 (Continuity)</strong>。这是唯一一种在欧几里得空间中连续的旋转表示，最适合神经网络回归。</li>
<li><strong>结论</strong>: ✅✅ <strong>Diffusion Policy / Pi0 的首选</strong>。</li>
</ul>
<h4>PyTorch 实现</h4>
<pre><code class="python">import torch
import torch.nn.functional as F</p>

<p>def compute_rotation_matrix_from_ortho6d(ortho6d):
    x_raw = ortho6d[:, 0:3]
    y_raw = ortho6d[:, 3:6]
    
    x = F.normalize(x_raw, dim=-1)
    z = torch.cross(x, y_raw, dim=-1)
    z = F.normalize(z, dim=-1)
    y = torch.cross(z, x, dim=-1)
    
    matrix = torch.stack([x, y, z], dim=-1) # (B, 3, 3)
    return matrix
</code></pre>

<hr></p>

<h3>4. 相机投影几何 (Camera Projection)</h3>

<p>VLA 模型如何理解 3D 世界？通过相机内参。</p>

<h4>4.1. 针孔相机模型 (Pinhole Model)</h4>
将 3D 点 $P_C = [X, Y, Z]^T$ 投影到 2D 像素平面 $p = [u, v]^T$：</p>

<pre><code class="math">Z \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = K \begin{bmatrix} X \\ Y \\ Z \end{bmatrix}
</code></pre>

<p>其中 $K$ 是 <strong>相机内参矩阵 (Intrinsics Matrix)</strong>：
<pre><code class="math">K = \begin{bmatrix} f_x &amp; 0 &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
</code></pre>
<ul><li>$f_x, f_y$: 焦距 (Focal Length)，单位是像素。</li>
<li>$c_x, c_y$: 光心 (Principal Point)，通常是图像中心。</li>
</ul>
<h4>4.2. 深度反投影 (Deprojection)</h4>
如果我们有深度图 $D(u, v) = Z$，可以将像素 $(u, v)$ 还原为 3D 点：
<pre><code class="math">X = \frac{(u - c_x) \cdot Z}{f_x}, \quad Y = \frac{(v - c_y) \cdot Z}{f_y}
</code></pre>
这就是 <strong>PointCloud</strong> 生成的原理。</p>

<hr></p>

<h3>5. 运动学 (Kinematics)</h3>

<h4>5.1. 正运动学 (Forward Kinematics, FK)</h4>
<ul><li><strong>输入</strong>: 关节角度 $\theta = [\theta_1, \dots, \theta_7]$。</li>
<li><strong>输出</strong>: 末端位姿 $T_{E}^{B}$。</li>
<li><strong>计算</strong>: 简单的矩阵连乘。$T_{E}^{B} = T_{1}^{B}(\theta_1) \times T_{2}^{1}(\theta_2) \dots$</li>
</ul>
<h4>5.2. 逆运动学 (Inverse Kinematics, IK)</h4>
<ul><li><strong>输入</strong>: 期望的末端位姿 $T_{target}$。</li>
<li><strong>输出</strong>: 关节角度 $\theta$。</li>
<li><strong>难点</strong>: </li>
</ul>    - <strong>多解</strong>: 同一个位置，机械臂可能有多种姿态到达 (Elbow up / Elbow down)。
    - <strong>无解</strong>: 目标点超出工作空间。
<ul><li><strong>VLA 中的应用</strong>: </li>
</ul>    - 通常 VLA 模型预测 <strong>End-effector Pose</strong> (Task Space)，然后通过 <strong>IK Solver</strong> (如 PyBullet IK 或 Franka IK) 计算出关节角度发送给电机。</p>

<hr></p>

<h3>6. 面试高频考点</h3>

<p><strong>Q: 为什么 VLA 模型通常预测 Delta Pose 而不是 Absolute Pose？</strong>
A: 
<li><strong>泛化性</strong>: Delta Action (如“向前移动 1cm”) 在不同位置、不同机器人尺寸下更通用。</li>
<li><strong>误差解耦</strong>: 绝对坐标强依赖于 Base Frame 的标定。如果相机外参 $T_{C}^{B}$ 有 1cm 的误差，预测绝对坐标就会全程偏差 1cm。而 Delta Action 配合高频闭环 (Closed-loop)，模型可以像人眼一样，“看着手”进行微调，对外参误差不敏感。</li></p>

<p><strong>Q: 为什么旋转矩阵不适合直接作为神经网络输出？</strong>
A: 旋转矩阵必须满足正交约束 $R^T R = I$ 和行列式 $\det(R)=1$。这是一个位于流形 (Manifold) 上的约束。神经网络输出的是无约束的 $\mathbb{R}^9$ 向量，很难保证正交性。强行正交化 (SVD) 会破坏梯度。</p>

<p><strong>Q: 解释一下 6D Rotation 的连续性优势。</strong>
A: 在 3D 旋转空间 $SO(3)$ 中，欧拉角有奇异点，四元数有双倍覆盖 (拓扑上是球面的双倍覆叠)。这导致从神经网络的欧几里得空间 $\mathbb{R}^n$ 到 $SO(3)$ 的映射在某些点是不连续的。6D 表示通过舍弃冗余约束 (正交性由后处理保证)，实现了 $\mathbb{R}^6 \to SO(3)$ 的连续映射，使得训练更稳定。</p>

<hr></p>

<h2>第17章 机器人控制方法</h2>

<blockquote><strong>面试场景</strong>: "介绍一下机械臂的正逆运动学，以及常用的控制方法（PID、MPC、阻抗控制）的区别？"</blockquote>

<p>本文涵盖机械臂的运动学建模、动力学建模，以及主流控制方法的原理与应用。</p>

<hr></p>

<h3>📊 控制方法全景图</h3>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     机械臂控制方法分类                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   控制层级              方法                    特点                         │
│   ─────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   关节空间控制          PID                    简单、快速、无需模型          │
│   (Joint Space)        Computed Torque        基于动力学模型                │
│                                                                             │
│   笛卡尔空间控制        位置控制               精确位置跟踪                  │
│   (Cartesian Space)    速度控制               轨迹跟踪                      │
│                                                                             │
│   力/柔顺控制           阻抗控制               弹簧-阻尼行为                 │
│   (Force/Compliance)   导纳控制               力输入→位置输出               │
│                        混合力位控制           不同方向不同控制              │
│                                                                             │
│   最优控制              MPC                    预测+优化，处理约束           │
│   (Optimal Control)    LQR                    线性系统最优                  │
│                                                                             │
│   学习型控制            RL (PPO/SAC)           端到端策略                    │
│   (Learning-based)     IL (BC)                模仿学习                      │
│                                                                             │
│   💡 VLA 趋势: 高层用 VLA 规划动作 → 底层用传统控制执行                      │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>1. 运动学 (Kinematics)</h3>

<h4>1.1 正运动学 (Forward Kinematics, FK)</h4>

<p><strong>定义</strong>: 给定关节角度 $\mathbf{q}$，求末端执行器位姿 $\mathbf{T}_{ee}$。</p>

<pre><code class="math">\mathbf{T}_{ee} = FK(\mathbf{q}) = \mathbf{T}_1(\theta_1) \cdot \mathbf{T}_2(\theta_2) \cdots \mathbf{T}_n(\theta_n)
</code></pre>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     正运动学示意图                                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   关节角度                                         末端位姿                  │
│   ┌─────────┐                                    ┌─────────┐               │
│   │ θ₁      │                                    │ x, y, z │               │
│   │ θ₂      │  ────► FK (DH 参数) ────►          │ R (旋转)│               │
│   │ ...     │                                    │         │               │
│   │ θₙ      │                                    └─────────┘               │
│   └─────────┘                                                              │
│                                                                             │
│   DH 参数 (Denavit-Hartenberg):                                            │
│   • a_i: 连杆长度 (沿 x 轴)                                                 │
│   • α_i: 连杆扭转 (绕 x 轴)                                                 │
│   • d_i: 连杆偏距 (沿 z 轴)                                                 │
│   • θ_i: 关节角度 (绕 z 轴)                                                 │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>代码实现</strong>:</p>

<pre><code class="python">import numpy as np
from scipy.spatial.transform import Rotation as R</p>

<p>def dh_transform(theta, d, a, alpha):
    """单个关节的 DH 变换矩阵"""
    ct, st = np.cos(theta), np.sin(theta)
    ca, sa = np.cos(alpha), np.sin(alpha)
    return np.array([
        [ct, -st<em>ca,  st</em>sa, a*ct],
        [st,  ct<em>ca, -ct</em>sa, a*st],
        [0,   sa,     ca,    d   ],
        [0,   0,      0,     1   ]
    ])</p>

<p>def forward_kinematics(joint_angles, dh_params):
    """
    正运动学计算
    
    Args:
        joint_angles: 关节角度 [θ₁, θ₂, ..., θₙ]
        dh_params: DH 参数表 [(d, a, α), ...]
    
    Returns:
        T_ee: 末端执行器变换矩阵 (4x4)
    """
    T = np.eye(4)
    for i, (theta, (d, a, alpha)) in enumerate(zip(joint_angles, dh_params)):
        T = T @ dh_transform(theta, d, a, alpha)
    return T</p>

<p>franka_dh = [
    (0.333, 0,      0),       # Joint 1
    (0,     0,     -np.pi/2), # Joint 2
    (0.316, 0,      np.pi/2), # Joint 3
    (0,     0.0825, np.pi/2), # Joint 4
    (0.384, -0.0825,-np.pi/2),# Joint 5
    (0,     0,      np.pi/2), # Joint 6
    (0.107, 0.088,  np.pi/2), # Joint 7 (flange)
]</p>

<p>q = [0, -np.pi/4, 0, -3*np.pi/4, 0, np.pi/2, np.pi/4]
T_ee = forward_kinematics(q, franka_dh)
position = T_ee[:3, 3]
rotation = R.from_matrix(T_ee[:3, :3])
</code></pre>

<h4>1.2 逆运动学 (Inverse Kinematics, IK)</h4>

<p><strong>定义</strong>: 给定末端执行器目标位姿 $\mathbf{T}_{target}$，求关节角度 $\mathbf{q}$。</p>

<pre><code class="math">\mathbf{q} = IK(\mathbf{T}_{target})
</code></pre>

<p><strong>方法对比</strong>:</p>

<table>
<thead><tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>解析解</strong></td>
<td>精确、快速</td>
<td>只适用于特定构型</td>
<td>6 轴工业臂</td>
</tr>
<tr>
<td><strong>数值迭代 (Jacobian)</strong></td>
<td>通用</td>
<td>可能不收敛</td>
<td>通用机械臂</td>
</tr>
<tr>
<td><strong>优化方法</strong></td>
<td>可加约束</td>
<td>较慢</td>
<td>冗余臂、避障</td>
</tr>
<tr>
<td><strong>学习方法 (NN)</strong></td>
<td>快速</td>
<td>需要大量数据</td>
<td>实时应用</td>
</tr>
</tbody></table>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     逆运动学方法对比                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   解析解 (Analytic):                                                        │
│   • 几何方法：利用臂的几何关系直接求解                                       │
│   • 适用于 6 自由度标准构型 (如 UR, Panda)                                  │
│   • 通常有多解 (最多 8 组)，需选择最优解                                    │
│                                                                             │
│   数值迭代 (Jacobian Pseudoinverse):                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   Δx = J(q) · Δq                                                   │   │
│   │   Δq = J⁺(q) · Δx   (J⁺ 为伪逆)                                    │   │
│   │                                                                     │   │
│   │   迭代: q_{k+1} = q_k + α · J⁺(q_k) · (x_target - FK(q_k))         │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   优化方法 (可加约束):                                                      │
│   min_q  ||FK(q) - T_target||² + λ||q - q_ref||²                          │
│   s.t.   q_min ≤ q ≤ q_max        (关节限位)                               │
│          ||q̇|| ≤ v_max            (速度限制)                               │
│          避障约束                                                           │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>数值 IK 实现</strong>:</p>

<pre><code class="python">def jacobian_ik(target_pose, q_init, dh_params, max_iter=100, tol=1e-6):
    """
    基于 Jacobian 伪逆的数值 IK
    
    Args:
        target_pose: 目标位姿 (4x4 变换矩阵)
        q_init: 初始关节角度
        dh_params: DH 参数
        
    Returns:
        q: 解出的关节角度
    """
    q = q_init.copy()
    
    for _ in range(max_iter):
        # 当前位姿
        T_current = forward_kinematics(q, dh_params)
        
        # 位姿误差
        pos_error = target_pose[:3, 3] - T_current[:3, 3]
        rot_error = rotation_error(T_current[:3, :3], target_pose[:3, :3])
        error = np.concatenate([pos_error, rot_error])
        
        if np.linalg.norm(error) &lt; tol:
            return q
        
        # 计算 Jacobian
        J = compute_jacobian(q, dh_params)
        
        # 伪逆求解
        J_pinv = np.linalg.pinv(J)
        dq = J_pinv @ error
        
        # 更新
        q = q + 0.5 * dq  # 步长 0.5
        
        # 关节限位
        q = np.clip(q, q_min, q_max)
    
    return q</p>

<p>def rotation_error(R_current, R_target):
    """计算旋转误差 (轴角形式)"""
    R_err = R_target @ R_current.T
    axis_angle = R.from_matrix(R_err).as_rotvec()
    return axis_angle
</code></pre>

<h4>1.3 雅可比矩阵 (Jacobian)</h4>

<p><strong>定义</strong>: 关节速度到末端速度的映射。</p>

<pre><code class="math">\dot{\mathbf{x}} = \mathbf{J}(\mathbf{q}) \cdot \dot{\mathbf{q}}
</code></pre>

<p>其中 $\dot{\mathbf{x}} = [v_x, v_y, v_z, \omega_x, \omega_y, \omega_z]^T$</p>

<p><strong>应用</strong>:
<ul><li><strong>速度控制</strong>: $\dot{\mathbf{q}} = \mathbf{J}^{-1} \dot{\mathbf{x}}$</li>
<li><strong>奇异性分析</strong>: $\det(\mathbf{J}) = 0$ 时奇异</li>
<li><strong>力传递</strong>: $\boldsymbol{\tau} = \mathbf{J}^T \mathbf{F}$ (末端力到关节力矩)</li>
</ul>
<hr></p>

<h3>2. 动力学 (Dynamics)</h3>

<h4>2.1 动力学方程</h4>

<p><strong>拉格朗日方程</strong>:</p>

<pre><code class="math">\mathbf{M}(\mathbf{q})\ddot{\mathbf{q}} + \mathbf{C}(\mathbf{q}, \dot{\mathbf{q}})\dot{\mathbf{q}} + \mathbf{g}(\mathbf{q}) = \boldsymbol{\tau}
</code></pre>

<ul><li>$\mathbf{M}(\mathbf{q})$: 惯性矩阵 (Mass Matrix)</li>
<li>$\mathbf{C}(\mathbf{q}, \dot{\mathbf{q}})$: 科氏力/离心力矩阵</li>
<li>$\mathbf{g}(\mathbf{q})$: 重力向量</li>
<li>$\boldsymbol{\tau}$: 关节力矩</li>
</ul>
<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     动力学建模方法                                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   解析建模 (基于模型):                                                      │
│   • 牛顿-欧拉法 (Newton-Euler): 递归计算，高效                              │
│   • 拉格朗日法 (Lagrangian): 能量方法，直观                                 │
│   • 需要精确的惯性参数 (质量、惯量、质心位置)                                │
│                                                                             │
│   数据驱动 (学习型):                                                        │
│   • 神经网络拟合: 输入 (q, q̇, q̈) → 输出 τ                                   │
│   • Gaussian Process: 不确定性估计                                         │
│   • 适合难以精确建模的场景 (柔性、摩擦)                                     │
│                                                                             │
│   常用库:                                                                   │
│   • Pinocchio: C++/Python, 高效动力学计算                                  │
│   • PyBullet: 仿真内置                                                     │
│   • Drake: 多体动力学                                                       │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2.2 动力学库使用 (Pinocchio)</h4>

<pre><code class="python">import pinocchio as pin
import numpy as np</p>

<p>model = pin.buildModelFromUrdf("panda.urdf")
data = model.createData()</p>

<p>q = np.array([0, -np.pi/4, 0, -3*np.pi/4, 0, np.pi/2, np.pi/4])
dq = np.zeros(7)
ddq = np.zeros(7)</p>

<p>pin.forwardKinematics(model, data, q)
ee_pose = data.oMi[-1]  # 末端位姿</p>

<p>M = pin.crba(model, data, q)</p>

<p>C_dq = pin.nle(model, data, q, dq) - pin.computeGeneralizedGravity(model, data, q)</p>

<p>g = pin.computeGeneralizedGravity(model, data, q)</p>

<p>tau = pin.rnea(model, data, q, dq, ddq)</p>

<p>ddq = pin.aba(model, data, q, dq, tau)</p>

<p>J = pin.computeFrameJacobian(model, data, q, frame_id, pin.ReferenceFrame.LOCAL_WORLD_ALIGNED)
</code></pre>

<hr></p>

<h3>3. 控制方法 (Control Methods)</h3>

<h4>3.1 PID 控制</h4>

<p><strong>最基础的关节空间控制</strong>:</p>

<pre><code class="math">\tau = K_p (q_d - q) + K_d (\dot{q}_d - \dot{q}) + K_i \int (q_d - q) dt
</code></pre>

<pre><code class="python">class PIDController:
    """单关节 PID 控制器"""
    def __init__(self, kp, ki, kd, dt=0.001):
        self.kp = kp
        self.ki = ki
        self.kd = kd
        self.dt = dt
        self.integral = 0
        self.prev_error = 0
    
    def compute(self, q_desired, q_current, dq_current=None):
        """
        计算控制力矩
        
        Args:
            q_desired: 目标位置
            q_current: 当前位置
            dq_current: 当前速度 (可选)
        """
        error = q_desired - q_current
        
        # P 项
        p_term = self.kp * error
        
        # I 项
        self.integral += error * self.dt
        self.integral = np.clip(self.integral, -1, 1)  # 积分限幅
        i_term = self.ki * self.integral
        
        # D 项
        if dq_current is not None:
            d_term = -self.kd * dq_current  # 速度反馈
        else:
            d_term = self.kd * (error - self.prev_error) / self.dt
        
        self.prev_error = error
        
        return p_term + i_term + d_term</p>

<p>class JointPIDController:
    def __init__(self, n_joints, kp, ki, kd, dt=0.001):
        self.controllers = [
            PIDController(kp[i], ki[i], kd[i], dt) 
            for i in range(n_joints)
        ]
    
    def compute(self, q_desired, q_current, dq_current):
        tau = np.zeros(len(self.controllers))
        for i, ctrl in enumerate(self.controllers):
            tau[i] = ctrl.compute(q_desired[i], q_current[i], dq_current[i])
        return tau
</code></pre>

<p><strong>优缺点</strong>:</p>

<table>
<thead><tr>
<th>优点</th>
<th>缺点</th>
</tr></thead>
<tbody>
<tr>
<td>简单、无需模型</td>
<td>无法处理非线性</td>
</tr>
<tr>
<td>调参直观</td>
<td>耦合系统性能差</td>
</tr>
<tr>
<td>计算快</td>
<td>无法处理约束</td>
</tr>
</tbody></table>

<h4>3.2 阻抗控制 (Impedance Control)</h4>

<p><strong>核心思想</strong>: 让机器人表现得像弹簧-阻尼系统。</p>

<pre><code class="math">\mathbf{F} = \mathbf{M}_d (\ddot{\mathbf{x}}_d - \ddot{\mathbf{x}}) + \mathbf{B}_d (\dot{\mathbf{x}}_d - \dot{\mathbf{x}}) + \mathbf{K}_d (\mathbf{x}_d - \mathbf{x})
</code></pre>

<ul><li>$\mathbf{M}_d$: 期望惯性</li>
<li>$\mathbf{B}_d$: 期望阻尼</li>
<li>$\mathbf{K}_d$: 期望刚度</li>
</ul>
<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     阻抗控制 vs 导纳控制                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   阻抗控制 (Impedance Control):                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   输入: 位置误差 Δx                                                 │   │
│   │   输出: 力 F                                                        │   │
│   │                                                                     │   │
│   │   F = K <em> Δx + B </em> Δẋ                                               │   │
│   │                                                                     │   │
│   │   适用: 位置控制型机器人 + 力传感器                                 │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   导纳控制 (Admittance Control):                                            │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   输入: 外力 F (力传感器测量)                                       │   │
│   │   输出: 位置修正 Δx                                                 │   │
│   │                                                                     │   │
│   │   Δx = (1/K) <em> F + (1/B) </em> ∫F dt                                    │   │
│   │                                                                     │   │
│   │   适用: 纯位置控制机器人 (无力矩接口)                               │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   💡 选择原则:                                                              │
│   • 能直接控制力/力矩 → 阻抗控制                                           │
│   • 只能控制位置 → 导纳控制                                                 │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>阻抗控制器实现</strong>:</p>

<pre><code class="python">class ImpedanceController:
    """笛卡尔空间阻抗控制"""
    def __init__(self, K, B, M=None):
        """
        Args:
            K: 刚度矩阵 (6x6)
            B: 阻尼矩阵 (6x6)
            M: 惯性矩阵 (6x6), 可选
        """
        self.K = np.diag(K) if len(K.shape) == 1 else K
        self.B = np.diag(B) if len(B.shape) == 1 else B
        self.M = np.diag(M) if M is not None and len(M.shape) == 1 else M
        
    def compute(self, x_current, x_desired, dx_current, dx_desired, ddx_desired=None):
        """
        计算期望的末端力
        
        Args:
            x_current: 当前末端位姿 (6,) [pos, ori]
            x_desired: 目标末端位姿 (6,)
            dx_current: 当前末端速度 (6,)
            dx_desired: 目标末端速度 (6,)
            
        Returns:
            F: 期望末端力 (6,)
        """
        # 位置误差
        pos_error = x_desired[:3] - x_current[:3]
        vel_error = dx_desired[:3] - dx_current[:3]
        
        # 姿态误差 (需要特殊处理)
        ori_error = orientation_error(x_current[3:], x_desired[3:])
        omega_error = dx_desired[3:] - dx_current[3:]
        
        error = np.concatenate([pos_error, ori_error])
        derror = np.concatenate([vel_error, omega_error])
        
        # 阻抗方程
        F = self.K @ error + self.B @ derror
        
        if self.M is not None and ddx_desired is not None:
            F += self.M @ ddx_desired
            
        return F
    
    def to_joint_torque(self, F, J):
        """转换为关节力矩"""
        return J.T @ F</p>

<p>impedance = ImpedanceController(
    K=[1000, 1000, 1000, 50, 50, 50],  # 平移刚度高，旋转刚度低
    B=[100, 100, 100, 10, 10, 10],
)</p>

<p>F_desired = impedance.compute(x_current, x_desired, dx_current, dx_desired)
tau = impedance.to_joint_torque(F_desired, J)
</code></pre>

<p><strong>VLA 中的应用</strong>:
<ul><li><strong>接触任务</strong>: 抓取时切换到低刚度，确保柔顺</li>
<li><strong>人机协作</strong>: 碰到人时表现柔软</li>
<li><strong>装配任务</strong>: 插入时需要柔顺</li>
</ul>
<h4>3.3 MPC (Model Predictive Control)</h4>

<p><strong>核心思想</strong>: 在线滚动优化，预测未来轨迹并优化。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     MPC 工作原理                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   时间轴:                                                                   │
│   ──────────────────────────────────────────────────────────────────►       │
│   t    t+1   t+2   ...   t+N (预测horizon)                                  │
│   │     │     │           │                                                 │
│   ▼     ▼     ▼           ▼                                                 │
│   x₀    x₁    x₂    ...   xₙ   (状态预测)                                   │
│         u₀    u₁    ...   uₙ₋₁ (控制输入)                                   │
│                                                                             │
│   优化问题:                                                                  │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   min  Σ ||x_k - x_ref||²_Q + ||u_k||²_R                            │   │
│   │                                                                     │   │
│   │   s.t. x_{k+1} = f(x_k, u_k)    (动力学约束)                        │   │
│   │        x_min ≤ x_k ≤ x_max      (状态约束)                          │   │
│   │        u_min ≤ u_k ≤ u_max      (输入约束)                          │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   滚动执行:                                                                  │
│   1. 在 t 时刻求解优化问题，得到 u₀, u₁, ..., uₙ₋₁                         │
│   2. 只执行 u₀                                                              │
│   3. 在 t+1 时刻重新求解                                                    │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>MPC 实现 (使用 CVXPY)</strong>:</p>

<pre><code class="python">import cvxpy as cp
import numpy as np</p>

<p>class LinearMPC:
    """线性 MPC 控制器"""
    def __init__(self, A, B, Q, R, N, x_min, x_max, u_min, u_max):
        """
        Args:
            A, B: 线性系统矩阵 x_{k+1} = A @ x_k + B @ u_k
            Q, R: 代价权重
            N: 预测 horizon
            x_min, x_max: 状态约束
            u_min, u_max: 输入约束
        """
        self.A, self.B = A, B
        self.Q, self.R = Q, R
        self.N = N
        self.n_x, self.n_u = A.shape[0], B.shape[1]
        self.x_min, self.x_max = x_min, x_max
        self.u_min, self.u_max = u_min, u_max
        
    def solve(self, x0, x_ref):
        """
        求解 MPC 优化问题
        
        Args:
            x0: 当前状态
            x_ref: 参考轨迹 (N+1, n_x)
            
        Returns:
            u_opt: 最优控制输入 (第一个)
        """
        # 变量
        x = cp.Variable((self.N + 1, self.n_x))
        u = cp.Variable((self.N, self.n_u))
        
        # 代价函数
        cost = 0
        constraints = [x[0] == x0]  # 初始状态约束
        
        for k in range(self.N):
            # 状态代价
            cost += cp.quad_form(x[k] - x_ref[k], self.Q)
            # 输入代价
            cost += cp.quad_form(u[k], self.R)
            # 动力学约束
            constraints += [x[k+1] == self.A @ x[k] + self.B @ u[k]]
            # 状态约束
            constraints += [self.x_min &lt;= x[k], x[k] &lt;= self.x_max]
            # 输入约束
            constraints += [self.u_min &lt;= u[k], u[k] &lt;= self.u_max]
        
        # 终端代价
        cost += cp.quad_form(x[self.N] - x_ref[self.N], self.Q * 10)
        
        # 求解
        prob = cp.Problem(cp.Minimize(cost), constraints)
        prob.solve(solver=cp.OSQP, warm_start=True)
        
        return u[0].value</p>

<p>dt = 0.01
A = np.array([[1, dt], [0, 1]])  # 双积分系统
B = np.array([[0.5<em>dt</em>*2], [dt]])
Q = np.diag([10, 1])
R = np.array([[0.1]])</p>

<p>mpc = LinearMPC(A, B, Q, R, N=20, 
                x_min=np.array([-10, -5]), x_max=np.array([10, 5]),
                u_min=np.array([-10]), u_max=np.array([10]))</p>

<p>x_current = np.array([0, 0])
x_ref = np.tile([1, 0], (21, 1))  # 目标位置 1
u_opt = mpc.solve(x_current, x_ref)
</code></pre>

<p><strong>MPC vs PID</strong>:</p>

<table>
<thead><tr>
<th>维度</th>
<th>MPC</th>
<th>PID</th>
</tr></thead>
<tbody>
<tr>
<td><strong>约束处理</strong></td>
<td>显式处理</td>
<td>无法处理</td>
</tr>
<tr>
<td><strong>预测能力</strong></td>
<td>有</td>
<td>无</td>
</tr>
<tr>
<td><strong>计算量</strong></td>
<td>高 (需在线优化)</td>
<td>低</td>
</tr>
<tr>
<td><strong>调参</strong></td>
<td>Q, R 矩阵</td>
<td>Kp, Ki, Kd</td>
</tr>
<tr>
<td><strong>适用</strong></td>
<td>复杂约束、最优控制</td>
<td>简单跟踪</td>
</tr>
</tbody></table>

<h4>3.4 控制方法对比总结</h4>

<table>
<thead><tr>
<th>方法</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
<th>VLA 应用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>PID</strong></td>
<td>误差反馈</td>
<td>简单、快速</td>
<td>无法处理耦合/约束</td>
<td>底层关节控制</td>
</tr>
<tr>
<td><strong>阻抗控制</strong></td>
<td>弹簧-阻尼行为</td>
<td>柔顺、安全</td>
<td>需调刚度参数</td>
<td>接触任务、人机协作</td>
</tr>
<tr>
<td><strong>导纳控制</strong></td>
<td>力→位置</td>
<td>适用位置控制臂</td>
<td>需要力传感器</td>
<td>拖动示教</td>
</tr>
<tr>
<td><strong>MPC</strong></td>
<td>滚动优化</td>
<td>可加约束、最优</td>
<td>计算量大</td>
<td>轨迹优化、避障</td>
</tr>
<tr>
<td><strong>Computed Torque</strong></td>
<td>动力学补偿</td>
<td>解耦控制</td>
<td>需精确模型</td>
<td>高精度任务</td>
</tr>
</tbody></table>

<hr></p>

<h3>4. VLA 中的控制流程</h3>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     VLA 控制架构                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   高层: VLA 模型 (OpenVLA, π0 等)                                           │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   输入: 图像 + 语言指令                                              │   │
│   │   输出: 末端增量 Δpose (或绝对 pose)                                 │   │
│   │   频率: 10-30 Hz                                                    │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                          │                                                  │
│                          ▼                                                  │
│   中层: 运动规划 / IK                                                       │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   Δpose → 轨迹插值 → IK 求解 → 关节目标 q_target                     │   │
│   │   可选: MPC 优化轨迹、避障                                           │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                          │                                                  │
│                          ▼                                                  │
│   底层: 关节控制 (驱动器内置)                                               │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   q_target → PID / 阻抗控制 → τ → 电机                              │   │
│   │   频率: 1000 Hz (硬实时)                                             │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   💡 VLA 主要负责高层决策，底层控制由传统方法保证安全和精度                  │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>5. 面试 Q&A</h3>

<h4>Q1: 正运动学和逆运动学的区别？</h4>

<table>
<thead><tr>
<th>维度</th>
<th>正运动学 (FK)</th>
<th>逆运动学 (IK)</th>
</tr></thead>
<tbody>
<tr>
<td><strong>输入</strong></td>
<td>关节角度 $\mathbf{q}$</td>
<td>末端位姿 $\mathbf{T}$</td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td>末端位姿 $\mathbf{T}$</td>
<td>关节角度 $\mathbf{q}$</td>
</tr>
<tr>
<td><strong>解的数量</strong></td>
<td>唯一解</td>
<td>可能多解或无解</td>
</tr>
<tr>
<td><strong>计算复杂度</strong></td>
<td>低 (矩阵乘法)</td>
<td>高 (优化/迭代)</td>
</tr>
<tr>
<td><strong>难度</strong></td>
<td>简单</td>
<td>复杂</td>
</tr>
</tbody></table>

<h4>Q2: PID 和 MPC 的适用场景？</h4>

<ul><li><strong>PID</strong>: 简单关节伺服、低延迟要求、单输入单输出</li>
<li><strong>MPC</strong>: 需要处理约束 (关节限位、速度限制)、需要预测 (避障)、多目标优化</li>
</ul>
<h4>Q3: 阻抗控制的刚度参数如何选择？</h4>

<ul><li><strong>高刚度 (K 大)</strong>: 精确位置跟踪，适合自由空间运动</li>
<li><strong>低刚度 (K 小)</strong>: 柔顺行为，适合接触任务、装配</li>
<li><strong>动态切换</strong>: VLA 抓取流程中，接近阶段高刚度，接触后低刚度</li>
</ul>
<h4>Q4: 为什么 VLA 底层不用 RL 控制？</h4>

<p><li><strong>安全性</strong>: 传统控制稳定性有保证，RL 可能出现意外动作</li>
<li><strong>频率</strong>: 底层控制需要 1kHz，RL 推理太慢</li>
<li><strong>分工</strong>: VLA 负责高层决策 (什么动作)，底层控制负责精确执行 (怎么执行)</li></p>

<h4>Q5: 动力学模型在 VLA 中的作用？</h4>

<ul><li><strong>仿真训练</strong>: Sim-to-Real 需要准确的动力学仿真</li>
<li><strong>重力补偿</strong>: 计算 $\mathbf{g}(\mathbf{q})$ 补偿重力</li>
<li><strong>力估计</strong>: 无力传感器时，通过动力学估计外力</li>
<li><strong>Computed Torque</strong>: 高精度轨迹跟踪</li>
</ul>
<hr></p>

<h3>📚 推荐资源</h3>

<h4>书籍</h4>
<ul><li><strong><a href="http://hades.mech.northwestern.edu/index.php/Modern_Robotics">Modern Robotics</a></strong>: 运动学/动力学经典教材</li>
<li><strong><a href="https://link.springer.com/book/10.1007/978-1-84628-642-1">Robotics: Modelling, Planning and Control</a></strong>: Siciliano 的控制圣经</li>
</ul>
<h4>代码库</h4>
<ul><li><strong><a href="https://github.com/stack-of-tasks/pinocchio">Pinocchio</a></strong>: 高效动力学计算</li>
<li><strong><a href="https://github.com/RobotLocomotion/drake">Drake</a></strong>: 多体动力学</li>
<li><strong><a href="https://pybullet.org/">PyBullet</a></strong>: 仿真 + IK</li>
</ul>
<h4>课程</h4>
<ul><li><strong><a href="https://cs.stanford.edu/groups/manips/teaching/cs223a/">Stanford CS223A</a></strong>: Introduction to Robotics</li>
<li><strong><a href="https://rsl.ethz.ch/education-students/lectures.html">ETH Robot Dynamics</a></strong>: 动力学与控制</li>
</ul>
<hr></p>

<hr></p>

<h2>第18章 感知技术</h2>

<blockquote><strong>面试场景</strong>: "请介绍你熟悉的视觉感知技术方案，以及在机器人系统中的应用。"</blockquote>

<p>本文涵盖机器人领域常用的视觉和多模态感知技术，包括检测、跟踪、Occupancy、深度估计等核心方案。</p>

<hr></p>

<h3>📊 感知技术全景图 (Perception Landscape)</h3>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     机器人感知技术栈                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        传感器层 (Sensors)                            │   │
│   │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐  │   │
│   │  │ RGB 相机 │ │ 深度相机 │ │  LiDAR   │ │  Radar   │ │ 触觉传感 │  │   │
│   │  │ Mono/Stereo│ │ ToF/Struct│ │ 3D点云   │ │ 毫米波   │ │ GelSight │  │   │
│   │  └──────────┘ └──────────┘ └──────────┘ └──────────┘ └──────────┘  │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                      感知算法层 (Perception)                         │   │
│   │                                                                     │   │
│   │   2D 感知              3D 感知              时序感知                 │   │
│   │   ─────────           ─────────            ─────────                │   │
│   │   • 目标检测          • 3D 检测            • 目标跟踪               │   │
│   │   • 语义分割          • 点云分割           • 轨迹预测               │   │
│   │   • 实例分割          • Occupancy          • 光流估计               │   │
│   │   • 关键点检测        • 深度估计           • 场景流                 │   │
│   │   • 开放词汇检测      • BEV 感知           • 视频理解               │   │
│   │                                                                     │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                      融合与理解层 (Fusion)                           │   │
│   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐               │   │
│   │  │ 多模态融合   │  │ 时空融合     │  │ 语义理解     │               │   │
│   │  │ Vision+LiDAR │  │ 多帧聚合     │  │ VLM 推理     │               │   │
│   │  └──────────────┘  └──────────────┘  └──────────────┘               │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        输出层 (Output)                               │   │
│   │  • 物体位姿 (6-DoF Pose)     • 场景表示 (Occupancy/NeRF)            │   │
│   │  • 语义地图 (Semantic Map)   • 可抓取点 (Grasp Points)              │   │
│   │  • 运动预测 (Motion Pred)    • 碰撞检测 (Collision)                 │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   💡 VLA 核心: 感知输出 → VLM 理解 → 动作生成                               │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>1. 目标检测 (Object Detection)</h3>

<h4>1.1 2D 目标检测演进</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     2D 目标检测技术演进                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Two-Stage (两阶段)                    One-Stage (单阶段)                  │
│   ─────────────────                     ─────────────────                   │
│   R-CNN (2014)                          YOLO v1 (2016)                      │
│      ↓                                     ↓                                │
│   Fast R-CNN (2015)                     SSD (2016)                          │
│      ↓                                     ↓                                │
│   Faster R-CNN (2015)                   YOLOv3 (2018)                       │
│      ↓                                     ↓                                │
│   Mask R-CNN (2017)                     YOLOv5/v7/v8 (2020-2023)            │
│      ↓                                     ↓                                │
│   Cascade R-CNN (2018)                  YOLO11 (2024)                       │
│                                                                             │
│   Transformer-Based                     Anchor-Free                         │
│   ─────────────────                     ────────────                        │
│   DETR (2020)                           CenterNet (2019)                    │
│      ↓                                     ↓                                │
│   Deformable DETR (2021)                FCOS (2019)                         │
│      ↓                                     ↓                                │
│   DINO (2022)                           RepPoints (2019)                    │
│      ↓                                                                      │
│   Co-DETR (2023)                                                            │
│      ↓                                                                      │
│   RT-DETR (2023) ← 实时 Transformer                                         │
│                                                                             │
│   💡 机器人常用: YOLOv8 (速度), RT-DETR (精度), DINO (开放词汇)             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>1.2 主流检测器对比</h4>

<table>
<thead><tr>
<th>模型</th>
<th>Backbone</th>
<th>COCO mAP</th>
<th>速度 (FPS)</th>
<th>特点</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>YOLOv8-N</strong></td>
<td>CSPDarknet</td>
<td>37.3</td>
<td>900+</td>
<td>极速</td>
<td>边缘部署</td>
</tr>
<tr>
<td><strong>YOLOv8-X</strong></td>
<td>CSPDarknet</td>
<td>53.9</td>
<td>280</td>
<td>高精度</td>
<td>服务器</td>
</tr>
<tr>
<td><strong>RT-DETR-L</strong></td>
<td>HGNetv2</td>
<td>53.0</td>
<td>114</td>
<td>无 NMS</td>
<td>实时高精度</td>
</tr>
<tr>
<td><strong>DINO</strong></td>
<td>Swin-L</td>
<td>63.3</td>
<td>5</td>
<td>SOTA</td>
<td>离线分析</td>
</tr>
<tr>
<td><strong>Grounding DINO</strong></td>
<td>Swin-T</td>
<td>52.5</td>
<td>15</td>
<td>开放词汇</td>
<td>零样本检测</td>
</tr>
<tr>
<td><strong>YOLO-World</strong></td>
<td>YOLOv8</td>
<td>45.7</td>
<td>52</td>
<td>开放词汇+快</td>
<td>机器人抓取</td>
</tr>
</tbody></table>

<h4>1.3 开放词汇检测 (Open-Vocabulary Detection)</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     开放词汇检测架构                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   传统检测: 只能检测训练时见过的类别 (80 类 COCO)                           │
│   开放词汇: 可以检测任意文本描述的物体                                      │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     Grounding DINO 架构                              │   │
│   │                                                                     │   │
│   │   ┌──────────┐                      ┌──────────┐                    │   │
│   │   │  Image   │                      │  Text    │                    │   │
│   │   │  输入    │                      │ "红色杯子"│                    │   │
│   │   └────┬─────┘                      └────┬─────┘                    │   │
│   │        │                                 │                          │   │
│   │        ▼                                 ▼                          │   │
│   │   ┌──────────┐                      ┌──────────┐                    │   │
│   │   │ Swin-T   │                      │ BERT     │                    │   │
│   │   │ Backbone │                      │ Encoder  │                    │   │
│   │   └────┬─────┘                      └────┬─────┘                    │   │
│   │        │                                 │                          │   │
│   │        └────────────┬────────────────────┘                          │   │
│   │                     ▼                                               │   │
│   │              ┌─────────────┐                                        │   │
│   │              │ Cross-Modal │ ← 图文特征交互                         │   │
│   │              │  Attention  │                                        │   │
│   │              └──────┬──────┘                                        │   │
│   │                     │                                               │   │
│   │                     ▼                                               │   │
│   │              ┌─────────────┐                                        │   │
│   │              │ Detection   │                                        │   │
│   │              │   Head      │                                        │   │
│   │              └──────┬──────┘                                        │   │
│   │                     │                                               │   │
│   │                     ▼                                               │   │
│   │              [Box: (x,y,w,h), Score: 0.95]                          │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   应用: 机器人抓取任意物体 - "帮我拿那个蓝色的水瓶"                         │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>1.4 检测代码示例</h4>

<pre><code class="python">from ultralytics import YOLOWorld</p>

<p>model = YOLOWorld('yolov8x-worldv2.pt')</p>

<p>model.set_classes(["red cup", "blue bottle", "keyboard", "mouse"])</p>

<p>results = model.predict(
    source='robot_view.jpg',
    conf=0.25,
    iou=0.45,
    device='cuda:0'
)</p>

<p>for result in results:
    boxes = result.boxes
    for box in boxes:
        cls_id = int(box.cls[0])
        conf = float(box.conf[0])
        xyxy = box.xyxy[0].tolist()
        class_name = model.names[cls_id]
        print(f"检测到 {class_name}: {conf:.2f}, 位置: {xyxy}")</p>

<p>from groundingdino.util.inference import load_model, predict</p>

<p>model = load_model("groundingdino_swinb_cogcoor.pth")</p>

<p>text_prompt = "red cup . blue bottle . person"</p>

<p>boxes, logits, phrases = predict(
    model=model,
    image=image,
    caption=text_prompt,
    box_threshold=0.35,
    text_threshold=0.25
)
</code></pre>

<hr></p>

<h3>2. 语义/实例分割 (Segmentation)</h3>

<h4>2.1 分割任务类型</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     分割任务对比                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   原图                语义分割            实例分割            全景分割       │
│   ┌─────────┐        ┌─────────┐        ┌─────────┐        ┌─────────┐     │
│   │ 🚗  🚗  │        │ ██  ██  │        │ ▓▓  ░░  │        │ ▓▓  ░░  │     │
│   │    🚶   │   →    │    ██   │   →    │    ▒▒   │   →    │    ▒▒   │     │
│   │ 🌳 🏠  │        │ ██  ██  │        │ ██  ██  │        │ ▓▓  ░░  │     │
│   └─────────┘        └─────────┘        └─────────┘        └─────────┘     │
│                      同类同色            同类不同色          Thing+Stuff    │
│                                                                             │
│   任务          输出                  应用场景                               │
│   ─────────────────────────────────────────────────────────────────────     │
│   语义分割      像素级类别标签        场景理解, 可行驶区域                   │
│   实例分割      像素级实例 ID         物体计数, 机器人抓取                   │
│   全景分割      语义 + 实例           自动驾驶, 完整场景理解                 │
│   部件分割      物体部件级分割        精细操作, 人体姿态                     │
│                                                                             │
│   💡 机器人抓取常用: 实例分割 (区分多个同类物体)                            │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2.2 主流分割模型</h4>

<table>
<thead><tr>
<th>模型</th>
<th>类型</th>
<th>特点</th>
<th>速度</th>
<th>应用</th>
</tr></thead>
<tbody>
<tr>
<td><strong>SAM (Segment Anything)</strong></td>
<td>通用分割</td>
<td>零样本, 交互式</td>
<td>中</td>
<td>标注工具, 机器人</td>
</tr>
<tr>
<td><strong>SAM 2</strong></td>
<td>视频分割</td>
<td>时序一致性</td>
<td>中</td>
<td>视频跟踪</td>
</tr>
<tr>
<td><strong>Mask2Former</strong></td>
<td>全景分割</td>
<td>统一架构</td>
<td>慢</td>
<td>场景理解</td>
</tr>
<tr>
<td><strong>OneFormer</strong></td>
<td>全景分割</td>
<td>单模型多任务</td>
<td>慢</td>
<td>通用分割</td>
</tr>
<tr>
<td><strong>FastSAM</strong></td>
<td>实例分割</td>
<td>SAM 加速版</td>
<td>快</td>
<td>实时应用</td>
</tr>
<tr>
<td><strong>MobileSAM</strong></td>
<td>实例分割</td>
<td>轻量化 SAM</td>
<td>快</td>
<td>边缘部署</td>
</tr>
<tr>
<td><strong>Grounded SAM</strong></td>
<td>开放词汇分割</td>
<td>文本驱动分割</td>
<td>中</td>
<td>机器人抓取</td>
</tr>
</tbody></table>

<h4>2.3 SAM (Segment Anything Model)</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     SAM 架构                                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        Image Encoder (ViT-H)                         │   │
│   │                                                                     │   │
│   │   输入图像 ──────► 图像特征 (256 × 64 × 64)                          │   │
│   │   (1024×1024)      (一次编码, 多次使用)                              │   │
│   └────────────────────────────────┬────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        Prompt Encoder                                │   │
│   │                                                                     │   │
│   │   Prompt 类型:                                                      │   │
│   │   • 点 (Point): 前景/背景点                                         │   │
│   │   • 框 (Box): 边界框                                                │   │
│   │   • 文本 (Text): 类别描述 (需要 Grounded SAM)                       │   │
│   │   • 掩码 (Mask): 粗略掩码                                           │   │
│   └────────────────────────────────┬────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        Mask Decoder                                  │   │
│   │                                                                     │   │
│   │   图像特征 + Prompt 特征 ──► 多个候选掩码 + IoU 分数                 │   │
│   │                                                                     │   │
│   │   输出: 3 个掩码 (whole, part, subpart) + 置信度                    │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   特点:                                                                     │
│   • 零样本: 无需针对特定类别训练                                           │
│   • 交互式: 支持点击、框选等多种 prompt                                    │
│   • 高质量: 边缘精细, 适合精确抓取                                         │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2.4 分割代码示例</h4>

<pre><code class="python">from sam2.build_sam import build_sam2_video_predictor</p>

<p>predictor = build_sam2_video_predictor(
    "sam2_hiera_large.pt",
    device="cuda"
)</p>

<p>with torch.inference_mode():
    state = predictor.init_state(video_path="robot_video.mp4")
    
    # 在第一帧给一个点 prompt
    frame_idx, obj_ids, masks = predictor.add_new_points_or_box(
        inference_state=state,
        frame_idx=0,
        obj_id=1,
        points=[[500, 300]],  # 点击位置
        labels=[1]  # 1=前景, 0=背景
    )
    
    # 传播到整个视频
    for frame_idx, obj_ids, masks in predictor.propagate_in_video(state):
        # masks: 每帧的分割掩码
        process_mask(frame_idx, masks)</p>

<p>from groundingdino.util.inference import predict as gd_predict
from segment_anything import sam_model_registry, SamPredictor</p>

<p>boxes, logits, phrases = gd_predict(
    model=gd_model,
    image=image,
    caption="red apple . green apple",
    box_threshold=0.3
)</p>

<p>sam = sam_model_registry<a href="checkpoint="sam_vit_h.pth"">"vit_h"</a>
predictor = SamPredictor(sam)
predictor.set_image(image)</p>

<p>masks, scores, _ = predictor.predict(
    box=boxes[0],  # 第一个检测框
    multimask_output=False
)
</code></pre>

<hr></p>

<h3>3. 目标跟踪 (Object Tracking)</h3>

<h4>3.1 跟踪任务分类</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     目标跟踪任务分类                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                    单目标跟踪 (SOT)                                   │   │
│   │                                                                     │   │
│   │   输入: 第一帧的目标框                                               │   │
│   │   输出: 后续帧的目标位置                                             │   │
│   │                                                                     │   │
│   │   Frame 1         Frame 2         Frame 3                           │   │
│   │   ┌─────────┐    ┌─────────┐    ┌─────────┐                        │   │
│   │   │  [目标] │ →  │   [目标]│ →  │     [目标]                        │   │
│   │   └─────────┘    └─────────┘    └─────────┘                        │   │
│   │                                                                     │   │
│   │   代表模型: SiamFC, SiamRPN++, OSTrack, MixFormer                   │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                   多目标跟踪 (MOT)                                    │   │
│   │                                                                     │   │
│   │   输入: 视频序列                                                     │   │
│   │   输出: 所有目标的轨迹 (ID + 位置)                                   │   │
│   │                                                                     │   │
│   │   Frame 1         Frame 2         Frame 3                           │   │
│   │   ┌─────────┐    ┌─────────┐    ┌─────────┐                        │   │
│   │   │ [1] [2] │ →  │ [1]  [2]│ →  │  [1] [2]│                        │   │
│   │   │   [3]   │    │    [3]  │    │   [3]   │                        │   │
│   │   └─────────┘    └─────────┘    └─────────┘                        │   │
│   │                                                                     │   │
│   │   代表模型: SORT, DeepSORT, ByteTrack, OC-SORT, BoT-SORT           │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   💡 机器人应用: MOT 跟踪多个物体, SOT 跟踪特定目标                         │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>3.2 多目标跟踪 (MOT) 范式</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     MOT 技术演进                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Tracking-by-Detection (检测后跟踪)                                        │
│   ─────────────────────────────────                                         │
│                                                                             │
│   ┌──────────┐    ┌──────────────┐    ┌──────────────┐                     │
│   │ Detector │ →  │ Association  │ →  │ Track Mgmt   │                     │
│   │ (YOLO等) │    │ (匹配算法)    │    │ (轨迹管理)   │                     │
│   └──────────┘    └──────────────┘    └──────────────┘                     │
│                                                                             │
│   匹配算法演进:                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                     │   │
│   │   SORT (2016)           DeepSORT (2017)        ByteTrack (2022)     │   │
│   │   ─────────────         ───────────────        ─────────────────    │   │
│   │   IoU + Kalman          IoU + ReID 特征        高低分框分离匹配      │   │
│   │   简单快速              外观特征增强           利用低置信度检测      │   │
│   │                                                                     │   │
│   │   OC-SORT (2023)        BoT-SORT (2022)        StrongSORT (2023)    │   │
│   │   ─────────────         ───────────────        ─────────────────    │   │
│   │   观测中心化            多线索融合             强 ReID + 后处理      │   │
│   │   处理遮挡              相机运动补偿           SOTA 精度            │   │
│   │                                                                     │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   Joint Detection &amp; Tracking (联合检测跟踪)                                 │
│   ────────────────────────────────────────                                  │
│   • FairMOT: 检测 + ReID 联合训练                                          │
│   • TrackFormer: Transformer 端到端跟踪                                    │
│   • MOTR: 多目标跟踪 Transformer                                           │
│                                                                             │
│   💡 机器人推荐: ByteTrack (速度) 或 BoT-SORT (精度)                        │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>3.3 ByteTrack 算法详解</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     ByteTrack 核心思想                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   传统方法: 只使用高置信度检测框 (score &gt; 0.5)                              │
│   ByteTrack: 高低分框分离匹配, 充分利用低置信度检测                         │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        算法流程                                      │   │
│   │                                                                     │   │
│   │   检测结果                                                          │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   ┌─────────────────────────────────────────┐                       │   │
│   │   │ 按置信度分离:                           │                       │   │
│   │   │ • 高分框 D_high (score &gt; τ_high)        │                       │   │
│   │   │ • 低分框 D_low  (τ_low &lt; score &lt; τ_high)│                       │   │
│   │   └─────────────────────────────────────────┘                       │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   ┌─────────────────────────────────────────┐                       │   │
│   │   │ 第一次匹配: 轨迹 T ↔ 高分框 D_high      │                       │   │
│   │   │ 使用 IoU + Kalman 预测                  │                       │   │
│   │   │ → 匹配成功的轨迹更新                    │                       │   │
│   │   │ → 未匹配轨迹 T_remain                   │                       │   │
│   │   └─────────────────────────────────────────┘                       │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   ┌─────────────────────────────────────────┐                       │   │
│   │   │ 第二次匹配: T_remain ↔ 低分框 D_low     │ ← 关键创新!           │   │
│   │   │ 被遮挡的目标可能只有低置信度检测        │                       │   │
│   │   │ → 匹配成功的轨迹恢复                    │                       │   │
│   │   └─────────────────────────────────────────┘                       │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   ┌─────────────────────────────────────────┐                       │   │
│   │   │ 轨迹管理:                               │                       │   │
│   │   │ • 未匹配高分框 → 新建轨迹               │                       │   │
│   │   │ • 连续 N 帧未匹配 → 删除轨迹            │                       │   │
│   │   └─────────────────────────────────────────┘                       │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   优势: 遮挡场景下 ID Switch 大幅减少                                       │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>3.4 跟踪代码示例</h4>

<pre><code class="python">from ultralytics import YOLO</p>

<p>model = YOLO('yolov8x.pt')</p>

<p>results = model.track(
    source='robot_video.mp4',
    tracker='bytetrack.yaml',  # 使用 ByteTrack
    persist=True,  # 跨帧保持 ID
    conf=0.25,
    iou=0.5,
    device='cuda:0'
)</p>

<p>for result in results:
    boxes = result.boxes
    if boxes.id is not None:
        track_ids = boxes.id.int().tolist()
        for i, track_id in enumerate(track_ids):
            bbox = boxes.xyxy[i].tolist()
            cls = int(boxes.cls[i])
            print(f"ID {track_id}: {model.names[cls]} at {bbox}")</p>

<p>from sam2.build_sam import build_sam2_video_predictor</p>

<p>predictor = build_sam2_video_predictor("sam2_hiera_large.pt")</p>

<p>state = predictor.init_state(video_path="video.mp4")</p>

<p>predictor.add_new_points_or_box(
    state, frame_idx=0, obj_id=1,
    points=[[300, 200]], labels=[1]
)</p>

<p>for frame_idx, obj_ids, masks in predictor.propagate_in_video(state):
    # masks: 像素级分割掩码
    # 比 bbox 跟踪更精确
    pass
</code></pre>

<hr></p>

<h3>4. 3D 感知 (3D Perception)</h3>

<h4>4.1 3D 目标检测</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     3D 目标检测方法分类                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   输入模态              代表方法                 特点                        │
│   ─────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   纯 LiDAR              PointPillars            点云柱状化, 快速             │
│   (点云)                CenterPoint             中心点检测, 无 anchor       │
│                         VoxelNet                体素化 + 3D CNN             │
│                         PointNet++              点云直接处理                │
│                                                                             │
│   纯 Camera             FCOS3D                  单目 3D 检测                │
│   (图像)                DETR3D                  Transformer 3D 检测        │
│                         BEVDet                  图像 → BEV → 检测          │
│                         BEVFormer               时空 BEV 特征               │
│                                                                             │
│   Camera + LiDAR        BEVFusion               多模态 BEV 融合             │
│   (融合)                TransFusion             Transformer 融合           │
│                         DeepFusion              深度特征融合                │
│                                                                             │
│   💡 机器人场景: 室内常用 RGB-D, 室外/自动驾驶用 LiDAR                      │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>4.2 BEV (Bird's Eye View) 感知</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     BEV 感知架构                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   为什么需要 BEV?                                                           │
│   • 透视图 (Perspective View): 近大远小, 遮挡严重                           │
│   • 鸟瞰图 (BEV): 尺度一致, 无遮挡, 适合规划                               │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     BEVFormer 架构                                   │   │
│   │                                                                     │   │
│   │   多相机输入 (6 个环视相机)                                          │   │
│   │   ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐                  │   │
│   │   │Front│ │F-L  │ │F-R  │ │Back │ │B-L  │ │B-R  │                  │   │
│   │   └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘                  │   │
│   │      │       │       │       │       │       │                      │   │
│   │      └───────┴───────┴───┬───┴───────┴───────┘                      │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ 2D Backbone │ (ResNet / Swin)                   │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ Spatial     │ ← 图像特征 → BEV 特征             │   │
│   │                   │ Cross-Attn  │   (可学习的视角转换)              │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ Temporal    │ ← 融合历史帧                       │   │
│   │                   │ Self-Attn   │   (时序建模)                      │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ BEV Feature │ (H × W × C)                       │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │          ┌───────────────┼───────────────┐                          │   │
│   │          ▼               ▼               ▼                          │   │
│   │   ┌─────────────┐ ┌─────────────┐ ┌─────────────┐                   │   │
│   │   │ 3D Detection│ │ Map Segm   │ │ Motion Pred │                   │   │
│   │   │ (3D 检测)   │ │ (地图分割) │ │ (运动预测)  │                   │   │
│   │   └─────────────┘ └─────────────┘ └─────────────┘                   │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   优势: 统一的 BEV 表示, 支持多任务                                         │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>4.3 深度估计 (Depth Estimation)</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     深度估计方法                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   方法              输入          精度       速度       适用场景             │
│   ─────────────────────────────────────────────────────────────────────     │
│   结构光            RGB-D 相机    高         实时       室内, 近距离         │
│   (Structured Light) (RealSense)                       (&lt; 10m)              │
│                                                                             │
│   ToF               ToF 相机      中         实时       室内/室外            │
│   (Time-of-Flight)  (Azure Kinect)                     (&lt; 5m)               │
│                                                                             │
│   双目立体          双目相机      中         实时       室外, 纹理丰富       │
│   (Stereo)          (ZED)                              (需要标定)           │
│                                                                             │
│   单目深度估计      单目相机      低-中      快         通用, 成本低         │
│   (Monocular)       (任意相机)                         (相对深度)           │
│                                                                             │
│   LiDAR             激光雷达      最高       实时       室外, 长距离         │
│                     (Velodyne)                         (&gt; 100m)             │
│                                                                             │
│   💡 机器人常用: RGB-D (室内) + 单目深度估计 (补充)                         │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>4.4 单目深度估计模型</h4>

<table>
<thead><tr>
<th>模型</th>
<th>特点</th>
<th>输出</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Depth Anything V2</strong></td>
<td>通用, 零样本</td>
<td>相对深度</td>
<td>通用场景</td>
</tr>
<tr>
<td><strong>Metric3D</strong></td>
<td>绝对深度</td>
<td>米制深度</td>
<td>需要真实尺度</td>
</tr>
<tr>
<td><strong>ZoeDepth</strong></td>
<td>多数据集训练</td>
<td>米制深度</td>
<td>室内/室外</td>
</tr>
<tr>
<td><strong>MiDaS</strong></td>
<td>鲁棒性强</td>
<td>相对深度</td>
<td>多样场景</td>
</tr>
<tr>
<td><strong>UniDepth</strong></td>
<td>统一架构</td>
<td>相对/绝对</td>
<td>研究用途</td>
</tr>
</tbody></table>

<pre><code class="python">from depth_anything_v2.dpt import DepthAnythingV2</p>

<p>model = DepthAnythingV2(
    encoder='vitl',
    features=256,
    out_channels=[256, 512, 1024, 1024]
)
model.load_state_dict(torch.load('depth_anything_v2_vitl.pth'))
model.eval()</p>

<p>image = cv2.imread('robot_view.jpg')
depth = model.infer_image(image)  # 相对深度图</p>

<p>depth_colored = cv2.applyColorMap(
    (depth * 255).astype(np.uint8), 
    cv2.COLORMAP_INFERNO
)
</code></pre>

<hr></p>

<h3>5. Occupancy 感知 (Occupancy Perception)</h3>

<h4>5.1 Occupancy 概念</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     Occupancy 网络概念                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   什么是 Occupancy?                                                         │
│   将 3D 空间离散化为体素网格, 预测每个体素的占用状态和语义类别              │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                     │   │
│   │   传统 3D 检测              Occupancy                               │   │
│   │   ─────────────             ─────────                               │   │
│   │   输出: 3D Bbox             输出: 体素网格                          │   │
│   │   ┌─────────────┐           ┌─────────────┐                        │   │
│   │   │   ┌───┐     │           │ ░░▓▓░░▓▓░░ │                        │   │
│   │   │   │车 │     │           │ ░░▓▓▓▓▓▓░░ │                        │   │
│   │   │   └───┘     │           │ ░░░░░░░░░░ │                        │   │
│   │   └─────────────┘           └─────────────┘                        │   │
│   │   只有物体边界框            每个体素都有占用状态                    │   │
│   │                                                                     │   │
│   │   优势:                                                             │   │
│   │   • 完整场景表示: 不仅是物体, 还有道路、建筑、植被                  │   │
│   │   • 任意形状: 不受 Bbox 限制, 可表示不规则物体                      │   │
│   │   • 规划友好: 直接用于碰撞检测和路径规划                            │   │
│   │                                                                     │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   体素分辨率: 通常 0.1m ~ 0.5m, 取决于计算资源和精度需求                    │
│   语义类别: 车辆、行人、道路、建筑、植被、天空等                            │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>5.2 Occupancy 网络架构</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     Occupancy Network 架构 (以 SurroundOcc 为例)            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     多相机输入                                       │   │
│   │   ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐                  │   │
│   │   │ Cam1│ │ Cam2│ │ Cam3│ │ Cam4│ │ Cam5│ │ Cam6│                  │   │
│   │   └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘                  │   │
│   │      └───────┴───────┴───┬───┴───────┴───────┘                      │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ 2D Backbone │                                   │   │
│   │                   │ (ResNet/Swin)│                                  │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ 2D → 3D    │ ← View Transformation             │   │
│   │                   │ Lifting    │   (LSS / BEVFormer 风格)          │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ 3D Encoder │ ← 3D 特征提取                      │   │
│   │                   │ (3D Conv)  │                                    │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────┐                                   │   │
│   │                   │ Occupancy  │                                    │   │
│   │                   │ Decoder    │                                    │   │
│   │                   └──────┬──────┘                                   │   │
│   │                          │                                          │   │
│   │                          ▼                                          │   │
│   │                   ┌─────────────────────────────────┐               │   │
│   │                   │     3D Occupancy Grid           │               │   │
│   │                   │     (X × Y × Z × C)             │               │   │
│   │                   │                                 │               │   │
│   │                   │  每个体素: [occupied, semantic] │               │   │
│   │                   └─────────────────────────────────┘               │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   代表模型: SurroundOcc, OccNet, TPVFormer, FB-OCC, OpenOccupancy          │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>5.3 Occupancy 在机器人中的应用</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     Occupancy 机器人应用                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   1. 碰撞检测与避障                                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   Occupancy Grid + 机器人体积 → 碰撞查询                            │   │
│   │                                                                     │   │
│   │   for voxel in robot_volume:                                        │   │
│   │       if occupancy[voxel] &gt; threshold:                              │   │
│   │           collision_detected = True                                 │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   2. 路径规划                                                               │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   Occupancy → 可通行区域 → A* / RRT 规划                            │   │
│   │                                                                     │   │
│   │   ░░░░░░░░░░                                                        │   │
│   │   ░░▓▓▓▓░░░░   ▓ = 障碍物                                           │   │
│   │   ░░░░░░░░░░   ░ = 可通行                                           │   │
│   │   ░░░░▓▓░░░░   * = 规划路径                                         │   │
│   │   ░░░░░░*░░░                                                        │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   3. 抓取规划                                                               │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   物体 Occupancy → 抓取点采样 → 碰撞检测 → 可行抓取姿态             │   │
│   │                                                                     │   │
│   │   • 找到物体表面体素                                                │   │
│   │   • 生成候选抓取姿态                                                │   │
│   │   • 检查夹爪体积与场景 Occupancy 是否碰撞                           │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   4. 场景重建与建图                                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   多帧 Occupancy 融合 → 全局地图                                    │   │
│   │                                                                     │   │
│   │   • TSDF (Truncated Signed Distance Function) 融合                  │   │
│   │   • 增量式更新                                                      │   │
│   │   • 语义标签传播                                                    │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>6. 多模态融合 (Multimodal Fusion)</h3>

<h4>6.1 融合策略</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     多模态融合策略                                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Early Fusion (早期融合)                                                   │
│   ─────────────────────                                                     │
│   ┌─────────┐ ┌─────────┐                                                  │
│   │ Camera  │ │ LiDAR   │                                                  │
│   └────┬────┘ └────┬────┘                                                  │
│        └─────┬─────┘                                                        │
│              ▼                                                              │
│        ┌───────────┐                                                        │
│        │  Concat   │ ← 原始数据/特征拼接                                    │
│        └─────┬─────┘                                                        │
│              ▼                                                              │
│        ┌───────────┐                                                        │
│        │  Encoder  │                                                        │
│        └───────────┘                                                        │
│   优点: 信息保留完整                                                        │
│   缺点: 模态对齐困难, 计算量大                                              │
│                                                                             │
│   ─────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   Mid Fusion (中期融合)                                                     │
│   ────────────────────                                                      │
│   ┌─────────┐ ┌─────────┐                                                  │
│   │ Camera  │ │ LiDAR   │                                                  │
│   └────┬────┘ └────┬────┘                                                  │
│        ▼           ▼                                                        │
│   ┌─────────┐ ┌─────────┐                                                  │
│   │Encoder 1│ │Encoder 2│ ← 各自编码                                       │
│   └────┬────┘ └────┬────┘                                                  │
│        └─────┬─────┘                                                        │
│              ▼                                                              │
│        ┌───────────┐                                                        │
│        │  Fusion   │ ← 特征融合 (Attention / Concat)                       │
│        └─────┬─────┘                                                        │
│              ▼                                                              │
│        ┌───────────┐                                                        │
│        │  Decoder  │                                                        │
│        └───────────┘                                                        │
│   优点: 灵活, 效果好                                                        │
│   缺点: 需要设计融合模块                                                    │
│                                                                             │
│   ─────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   Late Fusion (晚期融合)                                                    │
│   ────────────────────                                                      │
│   ┌─────────┐ ┌─────────┐                                                  │
│   │ Camera  │ │ LiDAR   │                                                  │
│   └────┬────┘ └────┬────┘                                                  │
│        ▼           ▼                                                        │
│   ┌─────────┐ ┌─────────┐                                                  │
│   │ Model 1 │ │ Model 2 │ ← 独立模型                                       │
│   └────┬────┘ └────┬────┘                                                  │
│        ▼           ▼                                                        │
│   ┌─────────┐ ┌─────────┐                                                  │
│   │ Pred 1  │ │ Pred 2  │ ← 各自预测                                       │
│   └────┬────┘ └────┬────┘                                                  │
│        └─────┬─────┘                                                        │
│              ▼                                                              │
│        ┌───────────┐                                                        │
│        │  Ensemble │ ← 结果融合 (NMS / 投票)                               │
│        └───────────┘                                                        │
│   优点: 简单, 模块化                                                        │
│   缺点: 信息利用不充分                                                      │
│                                                                             │
│   💡 VLA 常用: Mid Fusion (视觉 + 语言 + 本体感觉)                          │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>6.2 BEVFusion 架构</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     BEVFusion: Camera + LiDAR 融合                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                     │   │
│   │   Camera Branch                    LiDAR Branch                     │   │
│   │   ──────────────                   ─────────────                    │   │
│   │   ┌──────────┐                     ┌──────────┐                     │   │
│   │   │ 6 Cameras│                     │ LiDAR PC │                     │   │
│   │   └────┬─────┘                     └────┬─────┘                     │   │
│   │        ▼                                ▼                           │   │
│   │   ┌──────────┐                     ┌──────────┐                     │   │
│   │   │ 2D CNN   │                     │PointPillars                    │   │
│   │   │ Backbone │                     │ Encoder  │                     │   │
│   │   └────┬─────┘                     └────┬─────┘                     │   │
│   │        ▼                                ▼                           │   │
│   │   ┌──────────┐                     ┌──────────┐                     │   │
│   │   │ LSS      │                     │ 3D → BEV │                     │   │
│   │   │ 2D → BEV │                     │ Scatter  │                     │   │
│   │   └────┬─────┘                     └────┬─────┘                     │   │
│   │        │                                │                           │   │
│   │        ▼                                ▼                           │   │
│   │   ┌──────────┐                     ┌──────────┐                     │   │
│   │   │ Camera   │                     │ LiDAR    │                     │   │
│   │   │ BEV Feat │                     │ BEV Feat │                     │   │
│   │   └────┬─────┘                     └────┬─────┘                     │   │
│   │        │                                │                           │   │
│   │        └────────────┬───────────────────┘                           │   │
│   │                     ▼                                               │   │
│   │              ┌─────────────┐                                        │   │
│   │              │ BEV Fusion  │ ← Convolution Fusion                   │   │
│   │              │ (Conv + Add)│                                        │   │
│   │              └──────┬──────┘                                        │   │
│   │                     │                                               │   │
│   │          ┌──────────┼──────────┐                                    │   │
│   │          ▼          ▼          ▼                                    │   │
│   │   ┌──────────┐┌──────────┐┌──────────┐                              │   │
│   │   │3D Detect ││BEV Segm  ││Motion    │                              │   │
│   │   │ Head     ││ Head     ││ Head     │                              │   │
│   │   └──────────┘└──────────┘└──────────┘                              │   │
│   │                                                                     │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   关键创新:                                                                 │
│   • Camera BEV: LSS (Lift-Splat-Shoot) 将 2D 特征提升到 3D                 │
│   • 统一 BEV 空间: Camera 和 LiDAR 特征在同一坐标系融合                    │
│   • 高效融合: 简单的卷积融合即可取得 SOTA 效果                             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>7. 位姿估计 (Pose Estimation)</h3>

<h4>7.1 物体 6-DoF 位姿估计</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     6-DoF 位姿估计方法                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   6-DoF = 3D 位置 (x, y, z) + 3D 旋转 (roll, pitch, yaw)                   │
│                                                                             │
│   方法分类:                                                                 │
│   ─────────                                                                 │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                     │   │
│   │   基于模型 (Model-Based)          无模型 (Model-Free)               │   │
│   │   ─────────────────────           ─────────────────                 │   │
│   │   需要 3D CAD 模型                不需要 CAD 模型                   │   │
│   │                                                                     │   │
│   │   • PVNet: 关键点投票             • FoundationPose: 零样本          │   │
│   │   • DenseFusion: RGB-D 融合       • Gen6D: 生成式位姿              │   │
│   │   • PoseCNN: 直接回归             • SAM-6D: SAM + 位姿              │   │
│   │   • GDR-Net: 几何引导             • MegaPose: 大规模预训练          │   │
│   │                                                                     │   │
│   │   优点: 精度高                    优点: 泛化性强                    │   │
│   │   缺点: 需要 CAD 模型             缺点: 精度略低                    │   │
│   │                                                                     │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   💡 机器人抓取: FoundationPose (零样本) + ICP 精细化                       │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>7.2 FoundationPose 架构</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     FoundationPose: 零样本 6-DoF 位姿估计                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   输入:                                                                     │
│   • RGB-D 图像                                                              │
│   • 物体参考 (CAD 模型 或 少量参考图像)                                     │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        FoundationPose 流程                           │   │
│   │                                                                     │   │
│   │   ┌──────────────────────────────────────────────────────────────┐  │   │
│   │   │  Pose Hypothesis Generation (位姿假设生成)                    │  │   │
│   │   │                                                              │  │   │
│   │   │  方式 1: CAD 模型 → 渲染多视角 → 模板匹配                     │  │   │
│   │   │  方式 2: 参考图像 → 特征匹配 → PnP 求解                       │  │   │
│   │   │                                                              │  │   │
│   │   │  输出: N 个候选位姿 {P_1, P_2, ..., P_N}                      │  │   │
│   │   └──────────────────────────────────────────────────────────────┘  │   │
│   │                              │                                      │   │
│   │                              ▼                                      │   │
│   │   ┌──────────────────────────────────────────────────────────────┐  │   │
│   │   │  Pose Refinement (位姿精化)                                   │  │   │
│   │   │                                                              │  │   │
│   │   │  Render-and-Compare:                                         │  │   │
│   │   │  ┌─────────┐    ┌─────────┐    ┌─────────┐                   │  │   │
│   │   │  │ 候选位姿│ →  │ 渲染图像│ →  │ 与观测  │ → 残差            │  │   │
│   │   │  │   P_i   │    │         │    │ 比较    │                   │  │   │
│   │   │  └─────────┘    └─────────┘    └─────────┘                   │  │   │
│   │   │                                                              │  │   │
│   │   │  迭代优化: P' = P + ΔP (Transformer 预测 ΔP)                  │  │   │
│   │   └──────────────────────────────────────────────────────────────┘  │   │
│   │                              │                                      │   │
│   │                              ▼                                      │   │
│   │   ┌──────────────────────────────────────────────────────────────┐  │   │
│   │   │  Pose Selection (位姿选择)                                    │  │   │
│   │   │                                                              │  │   │
│   │   │  Score 网络评估每个精化后的位姿, 选择最优                     │  │   │
│   │   └──────────────────────────────────────────────────────────────┘  │   │
│   │                              │                                      │   │
│   │                              ▼                                      │   │
│   │                    最终 6-DoF 位姿                                  │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   特点:                                                                     │
│   • 零样本: 不需要针对特定物体训练                                         │
│   • 灵活输入: 支持 CAD 模型或参考图像                                      │
│   • 高精度: 迭代精化达到亚毫米级精度                                       │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>7.3 位姿估计代码示例</h4>

<pre><code class="python">from foundationpose import FoundationPose</p>

<p>model = FoundationPose(
    model_path='foundation_pose.pth',
    device='cuda'
)</p>

<p>rgb = cv2.imread('scene.png')
depth = np.load('depth.npy')
mask = get_object_mask(rgb)  # SAM 或其他分割
mesh = trimesh.load('object.obj')</p>

<p>pose = model.estimate_pose(
    rgb=rgb,
    depth=depth,
    mask=mask,
    mesh=mesh
)</p>

<p>print(f"位置: {pose[:3, 3]}")
print(f"旋转: {pose[:3, :3]}")</p>

<p>rendered = model.render_pose(mesh, pose, intrinsics)
overlay = cv2.addWeighted(rgb, 0.5, rendered, 0.5, 0)
</code></pre>

<hr></p>

<h3>8. 面试高频 Q&A</h3>

<h4>Q1: 目标检测中 Anchor-Based 和 Anchor-Free 的区别？</h4>

<table>
<thead><tr>
<th>维度</th>
<th>Anchor-Based</th>
<th>Anchor-Free</th>
</tr></thead>
<tbody>
<tr>
<td><strong>代表</strong></td>
<td>Faster R-CNN, YOLOv3</td>
<td>CenterNet, FCOS</td>
</tr>
<tr>
<td><strong>原理</strong></td>
<td>预设锚框, 预测偏移</td>
<td>直接预测中心点/边界</td>
</tr>
<tr>
<td><strong>超参数</strong></td>
<td>需要设计 anchor 尺寸/比例</td>
<td>无需 anchor 超参数</td>
</tr>
<tr>
<td><strong>密集物体</strong></td>
<td>可能漏检</td>
<td>中心点重叠问题</td>
</tr>
<tr>
<td><strong>训练</strong></td>
<td>需要正负样本匹配</td>
<td>更简单直接</td>
</tr>
<tr>
<td><strong>趋势</strong></td>
<td>逐渐被取代</td>
<td>主流方向</td>
</tr>
</tbody></table>

<h4>Q2: NMS 的作用和改进方法？</h4>

<p><strong>NMS (Non-Maximum Suppression)</strong>:
<ul><li><strong>作用</strong>: 去除重叠的冗余检测框</li>
<li><strong>流程</strong>: 按置信度排序 → 保留最高分框 → 删除 IoU > 阈值的框 → 重复</li>
</ul>
<strong>改进方法</strong>:
<ul><li><strong>Soft-NMS</strong>: 不直接删除, 而是降低重叠框的分数</li>
<li><strong>DIoU-NMS</strong>: 考虑中心点距离, 不只是 IoU</li>
<li><strong>无 NMS</strong>: DETR 系列直接输出去重结果</li>
</ul>
<h4>Q3: 如何处理小目标检测？</h4>

<p><li><strong>多尺度特征</strong>: FPN (Feature Pyramid Network) 融合多层特征</li>
<li><strong>高分辨率输入</strong>: 增大输入图像尺寸</li>
<li><strong>数据增强</strong>: Mosaic, Copy-Paste 增加小目标样本</li>
<li><strong>Anchor 设计</strong>: 增加小尺寸 anchor</li>
<li><strong>注意力机制</strong>: 增强小目标区域特征</li>
<li><strong>SAHI</strong>: 滑动窗口切图检测, 再合并结果</li></p>

<h4>Q4: BEV 感知相比传统 3D 检测的优势？</h4>

<p><li><strong>统一表示</strong>: 不同传感器特征在同一坐标系融合</li>
<li><strong>尺度一致</strong>: 无透视畸变, 远近物体尺度相同</li>
<li><strong>规划友好</strong>: BEV 地图直接用于路径规划</li>
<li><strong>多任务</strong>: 检测、分割、预测共享 BEV 特征</li>
<li><strong>时序建模</strong>: 方便融合历史帧信息</li></p>

<h4>Q5: 机器人抓取中常用的感知 Pipeline？</h4>

<pre><code class="">1. 场景感知
   RGB-D 输入 → 目标检测 (YOLO-World) → 实例分割 (SAM)
   
<li>位姿估计</li>
   分割掩码 + 深度 → 6-DoF 位姿 (FoundationPose)
   
<li>抓取规划</li>
   物体位姿 + 场景点云 → 抓取点生成 (GraspNet) → 碰撞检测
   
<li>执行</li>
   抓取姿态 → 运动规划 → 机械臂执行
</code></pre>

<hr></p>

<h3>📚 推荐资源</h3>

<h4>论文</h4>
<ul><li><a href="https://arxiv.org/abs/2401.17270">YOLO-World: Real-Time Open-Vocabulary Object Detection</a></li>
<li><a href="https://arxiv.org/abs/2304.02643">Segment Anything</a></li>
<li><a href="https://arxiv.org/abs/2110.06864">ByteTrack: Multi-Object Tracking by Associating Every Detection Box</a></li>
<li><a href="https://arxiv.org/abs/2203.17270">BEVFormer: Learning Bird's-Eye-View Representation</a></li>
<li><a href="https://arxiv.org/abs/2312.08344">FoundationPose: Unified 6D Pose Estimation</a></li>
</ul>
<h4>代码库</h4>
<ul><li><a href="https://github.com/ultralytics/ultralytics">Ultralytics YOLO</a></li>
<li><a href="https://github.com/facebookresearch/segment-anything">Segment Anything (SAM)</a></li>
<li><a href="https://github.com/open-mmlab/mmdetection3d">MMDetection3D</a></li>
<li><a href="https://github.com/NVlabs/FoundationPose">FoundationPose</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第19章 点云与 SLAM</h2>

<blockquote><strong>面试场景</strong>: “请比较 Visual SLAM 与 LiDAR SLAM 的区别；点云特征网络有哪些？实际工程如何选择？”</blockquote>

<hr></p>

<h3>🌐 感知任务视角</h3>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                         点云 &amp; SLAM 任务矩阵                                │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│                     输入: 3D 点云 (LiDAR / 深度相机 / SFM)                │
│                                                                          │
│   ┌───────────────┬────────────────────┬─────────────────────────────┐   │
│   │ 任务类型      │ 目标               │ 常用方法                     │   │
│   ├───────────────┼────────────────────┼─────────────────────────────┤   │
│   │ 几何理解      │ 去噪、配准、重建    │ ICP, NDT, Poisson, NeRF      │   │
│   │ 语义理解      │ 分类、检测、分割    │ PointNet++, KPConv, Minkowski│   │
│   │ 动态理解      │ 轨迹、场景流        │ FlowNet3D, PV-RCNN           │   │
│   │ 自定位 (SLAM) │ 位姿、地图          │ LOAM, LIO-SAM, ORB-SLAM3     │   │
│   └───────────────┴────────────────────┴─────────────────────────────┘   │
│                                                                          │
│   输出: 语义地图、占据栅格、关键帧图、稀疏/稠密点云                      │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>1. 点云处理基础</h3>

<h4>1.1 常用表示</h4>

<table>
<thead><tr>
<th>表示方式</th>
<th>描述</th>
<th>优势</th>
<th>劣势</th>
<th>典型网络</th>
</tr></thead>
<tbody>
<tr>
<td>原始点云 (XYZRGB)</td>
<td>无结构集合</td>
<td>完整保留几何</td>
<td>不规则、不能直接用 CNN</td>
<td>PointNet</td>
</tr>
<tr>
<td>体素 (Voxel)</td>
<td>划分为 3D 网格</td>
<td>结构化，可用 3D CNN</td>
<td>高维，稀疏浪费计算</td>
<td>VoxelNet</td>
</tr>
<tr>
<td>点柱 (Pillar)</td>
<td>沿 z 轴积分为柱</td>
<td>兼顾稀疏性和结构性</td>
<td>z 信息损失</td>
<td>PointPillars</td>
</tr>
<tr>
<td>切片 (Range Image)</td>
<td>极坐标投影</td>
<td>适合 LiDAR</td>
<td>失真</td>
<td>RangeNet++</td>
</tr>
<tr>
<td>BEV</td>
<td>鸟瞰投影</td>
<td>规划友好</td>
<td>精度受高度影响</td>
<td>BEVFusion</td>
</tr>
</tbody></table>

<h4>1.2 特征提取网络</h4>

<table>
<thead><tr>
<th>类型</th>
<th>代表网络</th>
<th>核心思想</th>
<th>适用</th>
</tr></thead>
<tbody>
<tr>
<td>MLP 全局</td>
<td>PointNet</td>
<td>对每个点独立 + 全局 Pooling</td>
<td>分类/粗分割</td>
</tr>
<tr>
<td>局部聚合</td>
<td>PointNet++</td>
<td>局部区域采样 + 集合</td>
<td>更细粒度</td>
</tr>
<tr>
<td>卷积核</td>
<td>KPConv</td>
<td>可变形 Point Kernel</td>
<td>密集点云</td>
</tr>
<tr>
<td>稀疏卷积</td>
<td>MinkowskiNet</td>
<td>Sparse Convolution</td>
<td>大规模点云</td>
</tr>
<tr>
<td>Transformer</td>
<td>Point-BERT, Point Transformer</td>
<td>自注意力</td>
<td>通用</td>
</tr>
</tbody></table>

<hr></p>

<h3>2. 点云语义理解</h3>

<h4>2.1 检测</h4>

<table>
<thead><tr>
<th>算法</th>
<th>输入</th>
<th>特点</th>
</tr></thead>
<tbody>
<tr>
<td>PointRCNN</td>
<td>点云</td>
<td>Two-stage, PointNet++ backbone</td>
</tr>
<tr>
<td>PV-RCNN</td>
<td>混合 (Voxel + Point)</td>
<td>先体素提特征，再回到原始点</td>
</tr>
<tr>
<td>SECOND</td>
<td>体素</td>
<td>SparseConv, 快速</td>
</tr>
<tr>
<td>CenterPoint</td>
<td>BEV</td>
<td>Anchor-free，检测中心点</td>
</tr>
</tbody></table>

<h4>2.2 分割</h4>

<ul><li><strong>RangeNet++</strong>: 将 LiDAR 投影到 range image，使用 2D CNN。</li>
<li><strong>MinkowskiNet</strong>: 稀疏卷积，多任务 (语义 + 实例)。</li>
<li><strong>PolarNet</strong>: 在极坐标中分割，兼顾速度与精度。</li>
</ul>
<h4>2.3 场景流 / 动态理解</h4>

<ul><li>FlowNet3D, HPLFlowNet: 学习帧间点云的速度场。</li>
<li>BEVFlow: 在 BEV 中估计场景流，适合自动驾驶。</li>
</ul>
<hr></p>

<h3>3. 点云配准 (Registration)</h3>

<h4>3.1 经典算法</h4>

<table>
<thead><tr>
<th>算法</th>
<th>思路</th>
<th>优点</th>
<th>缺点</th>
</tr></thead>
<tbody>
<tr>
<td>ICP</td>
<td>最近邻点对齐 + 最小二乘</td>
<td>简单</td>
<td>容易陷入局部，需良好初始化</td>
</tr>
<tr>
<td>G-ICP</td>
<td>基于高斯分布的 ICP</td>
<td>精度更好</td>
<td>计算量稍高</td>
</tr>
<tr>
<td>NDT</td>
<td>将点云建模为高斯体素</td>
<td>收敛范围大</td>
<td>需要调分辨率</td>
</tr>
<tr>
<td>TEASER++</td>
<td>鲁棒估计</td>
<td>可抗离群点</td>
<td>计算开销大</td>
</tr>
</tbody></table>

<h4>3.2 学习型配准</h4>

<ul><li>DCP / Deep Closest Point</li>
<li>Predator</li>
<li>FCGF (Fully Convolutional Geometric Features)</li>
</ul>
<hr></p>

<h3>4. SLAM 技术谱系</h3>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                         SLAM 分类                                          │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   视觉 SLAM (Mono / Stereo / RGB-D)                                       │
│   ───────────────                                                         │
│   • ORB-SLAM2/3 (特征)                                                    │
│   • DSO / LSD-SLAM (直接法)                                               │
│   • VINS-Mono / OKVIS (VIO)                                              │
│                                                                          │
│   LiDAR SLAM                                                             │
│   ──────────                                                             │
│   • LOAM, LeGO-LOAM                                                      │
│   • Cartographer                                                        │
│   • LIO-SAM (LiDAR-Inertial)                                            │
│                                                                          │
│   多传感器 SLAM                                                          │
│   ───────────                                                           │
│   • VIO + GPS (VINS-Fusion)                                              │
│   • Multi-camera + IMU (ORB-SLAM3)                                       │
│   • Tightly-Coupled LiDAR-IMU-Camera                                    │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>4.1 视觉 SLAM 流程</h4>

<pre><code class="">图像 → 特征提取 (ORB/SIFT) → 匹配 → 位姿估计 (PnP + RANSAC) →
滑动窗口优化 / BA → 回环检测 (DBoW2) → 图优化 (g2o)
</code></pre>

<table>
<thead><tr>
<th>模块</th>
<th>关键技术</th>
</tr></thead>
<tbody>
<tr>
<td>前端</td>
<td>FAST/ORB、光流跟踪、金字塔 LK</td>
</tr>
<tr>
<td>后端</td>
<td>Bundle Adjustment, Pose Graph</td>
</tr>
<tr>
<td>回环</td>
<td>Bag-of-Words, Place Recognition</td>
</tr>
<tr>
<td>地图</td>
<td>稀疏路标 (Landmarks)</td>
</tr>
</tbody></table>

<h4>4.2 LiDAR SLAM</h4>

<ul><li><strong>LOAM</strong>: 分离扫描匹配和运动补偿，特征点 (Edge/Plane)。</li>
<li><strong>LeGO-LOAM</strong>: 针对地面车辆，分割地面与障碍。</li>
<li><strong>LIO-SAM</strong>:</li>
</ul>  - 前端：LiDAR 特征 + IMU 预积分
  - 后端：因子图 (GTSAM)
  - 回环检测 + Pose Graph</p>

<h4>4.3 多传感器 (VIO / LIO)</h4>

<table>
<thead><tr>
<th>系统</th>
<th>传感器</th>
<th>特点</th>
</tr></thead>
<tbody>
<tr>
<td>VINS-Mono</td>
<td>单目 + IMU</td>
<td>滑动窗口 BA，实时</td>
</tr>
<tr>
<td>ORB-SLAM3</td>
<td>Mono/Stereo/RGBD + IMU</td>
<td>关键帧图优化</td>
</tr>
<tr>
<td>LIO-SAM</td>
<td>LiDAR + IMU + (可选) GPS</td>
<td>高精度，开源</td>
</tr>
<tr>
<td>Cartographer</td>
<td>LiDAR + IMU + Wheel</td>
<td>Google 开源</td>
</tr>
</tbody></table>

<hr></p>

<h3>5. 多模态融合策略</h3>

<table>
<thead><tr>
<th>融合方式</th>
<th>描述</th>
<th>代表系统</th>
</tr></thead>
<tbody>
<tr>
<td>松耦合</td>
<td>先分别估计，再通过 EKF 融合</td>
<td>robot_localization</td>
</tr>
<tr>
<td>紧耦合</td>
<td>在同一优化框架中联合估计</td>
<td>VINS-Fusion, LIO-SAM</td>
</tr>
<tr>
<td>Graph-SLAM</td>
<td>因子图表示所有约束</td>
<td>GTSAM 系列</td>
</tr>
</tbody></table>

<p><strong>选择建议</strong>:
<ul><li>实时性优先 → 松耦合 EKF</li>
<li>高精度 → 紧耦合 + 因子图</li>
<li>多传感器冗余 → Graph-SLAM</li>
</ul>
<hr></p>

<h3>6. 工程落地 Checklist</h3>

<ul><li>[ ] 时间同步：硬件触发 / PTP / 时戳对齐</li>
<li>[ ] 传感器标定：外参 (Hand-eye)，内参 (LiDAR-to-Camera)</li>
<li>[ ] 地图管理：关键帧稀疏化、循环检测</li>
<li>[ ] 动态物体处理：语义分割剔除行人/车辆</li>
<li>[ ] 异常检测：监控轨迹残差、速度跳变</li>
<li>[ ] 回环策略：近似最近邻 / 基于学习的场景识别</li>
</ul>
<hr></p>

<h3>7. 代码片段</h3>

<h4>7.1 Open3D ICP</h4>

<pre><code class="python">import open3d as o3d</p>

<p>source = o3d.io.read_point_cloud("scan1.pcd")
target = o3d.io.read_point_cloud("scan2.pcd")</p>

<p>threshold = 0.05
trans_init = np.eye(4)</p>

<p>reg_p2p = o3d.pipelines.registration.registration_icp(
    source, target, threshold, trans_init,
    o3d.pipelines.registration.TransformationEstimationPointToPlane()
)
print(reg_p2p.transformation)
</code></pre>

<h4>7.2 LIO-SAM Launch (ROS)</h4>

<pre><code class="xml">&lt;launch&gt;
  &lt;node pkg="lio_sam" type="lio_sam_imuPreintegration" name="lio_sam"&gt;
    &lt;param name="sensor" value="velodyne" /&gt;
    &lt;param name="imu_topic" value="/imu/data" /&gt;
    &lt;param name="pointCloudTopic" value="/velodyne_points" /&gt;
    &lt;param name="gpsTopic" value="/gps/fix" /&gt;
  &lt;/node&gt;
&lt;/launch&gt;
</code></pre>

<hr></p>

<h3>8. 面试 Q&A</h3>

<h4>Q1: Visual SLAM vs LiDAR SLAM？</h4>

<ul><li>视觉：信息丰富、成本低，但易受光照/纹理影响。</li>
<li>LiDAR：几何精确、鲁棒，但成本高、分辨率有限。</li>
<li>融合：使用 LiDAR 提供全局几何，视觉提供语义和精细结构。</li>
</ul>
<h4>Q2: 如何处理点云中的动态物体？</h4>

<p><li>语义分割剔除动态类别 (车/人)。</li>
<li>基于 RANSAC/运动一致性检测异常速度。</li>
<li>使用多传感器 (IMU) 区分静态 vs 动态。</li></p>

<h4>Q3: 回环检测的关键步骤？</h4>

<ul><li>选择候选关键帧（时间/空间最近）。</li>
<li>构建描述子 (BoW / Scan Context)。</li>
<li>匹配验证 (几何对齐)。</li>
<li>添加回环约束，优化 Pose Graph。</li>
</ul>
<h4>Q4: ICP 何时会失败？如何改善？</h4>

<ul><li>初始估计差 → 使用全局配准 / NDT 先粗对齐。</li>
<li>动态物体多 → 预处理剔除异常点。</li>
<li>噪声大 → 采用点到平面或鲁棒核函数。</li>
</ul>
<hr></p>

<h3>📚 推荐资源</h3>

<ul><li><em>3D Point Cloud Processing</em> — T.-Y. Lin</li>
<li><em>SLAM for Dummies</em> (Online)</li>
<li>Open3D, PCL, Cupoch (GPU) — 点云处理库</li>
<li>LIO-SAM, VINS-Fusion, ORB-SLAM3 — 开源实现</li>
<li>Scan Context, OverlapNet — 回环检测</li>
</ul>
<hr></p>

<hr></p>

<h2>第20章 状态估计</h2>

<blockquote><strong>面试场景</strong>: “如何利用 IMU + 相机做状态估计？Kalman Filter 与 Particle Filter 有哪些区别？”</blockquote>

<p>本章整理 Kalman/EKF/UKF、Particle Filter 以及多传感器融合的工程实践，帮助在面试中自信回答状态估计相关问题。</p>

<hr></p>

<h3>🧭 状态估计任务分层</h3>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                         状态估计层次                                        │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   低层 (IMU 惯导)                     中层 (组合导航)                    │
│   ───────────────                     ───────────────                    │
│   • 姿态/角速度估计                   • VIO/VINS (IMU + Camera)          │
│   • 零偏校正                          • Wheel Odometry + IMU             │
│                                                                          │
│   高层 (全局定位)                     语义层 (高阶)                      │
│   ───────────────                     ───────────────                    │
│   • SLAM (EKF-SLAM, Graph-SLAM)       • 对象级跟踪                       │
│   • GNSS-RTK + 惯导组合               • 语义状态估计                     │
│                                                                          │
│   核心构件:                                                                │
│   - Motion Model (系统模型)                                               │
│   - Sensor Model (观测模型)                                               │
│   - Filter / Optimizer                                                    │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>1. Kalman Filter 家族</h3>

<h4>1.1 线性 KF</h4>

<p>系统模型 (离散):
\[
x_k = A x_{k-1} + B u_k + w_k,\quad w_k \sim \mathcal{N}(0, Q)
\]
观测模型:
\[
z_k = H x_k + v_k,\quad v_k \sim \mathcal{N}(0, R)
\]</p>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                         Kalman Filter 循环                                  │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   预测 (Predict)                                                         │
│   ────────────────────────────────────────────────────────────────────   │
│   x̂⁻ₖ = A x̂ₖ₋₁ + B uₖ                                                   │
│   P⁻ₖ = A Pₖ₋₁ Aᵀ + Q                                                    │
│                                                                          │
│   更新 (Update)                                                          │
│   ────────────────────────────────────────────────────────────────────   │
│   Kₖ = P⁻ₖ Hᵀ (H P⁻ₖ Hᵀ + R)⁻¹                                           │
│   x̂ₖ = x̂⁻ₖ + Kₖ (zₖ - H x̂⁻ₖ)                                           │
│   Pₖ = (I - Kₖ H) P⁻ₖ                                                    │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>1.2 扩展 Kalman Filter (EKF)</h4>

<ul><li>适用于非线性系统：\( x_k = f(x_{k-1}, u_k) + w_k \), \( z_k = h(x_k) + v_k \)</li>
<li>用雅可比矩阵线性化：</li>
</ul>  - \( F_k = \frac{\partial f}{\partial x} \big|_{x=\hat{x}_{k-1}} \)
  - \( H_k = \frac{\partial h}{\partial x} \big|_{x=\hat{x}_{k}^{-}} \)
<ul><li>缺点：线性化误差大时会发散，需要保持小时间步或使用重线性化。</li>
</ul>
<h4>1.3 无迹 Kalman Filter (UKF)</h4>

<ul><li>不再线性化，而是用 <strong>Sigma Points</strong> 捕获均值/协方差传播。</li>
<li>步骤：</li>
</ul>  1. 选择 \( 2n + 1 \) 个 sigma 点 \( \chi_i \)
  2. 通过非线性函数传播 \( \chi_i' = f(\chi_i) \)
  3. 加权重构均值 \(\hat{x}\) 与协方差 \(P\)
<ul><li>优点：对高度非线性的系统更稳健，不需要求雅可比。</li>
<li>缺点：计算量稍高，参数 (α, β, κ) 需调节。</li>
</ul>
<h4>1.4 选择指南</h4>

<table>
<thead><tr>
<th>场景</th>
<th>推荐滤波器</th>
<th>备注</th>
</tr></thead>
<tbody>
<tr>
<td>线性系统 + 高频更新</td>
<td>KF</td>
<td>如电机位置闭环</td>
</tr>
<tr>
<td>轻微非线性 + 高速运行</td>
<td>EKF</td>
<td>航姿系统、VIO</td>
</tr>
<tr>
<td>强非线性 + 噪声复杂</td>
<td>UKF</td>
<td>视觉角度 + IMU</td>
</tr>
</tbody></table>

<hr></p>

<h3>2. 粒子滤波 (Particle Filter)</h3>

<h4>2.1 原理</h4>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                     Particle Filter 流程                                   │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│ 1. 采样: 从上一时刻粒子集合根据运动模型采样新的粒子                      │
│ 2. 加权: 根据观测模型计算粒子权重 (似然)                                 │
│ 3. 归一化: 权重归一化                                                     │
│ 4. 重采样: 根据权重重新采样，避免退化                                    │
│ 5. 状态估计: 求加权平均或选最大权重粒子                                 │
│                                                                          │
│   常用技巧:                                                               │
│   - 低方差重采样 (Low-variance resampling)                               │
│   - 自适应粒子数 (KLD-Sampling)                                          │
│   - Log-Likelihood 累积避免下溢                                          │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2.2 KF vs Particle Filter</h4>

<table>
<thead><tr>
<th>对比项</th>
<th>Kalman 系列</th>
<th>Particle Filter</th>
</tr></thead>
<tbody>
<tr>
<td>状态分布假设</td>
<td>高斯</td>
<td>任意分布</td>
</tr>
<tr>
<td>维度扩展</td>
<td>易受维度诅咒影响较小</td>
<td>粒子数随维度指数增加</td>
</tr>
<tr>
<td>计算量</td>
<td>低 (矩阵运算)</td>
<td>高 (大量粒子)</td>
</tr>
<tr>
<td>收敛速度</td>
<td>快</td>
<td>慢 (依赖粒子权重)</td>
</tr>
<tr>
<td>适用场景</td>
<td>机器人姿态、VIO</td>
<td>全局定位、非高斯噪声</td>
</tr>
</tbody></table>

<hr></p>

<h3>3. 传感器融合模式</h3>

<h4>3.1 常见组合</h4>

<table>
<thead><tr>
<th>组合</th>
<th>说明</th>
<th>典型系统</th>
</tr></thead>
<tbody>
<tr>
<td>IMU + Encoder</td>
<td>惯性测量 + 轮速，补偿滑移</td>
<td>地面移动机器人</td>
</tr>
<tr>
<td>IMU + Camera</td>
<td>Visual-Inertial Odometry (VIO)</td>
<td>VINS-Mono, OKVIS</td>
</tr>
<tr>
<td>IMU + LiDAR</td>
<td>LiDAR-Inertial Odometry</td>
<td>LIO-SAM</td>
</tr>
<tr>
<td>Wheel + GNSS + IMU</td>
<td>车辆定位</td>
<td>自动驾驶</td>
</tr>
<tr>
<td>Vision + Tactile</td>
<td>末端执行器状态估计</td>
<td>触觉闭环控制</td>
</tr>
</tbody></table>

<h4>3.2 融合架构</h4>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                     典型 VIO (EKF-based)                                   │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   IMU 高频积分 → 状态预测                                                 │
│          │                                                               │
│          ▼                                                               │
│   EKF 预测步骤 (Propagate)                                               │
│          │                                                               │
│   相机关键帧提取 → 特征匹配                                              │
│          │                                                               │
│          ▼                                                               │
│   EKF 更新步骤 (Update)                                                   │
│          │                                                               │
│          ▼                                                               │
│   输出: 位姿、速度、偏置                                                 │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>3.3 Graph-based 融合</h4>

<ul><li>使用因子图 (Factor Graph) 或 Bundle Adjustment 表述状态估计。</li>
<li>节点: 机器人状态 (姿态、速度、偏置)。</li>
<li>因子: IMU 预积分、视觉重投影误差、里程计约束等。</li>
<li>优点：可离线批处理 / 滑动窗口，精度高；缺点：实时性需优化。</li>
</ul>
<hr></p>

<h3>4. 代码实现片段</h3>

<h4>4.1 PyTorch EKF 结构</h4>

<pre><code class="python">import torch</p>

<p>class ExtendedKalmanFilter:
    def __init__(self, state_dim, meas_dim):
        self.state_dim = state_dim
        self.meas_dim = meas_dim
        self.x = torch.zeros(state_dim, 1)
        self.P = torch.eye(state_dim)
        self.Q = torch.eye(state_dim) * 1e-3
        self.R = torch.eye(meas_dim) * 1e-2</p>

<p>    def predict(self, f, F, u=torch.zeros(0)):
        # f: 状态转移函数, F: 雅可比
        self.x = f(self.x, u)
        self.P = F(self.x) @ self.P @ F(self.x).T + self.Q</p>

<p>    def update(self, z, h, H):
        y = z - h(self.x)
        S = H(self.x) @ self.P @ H(self.x).T + self.R
        K = self.P @ H(self.x).T @ torch.linalg.inv(S)
        self.x = self.x + K @ y
        self.P = (torch.eye(self.state_dim) - K @ H(self.x)) @ self.P
</code></pre>

<h4>4.2 粒子滤波 Python 原型</h4>

<pre><code class="python">import numpy as np</p>

<p>class ParticleFilter:
    def __init__(self, num_particles, motion_model, sensor_model):
        self.n = num_particles
        self.motion_model = motion_model
        self.sensor_model = sensor_model
        self.particles = np.zeros((num_particles, 3))  # x, y, theta
        self.weights = np.ones(num_particles) / num_particles</p>

<p>    def predict(self, u, dt):
        noise = np.random.normal(0, [0.01, 0.01, 0.005], size=self.particles.shape)
        self.particles = self.motion_model(self.particles, u, dt) + noise</p>

<p>    def update(self, z):
        self.weights *= self.sensor_model.likelihood(self.particles, z)
        self.weights += 1e-300  # 避免全 0
        self.weights /= np.sum(self.weights)</p>

<p>    def resample(self):
        cumulative = np.cumsum(self.weights)
        cumulative[-1] = 1.0
        indexes = np.searchsorted(cumulative, np.random.rand(self.n))
        self.particles = self.particles[indexes]
        self.weights.fill(1.0 / self.n)</p>

<p>    def estimate(self):
        return np.average(self.particles, weights=self.weights, axis=0)
</code></pre>

<hr></p>

<h3>5. 工程实践</h3>

<h4>5.1 预积分 (IMU Pre-integration)</h4>

<ul><li>Avoid integrating IMU between every pair of frames.</li>
<li>在 VIO 中预计算 IMU 约束，使优化只跟状态增量相关。</li>
<li>使用 <code>gtsam::PreintegratedImuMeasurements</code> 等工具。</li>
</ul>
<h4>5.2 零偏估计 (Bias Estimation)</h4>

<ul><li>IMU 零偏随时间漂移，必须纳入状态向量：</li>
</ul>  \[
  x = \begin{bmatrix} p & v & q & b_a & b_g \end{bmatrix}
  \]
<ul><li>噪声模型中添加随机游走：</li>
</ul>  \[
  b_{k} = b_{k-1} + w_b
  \]</p>

<h4>5.3 观测异常检测</h4>

<ul><li><strong>Mahalanobis Distance</strong>: 判断测量是否异常：</li>
</ul>  \[
  \nu = z - h(\hat{x});\quad d^2 = \nu^\top S^{-1} \nu
  \]
  若 \( d^2 > \chi^2_{n,\alpha} \)，则拒绝该观测。
<ul><li><strong>Innovation Gating</strong>: 只在创新位于阈值内时更新。</li>
</ul>
<h4>5.4 ROS2 / robot_localization</h4>

<ul><li><code>ekf_node</code>: 融合 IMU + Wheel + GPS。</li>
<li><code>ukf_node</code>: 支持 UKF。</li>
<li>需要正确配置 <code>imu0_config</code>, <code>odom0_config</code>, <code>two_d_mode</code> 等参数。</li>
</ul>
<hr></p>

<h3>6. 面试 Q&A</h3>

<h4>Q1: EKF 为什么可能会发散？如何缓解？</h4>

<ul><li>线性化误差大：使用小时间步、重复线性化或改用 UKF。</li>
<li>噪声协方差设定不合理：增大 Q/R 使滤波更保守。</li>
<li>初始协方差过小：导致信任预测过度，建议放大 \(P_0\)。</li>
</ul>
<h4>Q2: Particle Filter 如何选择粒子数？</h4>

<ul><li>经验：每维 50~100 个粒子。</li>
<li>可用 <strong>自适应粒子数</strong>：当 ESS (effective sample size) 高于阈值时减少粒子，低于时增加。</li>
</ul>
<h4>Q3: IMU + 相机融合的关键难点？</h4>

<p><li>时间同步 (时间戳对齐，PTP/触发)。</li>
<li>IMU 到相机的外参标定 (手眼标定)。</li>
<li>IMU 噪声模型 (加速度/陀螺零偏、随机游走)。</li>
<li>滑动窗口优化的准实时实现 (Ceres / GTSAM)。</li></p>

<h4>Q4: 如何检测传感器失效？</h4>

<ul><li>监控创新 (Innovation) 大小。</li>
<li>监控输出方差是否异常增大。</li>
<li>多传感器互相验证 (Cross-check)：例如视觉失败时降级为轮速 + IMU。</li>
</ul>
<hr></p>

<h3>📚 推荐资源</h3>

<ul><li><em>Probabilistic Robotics</em> — Thrun et al.</li>
<li><em>State Estimation for Robotics</em> — Timothy D. Barfoot</li>
<li>VINS-Mono: <a href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono">github.com/HKUST-Aerial-Robotics/VINS-Mono</a></li>
<li>robot_localization: <a href="http://docs.ros.org/en/noetic/api/robot_localization/html/index.html">docs.ros.org/en/noetic/api/robot_localization</a></li>
<li>GTSAM: <a href="https://gtsam.org/">gtsam.org</a></li>
</ul>
<hr></p>

<hr></p>

<h1>第五部分：抓取与运动规划</h1>

<hr></p>

<h2>第21章 抓取算法</h2>

<blockquote><strong>面试场景</strong>: "介绍一下你用过的抓取算法库，DexGraspNet 和 Contact-GraspNet 有什么区别？"</blockquote>

<p>本文涵盖主流抓取算法库（DexGraspNet、GraspGF、Contact-GraspNet 等）以及 Isaac Sim/Gym、SAPIEN 仿真平台的深度使用指南。</p>

<hr></p>

<h3>📊 抓取算法全景图</h3>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     抓取算法分类                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   输入模态              算法类型              代表方法                       │
│   ─────────────────────────────────────────────────────────────────────     │
│                                                                             │
│   RGB-D / 点云          分析式 (Analytic)     GraspIt!, OpenRAVE            │
│      │                                                                      │
│      │                  采样+评分             GPD, PointNetGPD              │
│      │                  (Sample &amp; Score)     Contact-GraspNet              │
│      │                                       AnyGrasp                       │
│      │                                                                      │
│      │                  生成式               GraspGF (Flow)                 │
│      │                  (Generative)         SE(3)-DiffusionFields         │
│      │                                                                      │
│      │                  灵巧手专用           DexGraspNet                    │
│      │                                       UniDexGrasp                    │
│      │                                       DexDiffuser                    │
│      │                                                                      │
│      └──► 端到端 VLA    感知→动作            RT-1/2, OpenVLA, π0            │
│                                                                             │
│   💡 趋势: 从采样评分 → 生成式 → 端到端 VLA                                 │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>1. 平行夹爪抓取算法 (Parallel-Jaw Grasp)</h3>

<h4>1.1 算法对比</h4>

<table>
<thead><tr>
<th>算法</th>
<th>输入</th>
<th>方法</th>
<th>特点</th>
<th>代码</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Dex-Net</strong></td>
<td>深度图</td>
<td>GQ-CNN 评分</td>
<td>大规模合成数据</td>
<td>github/BerkeleyAutomation</td>
</tr>
<tr>
<td><strong>GPD</strong></td>
<td>点云</td>
<td>采样 + CNN 评分</td>
<td>经典基线</td>
<td>github/atenpas/gpd</td>
</tr>
<tr>
<td><strong>PointNetGPD</strong></td>
<td>点云</td>
<td>PointNet 特征 + 评分</td>
<td>端到端</td>
<td>github/lianghongzhuo</td>
</tr>
<tr>
<td><strong>Contact-GraspNet</strong></td>
<td>点云</td>
<td>6-DoF 直接回归</td>
<td>快速、单次前向</td>
<td>github/NVlabs</td>
</tr>
<tr>
<td><strong>AnyGrasp</strong></td>
<td>点云</td>
<td>大规模预训练</td>
<td>泛化强</td>
<td>github/graspnet</td>
</tr>
<tr>
<td><strong>GraspGF</strong></td>
<td>点云</td>
<td>Score-based 生成</td>
<td>多样性好</td>
<td>github/graspgf</td>
</tr>
<tr>
<td><strong>VGN</strong></td>
<td>TSDF</td>
<td>3D CNN</td>
<td>体素表示</td>
<td>github/ethz-asl</td>
</tr>
</tbody></table>

<h4>1.2 Dex-Net 系列</h4>

<blockquote><strong>Dex-Net</strong> (Dexterity Network) 是 UC Berkeley AUTOLAB 开发的数据驱动抓取系统，通过大规模合成数据训练抓取质量预测网络。</blockquote>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     Dex-Net 系统架构                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   数据生成 (Offline):                                                       │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   3D 物体模型 (ShapeNet, YCB, 自定义)                               │   │
│   │        │                                                            │   │
│   │        ▼                                                            │   │
│   │   抓取采样 (Antipodal Sampling)                                     │   │
│   │        │                                                            │   │
│   │        ▼                                                            │   │
│   │   力闭合分析 (Ferrari-Canny Metric)                                 │   │
│   │        │                                                            │   │
│   │        ▼                                                            │   │
│   │   渲染深度图 + 标注抓取质量                                         │   │
│   │        │                                                            │   │
│   │        ▼                                                            │   │
│   │   Dex-Net 数据集 (数百万样本)                                       │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   推理 (Online):                                                            │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   深度图 ──► 抓取采样 ──► GQ-CNN ──► 抓取质量评分 ──► 最佳抓取      │   │
│   │                                                                     │   │
│   │   GQ-CNN (Grasp Quality CNN):                                       │   │
│   │   • 输入: 深度图裁剪 (32x32) + 抓取参数 (角度, 深度)               │   │
│   │   • 输出: 抓取成功概率 P(success | grasp, depth)                   │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<p><strong>Dex-Net 系列版本</strong>:</p>

<table>
<thead><tr>
<th>版本</th>
<th>发布</th>
<th>特点</th>
<th>数据规模</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Dex-Net 1.0</strong></td>
<td>2017</td>
<td>首个大规模合成数据集</td>
<td>10K 物体, 2.5M 抓取</td>
</tr>
<tr>
<td><strong>Dex-Net 2.0</strong></td>
<td>2017</td>
<td>GQ-CNN, 平行夹爪</td>
<td>6.7M 点云-抓取对</td>
</tr>
<tr>
<td><strong>Dex-Net 3.0</strong></td>
<td>2018</td>
<td>吸盘抓取</td>
<td>2.8M 吸盘抓取</td>
</tr>
<tr>
<td><strong>Dex-Net 4.0</strong></td>
<td>2019</td>
<td>多模态 (夹爪+吸盘)</td>
<td>混合策略</td>
</tr>
</tbody></table>

<p><strong>GQ-CNN 架构</strong>:</p>

<pre><code class="python">import torch
import torch.nn as nn</p>

<p>class GQCNN(nn.Module):
    """Grasp Quality CNN"""
    def __init__(self):
        super().__init__()
        # 图像分支 (深度图)
        self.conv1 = nn.Conv2d(1, 64, 7, stride=2)
        self.conv2 = nn.Conv2d(64, 64, 5, stride=2)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=2)
        
        # 抓取参数分支
        self.grasp_fc = nn.Linear(1, 16)  # 抓取深度
        
        # 融合层
        self.fc1 = nn.Linear(64 <em> 2 </em> 2 + 16, 1024)
        self.fc2 = nn.Linear(1024, 1024)
        self.fc3 = nn.Linear(1024, 2)  # 成功/失败
        
    def forward(self, depth_crop, grasp_depth):
        """
        Args:
            depth_crop: (B, 1, 32, 32) 深度图裁剪
            grasp_depth: (B, 1) 抓取深度
        """
        # 图像特征
        x = F.relu(self.conv1(depth_crop))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        
        # 抓取参数特征
        g = F.relu(self.grasp_fc(grasp_depth))
        
        # 融合
        x = torch.cat([x, g], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        
        return F.softmax(x, dim=1)</p>

<p>def predict_grasp_quality(model, depth_image, grasp_candidates):
    """评估候选抓取的质量"""
    scores = []
    for grasp in grasp_candidates:
        # 裁剪深度图 (以抓取点为中心, 旋转对齐)
        crop = crop_and_rotate(depth_image, grasp.center, grasp.angle, size=32)
        crop = torch.from_numpy(crop).unsqueeze(0).unsqueeze(0).float()
        depth = torch.tensor([[grasp.depth]]).float()
        
        with torch.no_grad():
            prob = model(crop, depth)
            scores.append(prob[0, 1].item())  # P(success)
    
    return scores
</code></pre>

<p><strong>Dex-Net 完整使用流程</strong>:</p>

<pre><code class="python">from autolab_core import RigidTransform, DepthImage
from gqcnn import GQCNN, GraspPlanner</p>

<p>model = GQCNN.load('models/gqcnn_pj.model')
planner = GraspPlanner(model)</p>

<p>depth_image = DepthImage.from_sensor(camera)</p>

<p>grasps = planner.plan(
    depth_image,
    camera_intrinsics,
    num_samples=1000,      # 采样数量
    num_candidates=100,     # 候选数量
    top_k=5,               # 返回 top-k
)</p>

<p>best_grasp = grasps[0]
print(f"抓取位置: {best_grasp.pose.position}")
print(f"抓取角度: {best_grasp.angle}")
print(f"成功概率: {best_grasp.quality:.3f}")</p>

<p>grasp_in_robot = camera_to_robot_transform * best_grasp.pose
</code></pre>

<p><strong>Dex-Net vs 其他方法对比</strong>:</p>

<table>
<thead><tr>
<th>维度</th>
<th>Dex-Net</th>
<th>Contact-GraspNet</th>
<th>GraspGF</th>
</tr></thead>
<tbody>
<tr>
<td><strong>输入</strong></td>
<td>深度图 (单视角)</td>
<td>点云</td>
<td>点云</td>
</tr>
<tr>
<td><strong>方法</strong></td>
<td>采样 + GQ-CNN 评分</td>
<td>直接回归</td>
<td>生成式</td>
</tr>
<tr>
<td><strong>速度</strong></td>
<td>~100ms</td>
<td>~50ms</td>
<td>~1s</td>
</tr>
<tr>
<td><strong>泛化</strong></td>
<td>中等 (需 Domain Randomization)</td>
<td>强</td>
<td>强</td>
</tr>
<tr>
<td><strong>多样性</strong></td>
<td>取决于采样</td>
<td>固定输出数</td>
<td>可控</td>
</tr>
<tr>
<td><strong>开源</strong></td>
<td>✅ 完整</td>
<td>✅</td>
<td>✅</td>
</tr>
</tbody></table>

<h4>1.3 Contact-GraspNet 架构</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     Contact-GraspNet                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   输入: 场景点云 P ∈ R^(N×3)                                                │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     PointNet++ Backbone                              │   │
│   │                                                                     │   │
│   │   点云 → Set Abstraction → Feature Propagation → 逐点特征 F         │   │
│   └────────────────────────────────┬────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     Multi-Head Prediction                            │   │
│   │                                                                     │   │
│   │   每个点预测:                                                        │   │
│   │   • 是否为接触点 (contact score)                                    │   │
│   │   • 抓取方向 (approach direction)                                   │   │
│   │   • 抓取宽度 (gripper width)                                        │   │
│   │   • 抓取角度 (baseline rotation)                                    │   │
│   └────────────────────────────────┬────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     Grasp Sampling &amp; NMS                             │   │
│   │                                                                     │   │
│   │   选择高分接触点 → 组合成 6-DoF 抓取 → NMS 去重                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   输出: 多个候选抓取 {(R, t, w)} ∈ SE(3) × R                               │
│                                                                             │
│   优势: 单次前向推理 (~50ms), 无需迭代采样                                  │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>1.4 GraspGF (Grasp Generative Flow)</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     GraspGF: Score-Based 生成抓取                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   核心思想: 学习抓取分布的 score function ∇_x log p(x|scene)                │
│                                                                             │
│   训练:                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   正样本抓取 x_0 → 加噪 x_t → 网络预测 score → Denoising Score Match │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   推理:                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   随机初始化 x_T ──► Langevin Dynamics ──► 高质量抓取 x_0            │   │
│   │                                                                     │   │
│   │   x_{t-1} = x_t + ε <em> ∇_x log p(x_t|scene) + √(2ε) </em> z             │   │
│   │                     ↑                                               │   │
│   │              网络预测的 score                                        │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   优势:                                                                     │
│   • 多样性: 可生成多个不同的抓取                                           │
│   • 处理多模态: 同一物体有多个合理抓取方式                                 │
│   • 可组合: 可加入额外约束 (如避障、任务约束)                              │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>2. 灵巧手抓取算法 (Dexterous Grasp)</h3>

<h4>2.1 算法对比</h4>

<table>
<thead><tr>
<th>算法</th>
<th>手型</th>
<th>方法</th>
<th>特点</th>
</tr></thead>
<tbody>
<tr>
<td><strong>DexGraspNet</strong></td>
<td>Shadow Hand</td>
<td>大规模数据集 + 优化</td>
<td>1.32M 抓取标注</td>
</tr>
<tr>
<td><strong>UniDexGrasp</strong></td>
<td>多种手型</td>
<td>统一表示 + RL</td>
<td>跨手型泛化</td>
</tr>
<tr>
<td><strong>DexDiffuser</strong></td>
<td>Shadow Hand</td>
<td>Diffusion 生成</td>
<td>多样性好</td>
</tr>
<tr>
<td><strong>DexGraspNet 2.0</strong></td>
<td>多种手型</td>
<td>改进数据生成</td>
<td>更多物体</td>
</tr>
<tr>
<td><strong>GenDexGrasp</strong></td>
<td>通用</td>
<td>生成式</td>
<td>零样本泛化</td>
</tr>
</tbody></table>

<h4>2.2 DexGraspNet 数据集</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     DexGraspNet 数据集                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   规模:                                                                     │
│   • 5,355 个物体 (来自 ShapeNet, YCB, EGAD 等)                             │
│   • 1.32M 个抓取姿态标注                                                   │
│   • Shadow Hand 灵巧手                                                     │
│                                                                             │
│   数据生成流程:                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   物体网格                                                          │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   初始化手姿态 (多种初始化策略)                                      │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   优化目标:                                                          │   │
│   │   • 接触点距离最小化                                                │   │
│   │   • 穿透惩罚                                                        │   │
│   │   • 力闭合约束 (Force Closure)                                      │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   Isaac Gym 仿真验证                                                │   │
│   │      │                                                              │   │
│   │      ▼                                                              │   │
│   │   成功抓取标注                                                       │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   标注内容:                                                                 │
│   • 手腕 6-DoF 位姿                                                        │
│   • 22 个关节角度                                                          │
│   • 接触点位置                                                             │
│   • 抓取质量分数                                                           │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2.3 UniDexGrasp 统一框架</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     UniDexGrasp: 跨手型灵巧抓取                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   挑战: 不同灵巧手 DoF 不同 (Shadow 22, Allegro 16, LEAP 16)               │
│                                                                             │
│   解决方案: 统一的接触表示                                                  │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     架构                                             │   │
│   │                                                                     │   │
│   │   物体点云 ─────────────────────────────┐                           │   │
│   │                                         │                           │   │
│   │   手型参数 (URDF) ─► 手指尖采样点 ─────┤                           │   │
│   │                                         ▼                           │   │
│   │                                   ┌──────────┐                      │   │
│   │                                   │ Encoder  │                      │   │
│   │                                   │ (共享)   │                      │   │
│   │                                   └────┬─────┘                      │   │
│   │                                        │                            │   │
│   │                                        ▼                            │   │
│   │                                   ┌──────────┐                      │   │
│   │                                   │ 接触预测 │                      │   │
│   │                                   │ + IK 求解│                      │   │
│   │                                   └────┬─────┘                      │   │
│   │                                        │                            │   │
│   │                                        ▼                            │   │
│   │                              手关节角度 + 手腕位姿                   │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   训练: 多手型数据混合 + RL 微调                                           │
│   优势: 一个模型适配多种灵巧手                                             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>3. Isaac Sim/Gym 深度使用</h3>

<h4>3.1 Isaac 生态系统</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     NVIDIA Isaac 生态                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     Isaac Sim (Omniverse)                            │   │
│   │                                                                     │   │
│   │   • 完整仿真平台 (GUI)                                              │   │
│   │   • USD 场景格式                                                    │   │
│   │   • RTX 光追渲染                                                    │   │
│   │   • ROS/ROS2 桥接                                                   │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                              │                                              │
│                              ▼                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     Isaac Lab (原 Orbit)                             │   │
│   │                                                                     │   │
│   │   • 基于 Isaac Sim 的 RL 框架                                       │   │
│   │   • 模块化任务定义                                                  │   │
│   │   • 支持 RSL-RL, RL-Games, SKRL                                    │   │
│   │   • 标准化的 Gym 接口                                               │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                              │                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     Isaac Gym (Legacy)                               │   │
│   │                                                                     │   │
│   │   • 轻量级 GPU 并行仿真                                             │   │
│   │   • 无 GUI，纯 Python API                                           │   │
│   │   • 适合大规模 RL 训练                                              │   │
│   │   • ⚠️ 已停止更新，迁移到 Isaac Lab                                 │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   选择建议:                                                                 │
│   • 新项目: Isaac Lab (官方推荐)                                           │
│   • 老项目: Isaac Gym (仍可用)                                             │
│   • 需要 ROS: Isaac Sim + ROS Bridge                                       │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>3.2 Isaac Lab 任务定义</h4>

<pre><code class="python">import omni.isaac.lab as lab
from omni.isaac.lab.envs import ManagerBasedRLEnv, ManagerBasedRLEnvCfg</p>

<p>@configclass
class GraspEnvCfg(ManagerBasedRLEnvCfg):
    """抓取任务配置"""
    
    # 场景配置
    scene: SceneCfg = SceneCfg(num_envs=4096, env_spacing=2.0)
    
    # 观测配置
    observations: ObservationsCfg = ObservationsCfg(
        policy=ObsGroup(
            joint_pos=JointPosTerm(asset_cfg=robot_cfg),
            joint_vel=JointVelTerm(asset_cfg=robot_cfg),
            object_pos=ObjectPosTerm(asset_cfg=object_cfg),
        )
    )
    
    # 动作配置
    actions: ActionsCfg = ActionsCfg(
        joint_pos=JointPositionAction(asset_cfg=robot_cfg, scale=0.1)
    )
    
    # 奖励配置
    rewards: RewardsCfg = RewardsCfg(
        reaching=ReachingReward(weight=1.0),
        grasping=GraspSuccessReward(weight=10.0),
        action_penalty=ActionPenalty(weight=-0.01),
    )
    
    # 终止条件
    terminations: TerminationsCfg = TerminationsCfg(
        time_out=TimeOutTerm(time_out=5.0),
        success=GraspSuccessTerm(),
    )</p>

<p>class GraspEnv(ManagerBasedRLEnv):
    cfg: GraspEnvCfg
    
    def __init__(self, cfg: GraspEnvCfg):
        super().__init__(cfg)
        
    def _setup_scene(self):
        # 添加机器人
        self.robot = Articulation(self.cfg.robot_cfg)
        # 添加物体
        self.object = RigidObject(self.cfg.object_cfg)
        # 添加地面
        self.ground = GroundPlane()</p>

<p>from omni.isaac.lab_tasks.utils import parse_env_cfg
from rsl_rl.runners import OnPolicyRunner</p>

<p>env_cfg = parse_env_cfg("Isaac-Grasp-v0")
env = GraspEnv(cfg=env_cfg)
runner = OnPolicyRunner(env, train_cfg, log_dir="logs")
runner.learn(num_learning_iterations=1000)
</code></pre>

<h4>3.3 Isaac Gym 并行仿真</h4>

<pre><code class="python">from isaacgym import gymapi, gymtorch
import torch</p>

<p>gym = gymapi.acquire_gym()
sim_params = gymapi.SimParams()
sim_params.physx.num_threads = 4
sim_params.physx.solver_type = 1  # TGS
sim_params.use_gpu_pipeline = True</p>

<p>sim = gym.create_sim(0, 0, gymapi.SIM_PHYSX, sim_params)</p>

<p>num_envs = 4096
envs = []
for i in range(num_envs):
    env = gym.create_env(sim, lower, upper, num_per_row)
    # 加载机器人和物体
    robot = gym.create_actor(env, robot_asset, pose, "robot", i, 0)
    object = gym.create_actor(env, object_asset, obj_pose, "object", i, 1)
    envs.append(env)</p>

<p>gym.prepare_sim(sim)
root_tensor = gym.acquire_actor_root_state_tensor(sim)
root_states = gymtorch.wrap_tensor(root_tensor)</p>

<p>for iter in range(max_iters):
    # 并行 step
    gym.simulate(sim)
    gym.fetch_results(sim, True)
    
    # 获取观测 (GPU tensor)
    obs = get_observations(root_states)
    
    # 策略推理 (GPU)
    actions = policy(obs)
    
    # 设置动作 (GPU tensor)
    gym.set_dof_position_target_tensor(sim, gymtorch.unwrap_tensor(actions))
</code></pre>

<hr></p>

<h3>4. SAPIEN / ManiSkill 使用</h3>

<h4>4.1 SAPIEN 特点</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────────────────┐
│                     SAPIEN 核心特性                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Part-Level Articulation (关节物体交互)                                    │
│   ─────────────────────────────────────                                     │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │   微波炉                                                            │   │
│   │   ├── base (固定)                                                   │   │
│   │   ├── door (revolute joint) ← 可开关                               │   │
│   │   └── turntable (revolute joint) ← 可旋转                          │   │
│   │                                                                     │   │
│   │   抽屉柜                                                            │   │
│   │   ├── cabinet_body (固定)                                           │   │
│   │   ├── drawer_1 (prismatic joint) ← 可拉出                          │   │
│   │   ├── drawer_2 (prismatic joint)                                   │   │
│   │   └── door (revolute joint)                                        │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   PartNet-Mobility 数据集                                                   │
│   • 2,346 个可交互物体模型                                                 │
│   • 46 个物体类别                                                          │
│   • 完整的关节标注                                                         │
│                                                                             │
│   渲染能力                                                                  │
│   • Vulkan-based 光栅化                                                    │
│   • 可选光线追踪                                                           │
│   • 支持 Domain Randomization                                              │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>4.2 ManiSkill Benchmark</h4>

<pre><code class="python">import gymnasium as gym
import mani_skill.envs</p>

<p>env = gym.make(
    "PickCube-v1",
    num_envs=256,  # GPU 并行
    obs_mode="rgbd",  # 观测模式
    control_mode="pd_ee_delta_pose",  # 控制模式
    render_mode="rgb_array",
)</p>

<p>obs, info = env.reset()</p>

<p>for _ in range(1000):
    action = env.action_space.sample()  # 随机动作
    obs, reward, terminated, truncated, info = env.step(action)
    
    if terminated.any() or truncated.any():
        obs, info = env.reset()</p>

<p>tasks = [
    "PickCube-v1",           # 抓取立方体
    "StackCube-v1",          # 堆叠立方体
    "PegInsertionSide-v1",   # 侧向插入
    "OpenCabinetDoor-v1",    # 开柜门
    "OpenCabinetDrawer-v1",  # 开抽屉
    "PushChair-v1",          # 推椅子
    "AssemblingKits-v1",     # 组装套件
]
</code></pre>

<h4>4.3 SAPIEN 自定义场景</h4>

<pre><code class="python">import sapien.core as sapien
from sapien.utils import Viewer</p>

<p>engine = sapien.Engine()
renderer = sapien.SapienRenderer()
engine.set_renderer(renderer)</p>

<p>scene = engine.create_scene()
scene.set_timestep(1/240)</p>

<p>scene.add_ground(altitude=0)</p>

<p>loader = scene.create_urdf_loader()
robot = loader.load("franka_panda/panda.urdf")
robot.set_root_pose(sapien.Pose([0, 0, 0]))</p>

<p>articulation_loader = scene.create_urdf_loader()
cabinet = articulation_loader.load("partnet_mobility/cabinet.urdf")</p>

<p>drawer_joint = cabinet.get_active_joints()[0]
drawer_joint.set_drive_property(stiffness=100, damping=10)</p>

<p>viewer = Viewer(renderer)
viewer.set_scene(scene)</p>

<p>while not viewer.closed:
    # 控制机器人
    robot.set_qpos(target_qpos)
    
    # 控制抽屉
    drawer_joint.set_drive_target(0.3)  # 打开 30cm
    
    scene.step()
    scene.update_render()
    viewer.render()
</code></pre>

<hr></p>

<h3>5. 抓取算法代码示例</h3>

<h4>5.1 Contact-GraspNet 推理</h4>

<pre><code class="python">import numpy as np
from contact_graspnet.inference import GraspEstimator</p>

<p>grasp_estimator = GraspEstimator(
    checkpoint_path='checkpoints/contact_graspnet.pth',
    device='cuda'
)</p>

<p>point_cloud = np.load('scene_pointcloud.npy')  # (N, 3)
colors = np.load('scene_colors.npy')  # (N, 3), 可选</p>

<p>grasps, scores, contacts = grasp_estimator.predict(
    point_cloud,
    colors=colors,
    forward_passes=5,  # 多次前向取平均
)</p>

<p>best_idx = np.argmax(scores)
best_grasp = grasps[best_idx]
print(f"最佳抓取位置: {best_grasp[:3, 3]}")
print(f"置信度: {scores[best_idx]:.3f}")
</code></pre>

<h4>5.2 DexGraspNet 数据加载</h4>

<pre><code class="python">import h5py
import numpy as np</p>

<p>with h5py.File('dexgraspnet/grasps.h5', 'r') as f:
    # 物体信息
    object_code = f['object_code'][()]
    object_scale = f['object_scale'][()]
    
    # 抓取姿态
    hand_qpos = f['hand_qpos'][()]  # (N, 22) 关节角度
    hand_trans = f['hand_trans'][()]  # (N, 3) 手腕位置
    hand_rot = f['hand_rot'][()]  # (N, 3, 3) 手腕旋转
    
    # 抓取质量
    grasp_quality = f['grasp_quality'][()]  # (N,)</p>

<p>good_grasps = grasp_quality &gt; 0.5
selected_qpos = hand_qpos[good_grasps]</p>

<p>from isaacgym import gymapi
</code></pre>

<h4>5.3 GraspGF 生成抓取</h4>

<pre><code class="python">from graspgf.model import GraspGFModel
from graspgf.sampling import langevin_dynamics</p>

<p>model = GraspGFModel.load_from_checkpoint('graspgf.ckpt')
model.eval()</p>

<p>scene_pc = torch.from_numpy(point_cloud).float().cuda()</p>

<p>num_samples = 100
init_grasps = torch.randn(num_samples, 9).cuda()  # 6D pose + 3D approach</p>

<p>with torch.no_grad():
    for t in reversed(range(1000)):
        # 预测 score
        score = model(init_grasps, scene_pc, t)
        
        # Langevin 更新
        noise_scale = get_noise_scale(t)
        init_grasps = init_grasps + 0.5 <em> score + noise_scale </em> torch.randn_like(init_grasps)</p>

<p>final_grasps = convert_to_se3(init_grasps)
</code></pre>

<hr></p>

<h3>6. 面试 Q&A</h3>

<h4>Q1: Dex-Net 的核心原理是什么？</h4>

<p><strong>Dex-Net 三大核心</strong>:
<li><strong>大规模合成数据</strong>: 在仿真中生成数百万抓取样本，解决真机数据稀缺问题</li>
<li><strong>GQ-CNN</strong>: 学习从深度图预测抓取成功概率</li>
<li><strong>Antipodal 采样</strong>: 基于几何约束的抓取候选生成</li></p>

<p><strong>为什么有效</strong>:
<ul><li>合成数据 + Domain Randomization 提高泛化</li>
<li>Ferrari-Canny Metric 提供物理可靠的标签</li>
<li>端到端学习避免手工特征设计</li>
</ul>
<h4>Q2: Contact-GraspNet vs GraspGF 区别？</h4>

<table>
<thead><tr>
<th>维度</th>
<th>Contact-GraspNet</th>
<th>GraspGF</th>
</tr></thead>
<tbody>
<tr>
<td><strong>方法</strong></td>
<td>判别式 (直接回归)</td>
<td>生成式 (Score-based)</td>
</tr>
<tr>
<td><strong>速度</strong></td>
<td>快 (~50ms)</td>
<td>慢 (~1s, 需迭代)</td>
</tr>
<tr>
<td><strong>多样性</strong></td>
<td>一次输出固定数量</td>
<td>可控制生成数量</td>
</tr>
<tr>
<td><strong>多模态</strong></td>
<td>较弱</td>
<td>强 (天然处理多模态)</td>
</tr>
<tr>
<td><strong>适用</strong></td>
<td>实时应用</td>
<td>需要多样抓取的场景</td>
</tr>
</tbody></table>

<h4>Q3: DexGraspNet 数据是怎么生成的？</h4>

<p><li><strong>初始化</strong>: 在物体周围随机采样手腕位姿</li>
<li><strong>优化</strong>: 最小化接触距离 + 穿透惩罚 + 力闭合约束</li>
<li><strong>验证</strong>: 在 Isaac Gym 中仿真，检查是否能成功抬起物体</li>
<li><strong>筛选</strong>: 保留成功率 > 50% 的抓取</li></p>

<h4>Q4: Isaac Gym vs Isaac Lab 怎么选？</h4>

<ul><li><strong>Isaac Gym</strong>: 老项目、需要最大并行度、不需要 GUI</li>
<li><strong>Isaac Lab</strong>: 新项目、需要模块化、需要与 Isaac Sim 互通</li>
</ul>
<h4>Q5: SAPIEN 的优势是什么？</h4>

<ul><li><strong>Part-level 交互</strong>: 专门为可交互物体设计 (开门、开抽屉)</li>
<li><strong>PartNet-Mobility</strong>: 大规模关节物体数据集</li>
<li><strong>轻量</strong>: 比 Isaac Sim 更轻量，适合快速实验</li>
</ul>
<h4>Q6: 如何评估抓取算法？</h4>

<table>
<thead><tr>
<th>指标</th>
<th>描述</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Success Rate</strong></td>
<td>抓取成功率 (仿真/真机)</td>
</tr>
<tr>
<td><strong>Clearance Rate</strong></td>
<td>抬起后保持稳定的比例</td>
</tr>
<tr>
<td><strong>Coverage</strong></td>
<td>能抓取的物体类别比例</td>
</tr>
<tr>
<td><strong>Diversity</strong></td>
<td>同一物体生成抓取的多样性</td>
</tr>
<tr>
<td><strong>Inference Time</strong></td>
<td>推理速度</td>
</tr>
</tbody></table>

<hr></p>

<h3>📚 推荐资源</h3>

<h4>论文</h4>
<ul><li><a href="https://arxiv.org/abs/2103.14127">Contact-GraspNet: Efficient 6-DoF Grasp Generation</a></li>
<li><a href="https://arxiv.org/abs/2210.02697">DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset</a></li>
<li><a href="https://arxiv.org/abs/2309.06038">GraspGF: Learning Score-based Grasping</a></li>
<li><a href="https://arxiv.org/abs/2303.00938">UniDexGrasp: Universal Robotic Dexterous Grasping</a></li>
</ul>
<h4>代码库</h4>
<ul><li><a href="https://github.com/NVlabs/contact_graspnet">Contact-GraspNet</a></li>
<li><a href="https://github.com/PKU-EPIC/DexGraspNet">DexGraspNet</a></li>
<li><a href="https://github.com/graspgf/graspgf">GraspGF</a></li>
<li><a href="https://github.com/NVIDIA-Omniverse/IsaacLab">Isaac Lab</a></li>
<li><a href="https://github.com/haosulab/ManiSkill">ManiSkill</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第22章 运动规划</h2>

<blockquote><strong>面试场景</strong>: “请你介绍一下机器人运动规划的常见方法，并说明在实际工程中如何选择？”</blockquote>

<p>本文总结采样式规划、轨迹优化、混合范式以及工程落地 (MoveIt / cuRobo) 的要点，帮助快速回答机器人运动规划相关问题。</p>

<hr></p>

<h3>🧭 运动规划全景</h3>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                       规划决策树                                           │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   起点 + 目标 + 约束                                                     │
│          │                                                               │
│          ├── 低维 (2D/3D) → 栅格搜索 (A<em>, D</em>)                             │
│          │                                                               │
│          └── 高维 (机械臂, 7DoF+)                                         │
│                 │                                                         │
│                 ├── 采样式规划 (Sampling-Based)                           │
│                 │     • RRT, RRT<em>, PRM, BIT</em>                              │
│                 │                                                         │
│                 ├── 轨迹优化 (Trajectory Optimization)                    │
│                 │     • CHOMP, TrajOpt, STOMP, GPMP                       │
│                 │                                                         │
│                 └── 混合方法                                              │
│                       • RRT + TrajOpt (先可行再优化)                      │
│                       • Learning + Planning (策略热启动)                  │
│                                                                          │
│   输出: 可行路径/轨迹 (Piecewise Linear / Polynomial / Spline)           │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<p>--- </p>

<blockquote><strong>面试场景</strong>: “请你介绍一下机器人运动规划的常见方法，并说明在实际工程中如何选择？”</blockquote>

<p>本文总结采样式规划、轨迹优化、混合范式以及工程落地 (MoveIt / cuRobo) 的要点，帮助快速回答机器人运动规划相关问题。</p>

<hr></p>

<h3>🧭 运动规划全景</h3>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                       规划决策树                                           │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   起点 + 目标 + 约束                                                     │
│          │                                                               │
│          ├── 低维 (2D/3D) → 栅格搜索 (A<em>, D</em>)                             │
│          │                                                               │
│          └── 高维 (机械臂, 7DoF+)                                         │
│                 │                                                         │
│                 ├── 采样式规划 (Sampling-Based)                           │
│                 │     • RRT, RRT<em>, PRM, BIT</em>                              │
│                 │                                                         │
│                 ├── 轨迹优化 (Trajectory Optimization)                    │
│                 │     • CHOMP, TrajOpt, STOMP, GPMP                       │
│                 │                                                         │
│                 └── 混合方法                                              │
│                       • RRT + TrajOpt (先可行再优化)                      │
│                       • Learning + Planning (策略热启动)                  │
│                                                                          │
│   输出: 可行路径/轨迹 (Piecewise Linear / Polynomial / Spline)           │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<hr></p>

<h3>1. 采样式规划 (Sampling-Based Planning)</h3>

<h4>1.1 核心理念</h4>

<ul><li>直接在配置空间 \( \mathcal{C} \) 中随机采样节点，避免显式离散化高维空间。</li>
<li>避免对障碍物几何进行显式建模，只需要碰撞检测函数 <code>collisionFree(q)</code>。</li>
<li>随采样次数增加，算法趋近于找到可行路径；带有 <em>-star</em> 的算法还能保证渐进最优 (asymptotically optimal)。</li>
</ul>
<h4>1.2 RRT / RRT*</h4>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                     RRT 扩展示意                                           │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   q_start ●                                                              │
│          │＼                                                             │
│          │  ＼                                                           │
│          │    ● q_new                                                    │
│          │    / │                                                        │
│          │  ／  │                                                       │
│       ● ─┘     │                                                        │
│    q_rand      │                                                        │
│                │                                                        │
│           障碍物                                                         │
│                                                                          │
│   步骤:                                                                   │
│   1. 随机采样 q_rand                                                      │
│   2. 找到树中最近节点 q_near                                              │
│   3. 朝 q_rand 方向扩展 step → q_new                                      │
│   4. 若连线无碰撞，将 q_new 加入树                                        │
│                                                                          │
│   RRT*: 在插入新节点时重新连线 (rewire)，选择代价更低的父节点，并更新子树    │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<table>
<thead><tr>
<th>算法</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr></thead>
<tbody>
<tr>
<td><strong>RRT</strong></td>
<td>实现简单，快速找到可行路径</td>
<td>路径质量差，抖动</td>
<td>高维配置空间，快速避障</td>
</tr>
<tr>
<td><strong>RRT\</strong>*</td>
<td>渐进最优，路径更平滑</td>
<td>计算量大 (rewire)</td>
<td>需要较高质量路径</td>
</tr>
<tr>
<td><strong>Informed RRT\</strong>*</td>
<td>采样限制在椭球区域，加速收敛</td>
<td>需要启发式</td>
<td>起止点已知、需加速</td>
</tr>
<tr>
<td><strong>BIT\</strong>*</td>
<td>批量采样 + 最优</td>
<td>实现复杂</td>
<td>需要兼顾速度和最优性</td>
</tr>
</tbody></table>

<h4>1.3 PRM / PRM\*</h4>

<ul><li><strong>PRM (Probabilistic Roadmap)</strong>: 先离线采样大量节点并构建无向图，在线阶段只需连接起点/终点。</li>
<li>适用于多次查询的场景 (同一空间多次规划)。</li>
<li>PRM\*: 渐进最优版本，连接半径随样本数量动态调整。</li>
</ul>
<hr></p>

<h3>2. 轨迹优化 (Trajectory Optimization)</h3>

<h4>2.1 思路</h4>

<ul><li>将轨迹离散化为 \( \mathbf{x}_{0:T} \)，定义优化目标 + 约束，转化为非线性优化问题。</li>
<li>典型目标：路径长度、速度/加速度正则、与障碍物距离惩罚。</li>
<li>约束：动力学约束、接触约束、关节限制、终端姿态等。</li>
</ul>
<h4>2.2 典型算法</h4>

<table>
<thead><tr>
<th>算法</th>
<th>核心思想</th>
<th>优势</th>
<th>局限</th>
</tr></thead>
<tbody>
<tr>
<td><strong>CHOMP</strong></td>
<td>基于梯度下降 + 碰撞势能，优化离散轨迹</td>
<td>平滑、可融入障碍势</td>
<td>对初值敏感，梯度局部</td>
</tr>
<tr>
<td><strong>TrajOpt</strong></td>
<td>以凸优化 (sequential convex) 形式求解</td>
<td>收敛快，可加复杂约束</td>
<td>需要良好初值</td>
</tr>
<tr>
<td><strong>STOMP</strong></td>
<td>采样多个扰动轨迹，根据代价加权平均</td>
<td>无需梯度，鲁棒</td>
<td>计算量大</td>
</tr>
<tr>
<td><strong>GPMP2</strong></td>
<td>将轨迹建模为高斯过程，使用因子图优化</td>
<td>与 SLAM/估计工具链一致</td>
<td>实现复杂</td>
</tr>
<tr>
<td><strong>MPC (Short-horizon)</strong></td>
<td>在线滚动优化局部轨迹</td>
<td>能处理动态障碍</td>
<td>需要实时算力</td>
</tr>
</tbody></table>

<h4>2.3 TrajOpt 代价函数示例</h4>

<pre><code class="">J(x) = w_smooth * Σ ||x_{t+1} - 2x_t + x_{t-1}||²    (平滑项)
     + w_collision * Σ max(0, d_safe - d(x_t))²       (碰撞约束软化)
     + w_goal * ||x_T - x_goal||²                     (终点约束)</p>

<p>subject to: joint_limits, velocity_limits, etc.
</code></pre>

<hr></p>

<h3>3. 混合方法与工程实践</h3>

<h4>3.1 采样 + 优化 (Warm-Start)</h4>

<p><li><strong>RRT 找到可行路径</strong>，保证碰撞自由。</li>
<li><strong>TrajOpt/CHOMP</strong> 以 RRT 结果作为初始轨迹，进一步平滑并满足动力学约束。</li>
<li>工程中常结合 MoveIt “OMPL (采样) + TrajOpt (优化)” Pipeline。</li></p>

<h4>3.2 学习辅助规划</h4>

<ul><li><strong>策略热启动</strong>: 用模仿/强化学习模型预测初始轨迹，再交给 TrajOpt 优化。</li>
<li><strong>价值函数引导采样</strong>: 训练一个 value network 评估节点好坏，提高采样效率 (Learning-RRT)。</li>
<li><strong>Diffusion/LfD</strong>: 直接生成可行轨迹，作为规划器候选。</li>
</ul>
<hr></p>

<h3>4. MoveIt & cuRobo 实战</h3>

<h4>4.1 MoveIt 规划流水线</h4>

<pre><code class="">┌──────────────────────────────────────────────────────────────────────────┐
│                        MoveIt Pipeline                                   │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Planning Scene ← Obstacles / Robot State                                │
│          │                                                               │
│          ▼                                                               │
│  OMPL Planner (RRTConnect, RRT*)                                         │
│          │  (可选前处理: Simplify)                                      │
│          ▼                                                               │
│  Trajectory Post-processing                                              │
│   • Time Parameterization (TOTG)                                         │
│   • Smoothing                                                            │
│                                                                          │
│  Controller Manager → FollowJointTrajectory                              │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>

<ul><li><strong>OMPL</strong>: 包含大部分采样式规划器 (RRT, RRT*, PRM, KPIECE)。</li>
<li><strong>Pilz</strong>: MoveIt 内置的笛卡尔规划器，适合直线/插补。</li>
<li><strong>TrajOpt / STOMP 插件</strong>: 需要额外安装，提供优化型规划。</li>
</ul>
<h4>4.2 NVIDIA cuRobo</h4>

<ul><li>基于 GPU 的运动规划库，采用并行化的 <strong>多种规划器候选 + 轨迹优化</strong>。</li>
<li>可在 Jetson Orin 上实现 <100ms 的 7DoF 机械臂规划。</li>
</ul>
<hr></p>

<h3>5. 代码参考</h3>

<pre><code class="python">import moveit_commander
import geometry_msgs.msg as geometry_msgs</p>

<p>moveit_commander.roscpp_initialize([])
robot = moveit_commander.RobotCommander()
group = moveit_commander.MoveGroupCommander("manipulator")</p>

<p>group.set_planner_id("RRTConnectkConfigDefault")
group.set_num_planning_attempts(10)
group.set_planning_time(2.0)</p>

<p>pose_goal = geometry_msgs.Pose()
pose_goal.position.x = 0.4
pose_goal.position.y = 0.2
pose_goal.position.z = 0.3
pose_goal.orientation.w = 1.0</p>

<p>group.set_pose_target(pose_goal)
plan = group.go(wait=True)
group.stop()
group.clear_pose_targets()
</code></pre>

<pre><code class="python">import ompl.base as ob
import ompl.geometric as og</p>

<p>def is_state_valid(space_information, state):
    # TODO: 调用碰撞检测
    return True</p>

<p>space = ob.RealVectorStateSpace(7)  # 7 DoF
bounds = ob.RealVectorBounds(7)
bounds.setLow(-3.14)
bounds.setHigh(3.14)
space.setBounds(bounds)</p>

<p>si = ob.SpaceInformation(space)
si.setStateValidityChecker(ob.StateValidityCheckerFn(
    lambda state: is_state_valid(si, state)))
si.setup()</p>

<p>problem = ob.ProblemDefinition(si)
start = ob.State(space)
goal = ob.State(space)
problem.setStartAndGoalStates(start, goal)</p>

<p>planner = og.RRTstar(si)
planner.setRange(0.1)  # 扩展步长
planner.setProblemDefinition(problem)
planner.setup()</p>

<p>if planner.solve(5.0):
    path = problem.getSolutionPath()
    path.interpolate(100)
</code></pre>

<hr></p>

<h3>6. 面试 Q&A</h3>

<h4>Q1: 采样式规划 vs 轨迹优化？</h4>

<ul><li>采样式：易找到可行解，适合复杂障碍空间；但路径抖动，需要后处理。</li>
<li>轨迹优化：可直接考虑动力学和成本，但对初值敏感，可能陷入局部最优。</li>
<li>工程实践：先采样找可行路径，再用 TrajOpt 平滑。</li>
</ul>
<h4>Q2: 如何处理动态障碍？</h4>

<p><li>使用 <strong>在线 Re-planning</strong> (RRT Replan) 或 <strong>MPC</strong>。</li>
<li>维护 <strong>占据栅格</strong> / <strong>ESDF</strong> 实时更新碰撞信息。</li>
<li>短时 MPC + 长期规划结合：全局路径 + 局部避障。</li></p>

<h4>Q3: 高维机械臂如何加速规划？</h4>

<ul><li>合理的 <strong>joint limits</strong> 与 <strong>sampling bounds</strong>。</li>
<li>使用 <strong>IK 解</strong> 作为起点，减少搜索空间。</li>
<li><strong>Task Space RRT</strong>: 在笛卡尔空间采样，再投影回 joint space。</li>
<li>利用 GPU (cuRobo) 并行求解。</li>
</ul>
<h4>Q4: CHOMP 为什么需要“障碍势能”？</h4>

<ul><li>通过距离场定义势能 \( U(q) = \phi(d(q)) \)，在轨迹优化时通过梯度远离障碍。</li>
<li>常用 SDF (Signed Distance Field) 或 ESDF 作为距离计算。</li>
</ul>
<h4>Q5: 如何保证规划结果可执行？</h4>

<ul><li><strong>时间参数化</strong>: TOTG / Iterative Parabolic Time Parameterization。</li>
<li><strong>速度/加速度限制</strong>: 确保关节命令不超限。</li>
<li><strong>轨迹跟踪控制</strong>: Feedforward + PD/MPC。</li>
</ul>
<hr></p>

<h3>📚 推荐阅读</h3>

<ul><li><em>Principles of Robot Motion</em> (MIT Press)</li>
<li>OMPL: <a href="https://ompl.kavrakilab.org/">ompl.kavrakilab.org</a></li>
<li>MoveIt Tutorials: <a href="https://moveit.picknik.ai/">moveit.picknik.ai</a></li>
<li>NVIDIA cuRobo: <a href="https://github.com/NVlabs/curobo">github.com/NVlabs/curobo</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第23章 触觉 VLA</h2>

<p>在机器人操作中，视觉 (Vision) 往往不足以完成所有任务。对于接触密集型 (Contact-rich) 任务（如在黑暗中摸索物体、精密装配、判断物体材质），<strong>触觉 (Tactile)</strong> 是不可或缺的模态。</p>

<p>2024-2025 年，VLA 领域开始爆发 "Vision-Tactile-Language-Action" 的研究，旨在赋予机器人"触觉语义"理解能力。</p>

<h3>1. 为什么需要触觉? (Why Tactile?)</h3>
<ul><li><strong>视觉遮挡 (Occlusion)</strong>: 当机械手抓取物体时，手掌会挡住摄像头视线。此时只有触觉能提供反馈。</li>
<li><strong>物理属性感知</strong>: 视觉无法直接判断物体的软硬、摩擦力、重量。</li>
<li><strong>微米级控制</strong>: 视觉通常有毫米级误差，而触觉传感器 (如 GelSight) 可以提供微米级的纹理信息。</li>
</ul>
<h3>2. 核心传感器技术</h3>
<ul><li><strong>GelSight / Digit</strong>: 基于光学的触觉传感器。</li>
</ul>    - <strong>原理</strong>: 内部有一个弹性体 (Elastomer) 和摄像头。当弹性体变形时，摄像头拍摄其表面的纹理变化。
    - <strong>优势</strong>: 输出是高分辨率图像，可以直接喂给 CNN/ViT 处理，与 CV 技术栈完美兼容。</p>

<h3>3. 最新模型进展 (2024-2025)</h3>

<h4>3.1 VLA-Touch (2025)</h4>
<blockquote><strong>论文</strong>: <a href="https://arxiv.org/abs/2502.xxxxx">VLA-Touch: Enhancing Generalist Robot Policies with Dual-Level Tactile Feedback</a></blockquote>
<blockquote><strong>核心思想</strong>: 双层反馈机制 (Dual-level Feedback)。</blockquote>

<ul><li><strong>背景</strong>: 现有的 VLA (如 RT-2) 缺乏触觉通道。直接微调大模型成本太高。</li>
<li><strong>架构细节</strong>:</li>
</ul>    1.  <strong>High-level Planning (VLM)</strong>:
        - <strong>Tactile Encoder</strong>: 使用 <strong>ResNet-50</strong> 或 <strong>ViT-B</strong> 编码 GelSight 图像。
        - <strong>Tactile-Language Model (TLM)</strong>: 预训练一个 Decoder-only Transformer，将触觉 Embedding 翻译成自然语言描述 (e.g., "I feel a smooth, hard surface")。
        - <strong>Prompting</strong>: 将生成的触觉描述作为 Prompt 喂给 VLM (如 GPT-4o 或 Gemini)，辅助其进行推理。
    2.  <strong>Low-level Control (Diffusion)</strong>:
        - <strong>Fusion</strong>: 使用 <strong>FiLM (Feature-wise Linear Modulation)</strong> 将触觉特征注入到 Diffusion Policy 的 U-Net 中。
        - <strong>Action Refinement</strong>: 触觉信号主要用于修正动作的最后几毫米 (Contact Phase)，确保接触力适中。
<ul><li><strong>优势</strong>: 无需重新训练整个 VLA，即插即用。</li>
</ul>
<h4>3.2 OmniVTLA (2025)</h4>
<blockquote><strong>论文</strong>: <a href="https://arxiv.org/abs/2503.xxxxx">OmniVTLA: A Unified Vision-Tactile-Language-Action Model</a></blockquote>
<blockquote><strong>核心思想</strong>: 统一的视触觉语言动作模型 (Unified Vision-Tactile-Language-Action Model)。</blockquote>

<ul><li><strong>架构细节</strong>:</li>
</ul>    - <strong>Unified Tokenization</strong>:
        - <strong>Vision</strong>: ViT Patch Embeddings.
        - <strong>Tactile</strong>: 同样使用 ViT 处理触觉图像，将其 Patch 化为 Tactile Tokens。
        - <strong>Language</strong>: 文本 Token。
    - <strong>Semantic Alignment (Contrastive Learning)</strong>:
        - 关键创新在于<strong>语义对齐</strong>。它不仅学习触觉的物理特征，还学习触觉的语义描述 (e.g., "slippery", "rough")。
        - <strong>Loss Function</strong>: $\mathcal{L} = \mathcal{L}_{action} + \lambda \mathcal{L}_{align}$。其中 $\mathcal{L}_{align}$ 是 InfoNCE Loss，拉近触觉 Embedding 与对应的材质描述文本 Embedding 的距离。
<ul><li><strong>训练</strong>: 使用大规模的多模态数据集 (包含图像、触觉图、语言指令、动作)。</li>
<li><strong>能力</strong>: 能够执行 "Pick up the softest object" (抓起最软的物体) 这种需要跨模态推理的任务。</li>
</ul>
<h3>4. 挑战与未来</h3>
<li> <strong>数据稀缺</strong>: 相比于图像数据，高质量的触觉-语言对 (Tactile-Language Pairs) 非常少。</li>
<li> <strong>Sim-to-Real</strong>: 触觉仿真非常困难 (涉及复杂的软体形变)，目前主要依赖真机数据收集。</li>
<li> <strong>硬件成本</strong>: 高分辨率触觉传感器 (如 GelSight) 依然昂贵且易损耗。</li></p>

<h3>5. 深度解析: ResNet vs ViT for Tactile</h3>
在触觉 VLA 中，选择 ResNet 还是 ViT 作为触觉编码器 (Tactile Encoder) 是一个关键的设计决策。这不仅仅是"CNN vs Transformer"的问题，而是关乎<strong>触觉信号的物理特性</strong>如何被编码。</p>

<h4>5.1 ResNet (CNN): 纹理与几何的专家</h4>
ResNet 在处理 GelSight 这类<strong>基于光学 (Optical-based)</strong> 的触觉传感器时表现出色，原因在于其<strong>归纳偏置 (Inductive Bias)</strong> 与触觉图像的特性高度契合。</p>

<p>*   <strong>技术细节</strong>:
    *   <strong>平移不变性 (Translation Invariance)</strong>: 触觉特征（如物体表面的凸起）可能出现在传感器的任何位置。ResNet 的<strong>权重共享 (Weight Sharing)</strong> 卷积核保证了无论凸起在哪里，提取的特征都是一致的。
    *   <strong>局部性 (Locality)</strong>: 触觉感知的核心是<strong>接触 (Contact)</strong>。接触通常发生在局部区域。ResNet 的卷积核 (e.g., 3x3) 强制模型关注局部像素的梯度变化，这对于检测<strong>边缘 (Edges)</strong>、<strong>纹理 (Textures)</strong> 和 <strong>滑移 (Slip)</strong> 至关重要。
    *   <strong>层级特征 (Hierarchical Features)</strong>: ResNet 通过 Pooling 不断下采样，自然地形成了从"微观纹理"到"宏观形状"的特征金字塔。这对于判断物体材质（微观）和抓取稳定性（宏观）都很有用。</p>

<h4>5.2 ViT (Transformer): 全局接触与多模态统一</h4>
ViT 在 OmniVTLA 等最新模型中更受欢迎，主要是为了<strong>多模态对齐</strong>和<strong>全局上下文</strong>。</p>

<p>*   <strong>技术细节</strong>:
    *   <strong>Patchify & Linear Projection</strong>: ViT 将 $224 \times 224$ 的触觉图像切分为 $16 \times 16$ 的 Patches，展平后通过线性层映射为 Embedding。这一步彻底打破了像素的网格结构，使其能与文本 Token 在同一向量空间中交互。
    *   <strong>全局感受野 (Global Receptive Field)</strong>: Self-Attention 允许每一个 Tactile Patch 在第一层就与其他所有 Patch 交互。这对于理解<strong>多点接触 (Multi-point Contact)</strong> 非常关键。例如，当手指捏住物体时，指尖两侧的受力分布是相关的，ResNet 需要堆叠多层才能"看"到这种长距离关联，而 ViT 一眼就能看到。
    *   <strong>位置编码 (Positional Encoding)</strong>: 由于 ViT 没有卷积的归纳偏置，它必须依赖可学习的位置编码来理解"哪里是上，哪里是下"。在触觉中，绝对位置往往对应着机械手的具体部位 (e.g., 指尖 vs 指腹)，这对控制很重要。</p>

<h4>5.3 核心差异对比表</h4>

<table>
<thead><tr>
<th>特性</th>
<th>ResNet (Tactile)</th>
<th>ViT (Tactile)</th>
</tr></thead>
<tbody>
<tr>
<td><strong>归纳偏置</strong></td>
<td>强 (平移不变, 局部性)</td>
<td>弱 (需数据学习)</td>
</tr>
<tr>
<td><strong>擅长特征</strong></td>
<td><strong>高频纹理</strong> (Texture), 边缘, 局部形变</td>
<td><strong>低频分布</strong> (Force Distribution), 全局接触模式</td>
</tr>
<tr>
<td><strong>数据需求</strong></td>
<td>低 (几千张图即可收敛)</td>
<td>高 (需 MAE 预训练或 ImageNet 迁移)</td>
</tr>
<tr>
<td><strong>多模态融合</strong></td>
<td>需通过 Pooling 压缩成向量后融合</td>
<td><strong>Token 级融合</strong> (可与 Text Token 拼接)</td>
</tr>
<tr>
<td><strong>典型应用</strong></td>
<td>材质识别, 滑移检测 (Slip Detection)</td>
<td>复杂操作策略 (Manipulation Policy), 跨模态推理</td>
</tr>
</tbody></table>

<h3>6. 面试常见问题</h3>
<strong>Q: 触觉图像 (Tactile Image) 和普通 RGB 图像有什么区别?</strong>
A: 触觉图像通常反映的是<strong>几何形状 (Geometry)</strong> 和 <strong>受力分布 (Force Distribution)</strong>，对光照变化不敏感，但对接触极其敏感。处理时通常不需要复杂的颜色增强，但需要关注纹理细节。</p>

<p><strong>Q: 如何将触觉融入 VLA?</strong>
A: 最简单的方法是将触觉图像视为额外的视觉通道 (Concat)，或者使用 Cross-Attention 将触觉特征注入到 Policy 中。最新的趋势是像 OmniVTLA 一样进行多模态对齐。</p>

<hr></p>

<hr></p>

<h1>第六部分：前沿模型解析</h1>

<hr></p>

<h2>第24章 RDT (Robotics Diffusion Transformer)</h2>

<blockquote><strong>核心论文</strong>: <a href="https://arxiv.org/abs/2410.07864">RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</a> (Liu et al., 2024)</blockquote>
<blockquote><strong>开发者</strong>: 清华大学 MARS Lab & 字节跳动</blockquote>
<blockquote><strong>代表模型</strong>: <strong>RDT-170M</strong>, <strong>RDT-1B</strong></blockquote>

<h3>1. 为什么需要 RDT? (Why?)</h3>

<h4>1.1 机器人学习的 Scaling Law 困境</h4>

<p>在 NLP 和 CV 领域，<strong>Scaling Law</strong> 已被充分验证：模型越大、数据越多，性能越好。然而，机器人学习领域一直缺乏类似的验证:</p>

<ul><li><strong>数据稀缺</strong>: 机器人数据收集成本高，数量级远低于图文数据。</li>
<li><strong>分布多样</strong>: 不同机器人 (单臂/双臂/人形) 的状态空间、动作空间差异巨大。</li>
<li><strong>缺乏统一架构</strong>: 之前的方法 (ACT, Diffusion Policy) 多为任务特定设计，难以规模化。</li>
</ul>
<h4>1.2 RDT 的核心目标</h4>

<p>RDT 是首个<strong>十亿参数级</strong>的机器人扩散基础模型，专为解决以下问题：</p>

<p><li><strong>Scaling</strong>: 证明机器人策略也能从"大模型+大数据"中获益。</li>
<li><strong>跨形态泛化 (Cross-Embodiment)</strong>: 单一模型适配不同机器人 (单臂/双臂)。</li>
<li><strong>双臂操作 (Bimanual)</strong>: 特别优化了需要两只手协调的复杂任务。</li></p>

<h3>2. 核心技术 (Core Techniques)</h3>

<h4>2.1 可扩展的 DiT 架构 (Scalable Diffusion Transformer)</h4>

<p>RDT 基于 <strong>DiT (Diffusion Transformer)</strong> 架构，将扩散过程与 Transformer 结合。</p>

<h4>2.1.1 为什么选择 DiT?</h4>

<table>
<thead><tr>
<th>架构</th>
<th>优点</th>
<th>缺点</th>
</tr></thead>
<tbody>
<tr>
<td><strong>U-Net (Diffusion Policy)</strong></td>
<td>成熟、稳定</td>
<td>难以规模化，参数效率低</td>
</tr>
<tr>
<td><strong>DiT</strong></td>
<td>与 LLM 架构统一，易于扩展</td>
<td>需要更多数据</td>
</tr>
</tbody></table>

<p>DiT 的核心思想是将扩散模型的去噪网络从 U-Net 替换为 Transformer，这使得：
<ul><li>可以直接借鉴 LLM 的训练技术 (FlashAttention, Gradient Checkpointing)。</li>
<li>参数量可以平滑扩展到数十亿。</li>
</ul>
<h4>2.1.2 RDT 的架构设计</h4>

<pre><code class="">                    ┌──────────────────────────────────────┐
                    │          Condition Encoder           │
                    │  (图像: SigLIP + 语言: T5)           │
                    └───────────────┬──────────────────────┘
                                    │ condition [B, L, D]
                                    ▼
┌──────────────┐    ┌──────────────────────────────────────┐
│  Noisy       │───▶│        DiT Backbone                  │
│  Action      │    │  (带 AdaLN-Zero 的 Transformer)      │
│  x_t         │    │                                      │
├──────────────┤    │  - 输入: x_t + t_emb + cond          │
│  Timestep t  │───▶│  - 输出: 预测噪声 ε 或 v             │
└──────────────┘    └───────────────┬──────────────────────┘
                                    │
                                    ▼
                         Denoised Action x_{t-1}
</code></pre>

<p><strong>关键组件</strong>:</p>

<p><li><strong>AdaLN-Zero (Adaptive Layer Normalization)</strong>:</li>
   - 将时间步 $t$ 和条件信息注入到每个 Transformer Block。
   - 公式: $\text{AdaLN}(h, c) = c_s \odot \text{LayerNorm}(h) + c_b$
   - 其中 $c_s, c_b$ 是从条件 $c$ 预测的 scale 和 bias。</p>

<p><li><strong>统一的动作表示 (Unified Action Representation)</strong>:</li>
   - <strong>单臂</strong>: $(x, y, z, roll, pitch, yaw, gripper) \in \mathbb{R}^7$
   - <strong>双臂</strong>: 拼接两个单臂 + 可选的躯干自由度 $\in \mathbb{R}^{14+}$
   - 通过 <strong>Padding + Masking</strong> 统一不同形态的动作维度。</p>

<h4>2.2 大规模预训练数据 (Pre-training Data)</h4>

<p>RDT 在多个大规模机器人数据集上预训练：</p>

<table>
<thead><tr>
<th>数据集</th>
<th>类型</th>
<th>规模</th>
<th>特点</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Open X-Embodiment</strong></td>
<td>真机</td>
<td>1M+ episodes</td>
<td>多种机器人形态</td>
</tr>
<tr>
<td><strong>DROID</strong></td>
<td>真机</td>
<td>76k episodes</td>
<td>高质量双臂数据</td>
</tr>
<tr>
<td><strong>RH20T</strong></td>
<td>真机</td>
<td>20k episodes</td>
<td>中国场景</td>
</tr>
<tr>
<td><strong>RoboTurk</strong></td>
<td>真机</td>
<td>2k episodes</td>
<td>远程遥操</td>
</tr>
<tr>
<td><strong>模拟数据</strong></td>
<td>仿真</td>
<td>500k+</td>
<td>RLBench, ManiSkill</td>
</tr>
</tbody></table>

<p><strong>数据配比策略</strong>:
<ul><li><strong>真机数据优先</strong>: 真机数据占 70%，模拟数据占 30%。</li>
<li><strong>双臂数据增强</strong>: 对双臂数据进行上采样，弥补其稀缺性。</li>
</ul>
<h4>2.3 条件编码 (Condition Encoding)</h4>

<p>RDT 使用<strong>多模态条件</strong>来指导动作生成：</p>

<pre><code class="python">class RDTConditionEncoder(nn.Module):
    def __init__(self):
        # 视觉编码器: SigLIP (Google 的 CLIP 变体)
        self.vision_encoder = SigLIPVisionModel.from_pretrained("google/siglip-base")
        
        # 语言编码器: T5-XXL
        self.language_encoder = T5EncoderModel.from_pretrained("google/t5-xxl")
        
        # 本体感知投影
        self.proprio_proj = nn.Linear(proprio_dim, hidden_dim)
        
        # 融合层
        self.fusion = nn.TransformerEncoder(...)
    
    def forward(self, images, language, proprio):
        # images: [B, N_cam, C, H, W] - 多相机
        # language: [B, L_text] - 语言指令
        # proprio: [B, D_proprio] - 本体感知
        
        # 编码
        vis_feat = self.vision_encoder(images.flatten(0, 1))  # [B*N_cam, D]
        vis_feat = vis_feat.view(B, N_cam, -1)
        
        lang_feat = self.language_encoder(language).last_hidden_state
        
        proprio_feat = self.proprio_proj(proprio).unsqueeze(1)
        
        # 融合
        combined = torch.cat([vis_feat, lang_feat, proprio_feat], dim=1)
        return self.fusion(combined)
</code></pre>

<h4>2.4 扩散训练与采样 (Diffusion Training & Sampling)</h4>

<p>RDT 使用 <strong>DDPM</strong> 框架进行训练，但采用了几个优化:</p>

<h4>2.4.1 训练目标</h4>

<pre><code class="math">\mathcal{L} = \mathbb{E}_{t, a_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(a_t, t, c) \|^2 \right]
</code></pre>

<p>其中:
<ul><li>$a_0$: 真实动作序列 (Action Chunk)</li>
<li>$a_t = \sqrt{\bar{\alpha}_t} a_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$: 加噪动作</li>
<li>$c$: 条件 (图像 + 语言 + 本体感知)</li>
<li>$\epsilon_\theta$: DiT 网络预测的噪声</li>
</ul>
<h4>2.4.2 推理加速</h4>

<p>RDT 使用 <strong>DDIM</strong> 将去噪步数从 100 步压缩到 <strong>10 步</strong>:</p>

<pre><code class="python">@torch.no_grad()
def sample(self, condition, num_steps=10):
    """DDIM 采样"""
    batch_size = condition.shape[0]
    
    # 从纯噪声开始
    x_t = torch.randn(batch_size, self.chunk_size, self.action_dim)
    
    # DDIM 步长
    timesteps = torch.linspace(1000, 0, num_steps + 1).long()
    
    for i in range(num_steps):
        t = timesteps[i]
        t_prev = timesteps[i + 1]
        
        # 预测噪声
        eps_pred = self.dit(x_t, t, condition)
        
        # DDIM 更新
        alpha_t = self.alphas_cumprod[t]
        alpha_prev = self.alphas_cumprod[t_prev] if t_prev &gt; 0 else 1.0
        
        x0_pred = (x_t - torch.sqrt(1 - alpha_t) * eps_pred) / torch.sqrt(alpha_t)
        x_t = torch.sqrt(alpha_prev) <em> x0_pred + torch.sqrt(1 - alpha_prev) </em> eps_pred
    
    return x_t
</code></pre>

<h3>3. 模型变体 (Model Variants)</h3>

<p>RDT 提供了不同规模的模型以适应不同的算力需求：</p>

<table>
<thead><tr>
<th>模型</th>
<th>参数量</th>
<th>层数</th>
<th>隐藏维度</th>
<th>头数</th>
<th>推理速度</th>
</tr></thead>
<tbody>
<tr>
<td><strong>RDT-170M</strong></td>
<td>170M</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>~50ms/step</td>
</tr>
<tr>
<td><strong>RDT-1B</strong></td>
<td>1.2B</td>
<td>48</td>
<td>2048</td>
<td>32</td>
<td>~200ms/step</td>
</tr>
</tbody></table>

<p><strong>Scaling Law 验证</strong>:
<ul><li>RDT-1B 在所有任务上都优于 RDT-170M，验证了机器人领域的 Scaling Law。</li>
<li>性能提升呈现<strong>对数线性</strong>关系：参数量翻 10 倍，性能提升约 15-20%。</li>
</ul>
<h3>4. 关键创新点 (Key Innovations)</h3>

<h4>4.1 统一的跨形态表示 (Unified Cross-Embodiment Representation)</h4>

<p><strong>问题</strong>: 不同机器人的动作维度不同 (单臂 7D, 双臂 14D, 人形 30+D)。</p>

<p><strong>解决方案</strong>: <strong>Padded Action Space</strong></p>

<pre><code class="python">MAX_ACTION_DIM = 32</p>

<p>def pad_action(action, embodiment_type):
    """将不同机器人的动作填充到统一维度"""
    if embodiment_type == "single_arm":
        # [7] -&gt; [32] (后面填 0)
        padded = F.pad(action, (0, MAX_ACTION_DIM - 7))
        mask = torch.cat([torch.ones(7), torch.zeros(MAX_ACTION_DIM - 7)])
    elif embodiment_type == "bimanual":
        # [14] -&gt; [32]
        padded = F.pad(action, (0, MAX_ACTION_DIM - 14))
        mask = torch.cat([torch.ones(14), torch.zeros(MAX_ACTION_DIM - 14)])
    # ...
    return padded, mask
</code></pre>

<p>训练时，只对有效维度计算损失:
<pre><code class="math">\mathcal{L} = \mathbb{E} \left[ \| m \odot (\epsilon - \hat{\epsilon}) \|^2 \right]
</code></pre>

<h4>4.2 双臂协调优化 (Bimanual Coordination)</h4>

<p>双臂操作需要两只手<strong>协同</strong>工作（如折衣服、搬运大物体）。RDT 通过以下设计增强双臂协调:</p>

<p><li><strong>动作拼接</strong>: 左右臂动作在序列维度拼接，而非独立预测。</li>
<li><strong>Cross-Arm Attention</strong>: 在 Transformer 中，左臂的 Token 可以 Attend 到右臂。</li>
<li><strong>对称性数据增强</strong>: 随机交换左右臂的输入/输出，增加数据多样性。</li></p>

<h4>4.3 高效微调 (Efficient Fine-tuning)</h4>

<p>预训练的 RDT 可以通过 <strong>LoRA</strong> 高效适配到新任务:</p>

<pre><code class="python">from peft import LoraConfig, get_peft_model</p>

<p>config = LoraConfig(
    r=16,                    # LoRA 秩
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # 只微调 QV 投影
    lora_dropout=0.05,
)</p>

<p>rdt_lora = get_peft_model(rdt_model, config)
</code></pre>

<p><strong>微调数据需求</strong>:
<ul><li><strong>10-50 条演示</strong>: 基础适配</li>
<li><strong>100-500 条演示</strong>: 高性能微调</li>
<li><strong>对比</strong>: 从头训练 Diffusion Policy 需要 1000+ 条</li>
</ul>
<h3>5. 实验结果 (Experimental Results)</h3>

<h4>5.1 Benchmark 性能</h4>

<table>
<thead><tr>
<th>任务</th>
<th>ACT</th>
<th>Diffusion Policy</th>
<th>OpenVLA</th>
<th>RDT-1B</th>
</tr></thead>
<tbody>
<tr>
<td><strong>LIBERO-90</strong></td>
<td>68.2%</td>
<td>73.5%</td>
<td>75.1%</td>
<td><strong>82.3%</strong></td>
</tr>
<tr>
<td><strong>Bimanual Fold</strong></td>
<td>45%</td>
<td>52%</td>
<td>48%</td>
<td><strong>71%</strong></td>
</tr>
<tr>
<td><strong>Pick & Place</strong></td>
<td>82%</td>
<td>85%</td>
<td>87%</td>
<td><strong>93%</strong></td>
</tr>
<tr>
<td><strong>长序列任务</strong></td>
<td>35%</td>
<td>42%</td>
<td>40%</td>
<td><strong>58%</strong></td>
</tr>
</tbody></table>

<h4>5.2 泛化能力</h4>

<p>RDT 在<strong>未见过的场景</strong>上也展现出良好的泛化:
<ul><li><strong>新物体</strong>: 成功率下降 < 10%</li>
<li><strong>新背景</strong>: 成功率下降 < 5%</li>
<li><strong>新机器人</strong>: 零样本迁移成功率 40%+</li>
</ul>
<h3>6. 与其他方法的对比 (Comparison)</h3>

<table>
<thead><tr>
<th>方法</th>
<th>架构</th>
<th>参数量</th>
<th>预训练</th>
<th>双臂支持</th>
<th>推理速度</th>
</tr></thead>
<tbody>
<tr>
<td><strong>ACT</strong></td>
<td>CVAE + Transformer</td>
<td>~10M</td>
<td>无</td>
<td>✅</td>
<td>最快</td>
</tr>
<tr>
<td><strong>Diffusion Policy</strong></td>
<td>U-Net</td>
<td>~50M</td>
<td>无</td>
<td>❌</td>
<td>慢</td>
</tr>
<tr>
<td><strong>OpenVLA</strong></td>
<td>LLM + Action Head</td>
<td>7B</td>
<td>✅</td>
<td>❌</td>
<td>慢</td>
</tr>
<tr>
<td><strong>π0</strong></td>
<td>VLM + Flow Matching</td>
<td>3B+</td>
<td>✅</td>
<td>✅</td>
<td>中等</td>
</tr>
<tr>
<td><strong>RDT-1B</strong></td>
<td>DiT</td>
<td>1.2B</td>
<td>✅</td>
<td>✅ (专门优化)</td>
<td>中等</td>
</tr>
</tbody></table>

<p><strong>RDT 的独特优势</strong>:
<ul><li><strong>专为双臂设计</strong>: 动作空间和注意力机制都针对双臂优化。</li>
<li><strong>验证 Scaling Law</strong>: 首个证明机器人领域 "bigger is better" 的模型。</li>
<li><strong>开源友好</strong>: 提供完整的预训练权重和微调代码。</li>
</ul>
<h3>7. 实战代码示例 (Code Example)</h3>

<pre><code class="python">import torch
from transformers import AutoModel</p>

<p>model = AutoModel.from_pretrained("thu-ml/RDT-1B")</p>

<p>images = torch.randn(1, 2, 3, 224, 224)  # 双相机
language = "fold the towel in half"
proprio = torch.randn(1, 14)  # 双臂本体感知</p>

<p>with torch.no_grad():
    condition = model.encode_condition(images, language, proprio)
    actions = model.sample(condition, num_steps=10)  # [1, chunk_size, 14]</p>

<p>print(f"Generated actions shape: {actions.shape}")
</code></pre>

<h4>7.1 LoRA 微调示例</h4>

<pre><code class="python">from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader</p>

<p>lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj"],
    lora_dropout=0.05,
)</p>

<p>model_lora = get_peft_model(model, lora_config)
print(f"Trainable params: {model_lora.num_parameters(only_trainable=True):,}")</p>

<p>optimizer = torch.optim.AdamW(model_lora.parameters(), lr=1e-4)</p>

<p>for batch in dataloader:
    images, language, proprio, gt_actions = batch
    
    # 前向传播
    loss = model_lora.compute_loss(images, language, proprio, gt_actions)
    
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>

<h3>8. 面试常见问题 (Q&A)</h3>

<p><strong>Q1: RDT 和 Diffusion Policy 的核心区别是什么?</strong></p>

<p>A:
<ul><li><strong>架构</strong>: RDT 使用 <strong>DiT (Transformer)</strong>，Diffusion Policy 使用 <strong>U-Net</strong>。</li>
<li><strong>规模</strong>: RDT 是十亿参数级基础模型，Diffusion Policy 通常 < 100M。</li>
<li><strong>预训练</strong>: RDT 在大规模多机器人数据上预训练，Diffusion Policy 通常从头训练。</li>
<li><strong>跨形态</strong>: RDT 统一处理不同机器人，Diffusion Policy 需为每种形态单独训练。</li>
</ul>
<strong>Q2: 为什么 RDT 专门优化双臂操作?</strong></p>

<p>A:
<ul><li><strong>数据稀缺</strong>: 双臂数据远少于单臂，需要专门的数据增强。</li>
<li><strong>协调性</strong>: 双臂任务需要两只手<strong>同步配合</strong>，不能独立预测。</li>
<li><strong>工业需求</strong>: 双臂机器人是工业和家庭场景的主要形态。</li>
</ul>
<strong>Q3: RDT 的 Scaling Law 是如何验证的?</strong></p>

<p>A:
<ul><li>训练了 <strong>170M, 500M, 1B</strong> 三个规模的模型。</li>
<li>保持数据和训练步数相同，只改变模型大小。</li>
<li>结果: <strong>参数量 ↑ 10x → 成功率 ↑ 15-20%</strong>，呈对数线性关系。</li>
</ul>
<strong>Q4: RDT 如何处理不同机器人的动作维度?</strong></p>

<p>A:
<ul><li><strong>统一填充</strong>: 所有动作填充到最大维度 (如 32D)。</li>
<li><strong>动态掩码</strong>: 训练和推理时，只对有效维度计算损失/输出。</li>
<li><strong>Embodiment Token</strong>: 可选地添加"机器人类型"嵌入。</li>
</ul>
<strong>Q5: RDT vs π0，哪个更好?</strong></p>

<p>A:
<ul><li><strong>RDT 优势</strong>: 专门优化双臂，开源完整，Scaling Law 验证充分。</li>
<li><strong>π0 优势</strong>: VLM 底座更强 (语义理解)，Flow Matching 比 DDPM 更快。</li>
<li><strong>选择建议</strong>: 双臂任务选 RDT；需要强语义理解选 π0。</li>
</ul>
<h3>9. 局限性与未来方向 (Limitations & Future)</h3>

<h4>9.1 当前局限</h4>

<ul><li><strong>推理速度</strong>: 10 步 DDIM 仍需 ~100ms，难以满足某些高频控制需求。</li>
<li><strong>数据需求</strong>: 预训练需要大规模数据，新形态机器人可能缺乏数据。</li>
<li><strong>安全性</strong>: 扩散模型的随机性可能导致不安全动作。</li>
</ul>
<h4>9.2 未来方向</h4>

<ul><li><strong>一致性蒸馏 (Consistency Distillation)</strong>: 将去噪步数压缩到 1 步。</li>
<li><strong>人形机器人</strong>: 扩展到 30+ 自由度的人形机器人。</li>
<li><strong>在线学习</strong>: 结合 RL 进行实时适应。</li>
</ul>
<h3>10. 参考资源 (References)</h3>

<ul><li><strong>论文</strong>: <a href="https://arxiv.org/abs/2410.07864">RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</a></li>
<li><strong>GitHub</strong>: <a href="https://github.com/thu-ml/RoboticsDiffusionTransformer">thu-ml/RDT-1B</a></li>
<li><strong>Hugging Face</strong>: <a href="https://huggingface.co/thu-ml/RDT-1B">thu-ml/RDT-1B</a></li>
<li><strong>博客</strong>: <a href="https://mars-lab.github.io/">清华 MARS Lab 技术博客</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第25章 π0.5 解析</h2>

<blockquote><strong>发布时间</strong>: 2025年4月</blockquote>
<blockquote><strong>核心定位</strong>: 从"多面手" (Generalist) 进化为"探险家" (Open-World Explorer)。</blockquote>

<p>π0.5 是 Physical Intelligence 在 π0 基础上的重大迭代，旨在解决机器人学习中最大的痛点：<strong>泛化到从未见过的环境 (Open-World Generalization)</strong>。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    π0.5 统一架构 (Unified Model)                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   输入: 📷 图像 + 📝 语言指令                                    │
│              │                                                  │
│              ▼                                                  │
│   ┌─────────────────────────────────────────┐                  │
│   │          VLM Backbone (3B+)             │                  │
│   │      视觉-语言多模态编码器               │                  │
│   └─────────────────┬───────────────────────┘                  │
│                     │                                          │
│                     ▼                                          │
│   ┌─────────────────────────────────────────┐                  │
│   │     🧠 Latent Thought (隐式推理)         │                  │
│   │   "Approach handle" → "Grasp" → "Lift"  │                  │
│   │        (高层语义子任务规划)              │                  │
│   └─────────────────┬───────────────────────┘                  │
│                     │                                          │
│         ┌───────────┴───────────┐                              │
│         │                       │                              │
│         ▼                       ▼                              │
│   ┌───────────┐          ┌───────────┐                         │
│   │   FAST    │          │   Flow    │                         │
│   │ Tokenizer │          │ Matching  │                         │
│   │  (训练)   │          │  (推理)   │                         │
│   └───────────┘          └─────┬─────┘                         │
│                                │                               │
│                                ▼                               │
│                    🦾 连续动作输出 (50Hz)                        │
│                                                                 │
│   特点: 分层推理 + 混合架构 (离散训练 / 连续推理)                │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. 核心架构：统一模型 (The Unified Model)</h3>

<h4>1.1 "自言自语" 的大脑</h4>
传统的机器人系统通常是分层的：
<ul><li>High-level Planner (LLM): "去把苹果拿起来" -> 输出坐标。</li>
<li>Low-level Controller (Policy): 根据坐标执行动作。</li>
</ul>
<strong>π0.5 打破了这种边界</strong>。它是一个端到端的 VLA，但引入了 <strong>Hierarchical Inference (分层推理)</strong> 机制。
<ul><li><strong>输入</strong>: 图像 + 语言指令。</li>
<li><strong>中间输出 (Latent Thought)</strong>: 模型首先在内部生成一个高层的语义子任务 (Semantic Subtask)，例如 "Approach the handle" (靠近把手)。</li>
<li><strong>最终输出 (Action)</strong>: 基于这个子任务，生成具体的电机控制信号 (Flow Matching)。</li>
</ul>
这种机制让模型具备了<strong>可解释性</strong>和<strong>长期规划能力</strong>。它不仅仅是在模仿动作，而是在理解"意图"。</p>

<h4>1.2 混合架构 (Hybrid Architecture)</h4>
为了平衡推理效率和控制精度，π0.5 采用了一种混合策略：
<ul><li><strong>Pre-training (预训练)</strong>: 使用 <strong>FAST Tokenizer</strong> (离散化) 进行大规模预训练。离散 Token 训练速度快，容易扩展到海量数据。</li>
<li><strong>Inference (推理)</strong>: 在最后一步使用 <strong>Flow Matching</strong> (连续化) 进行微调和生成。</li>
<li><strong>优势</strong>: 既享受了 Tokenization 的训练效率，又保留了 Flow Matching 的控制平滑度。</li>
</ul>
<h3>2. 训练数据的艺术 (Data Strategy)</h3>

<p>π0.5 的强大泛化能力来自于其独特的训练数据配比。</p>

<h4>2.1 异构数据 Co-training</h4>
它不再仅仅依赖机器人数据 (Robot Data)，而是混合了三种数据源：
<li> <strong>Robot Data (OXE + 自研)</strong>: 高质量，含动作标签。用于学习物理控制。</li>
<li> <strong>Internet Videos (YouTube)</strong>: 海量，无动作标签。用于学习"世界模型" (World Model) —— 知道物体被推会动，水倒出来会流。</li>
<li> <strong>Simulation Data</strong>: 完美标注，但有 Reality Gap。用于学习长序列逻辑。</li></p>

<h4>2.2 Cross-Embodiment Alignment (跨形态对齐)</h4>
π0.5 能够控制双臂机器人、移动底盘、甚至四足机器人。
<ul><li><strong>统一动作空间</strong>: 将不同机器人的动作映射到一个共享的 <strong>Latent Action Space</strong>。</li>
<li><strong>效果</strong>: 你在一个单臂机器人上训练的"抓杯子"技能，可以 Zero-shot 迁移到双臂机器人上 (只需微调少量参数)。</li>
</ul>
<h3>3. 核心能力突破 (Capabilities)</h3>

<h4>3.1 开放世界泛化 (Open-World Generalization)</h4>
<ul><li><strong>场景</strong>: 把机器人扔到一个从未见过的厨房 (Airbnb)。</li>
<li><strong>表现</strong>: π0.5 能够识别出从未见过的咖啡机型号，并根据通用的"按按钮"知识尝试操作，而不是因为纹理不同而死机。</li>
<li><strong>原理</strong>: 这种能力来自于 VLM Backbone (3B -> 5B) 强大的视觉语义理解能力。</li>
</ul>
<h4>3.2 长序列任务 (Long-Horizon Tasks)</h4>
<ul><li><strong>任务</strong>: "把桌子收拾干净" (Bus the table)。</li>
<li><strong>分解</strong>: </li>
</ul>    1. 识别所有垃圾。
    2. 规划顺序 (先拿大的，再擦水的)。
    3. 执行动作。
<ul><li><strong>提升</strong>: 相比 π0，π0.5 在这种多阶段任务上的成功率提升了 40% 以上。</li>
</ul>
<h3>4. 与 π0 和 π0.6 的对比</h3>

<table>
<thead><tr>
<th>特性</th>
<th>π0 (Base)</th>
<th>π0.5 (Explorer)</th>
<th>π0.6 (Master)</th>
</tr></thead>
<tbody>
<tr>
<td><strong>核心关注</strong></td>
<td>基础控制，物理理解</td>
<td><strong>环境泛化，分层推理</strong></td>
<td>极致熟练度，自我进化</td>
</tr>
<tr>
<td><strong>架构</strong></td>
<td>Flow Matching</td>
<td><strong>Unified (FAST + Flow)</strong></td>
<td>Unified + <strong>Action Expert</strong></td>
</tr>
<tr>
<td><strong>训练方式</strong></td>
<td>BC (模仿学习)</td>
<td><strong>Co-training (Web + Sim)</strong></td>
<td><strong>Offline RL (Recap)</strong></td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>固定环境重复操作</td>
<td><strong>新环境探索，家务</strong></td>
<td>工厂流水线，高精度装配</td>
</tr>
</tbody></table>

<blockquote><strong>面试 Tip</strong>: 如果被问到 π0.5 的创新点，重点答 <strong>"Hierarchical Inference" (分层推理)</strong> 和 <strong>"Open-world Generalization" (开放世界泛化)</strong>。它是连接"通识大模型"和"物理执行器"的关键桥梁。</blockquote>

<hr></p>

<hr></p>

<h2>第26章 π0.6 解析</h2>

<blockquote><strong>发布时间</strong>: 2025年11月</blockquote>
<blockquote><strong>核心定位</strong>: 从"探险家" (Explorer) 进化为"大师" (Master)。<strong>Self-Improvement (自我进化)</strong> 是其灵魂。</blockquote>

<p>如果说 π0.5 解决了"去哪儿" (泛化) 的问题，那么 π0.6 则解决了"做得好" (熟练度) 的问题。</p>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                π0.6 / π*0.6 架构与训练流程                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                     ┌──────────────────┐                        │
│                     │  5B VLM Backbone │ ◀── 更大的语义理解     │
│                     │     (大脑)       │                        │
│                     └────────┬─────────┘                        │
│                              │                                  │
│                              ▼                                  │
│                     ┌──────────────────┐                        │
│                     │  Action Expert   │ ◀── 新增! 精细控制     │
│                     │     (小脑)       │     高频电机信号        │
│                     └────────┬─────────┘                        │
│                              │                                  │
│                              ▼                                  │
│                        🦾 精细动作                               │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                     π*0.6 Recap 训练流程                         │
│                                                                 │
│   Phase 1: 执行任务 ─────────▶ 收集数据 (✅ 成功 + ❌ 失败)       │
│                                    │                            │
│   Phase 2: 分析轨迹 ◀──────────────┘                            │
│              │                                                  │
│              ▼                                                  │
│   ┌─────────────────────────────────────┐                       │
│   │         Recap 算法 (Offline RL)     │                       │
│   │  ┌─────────────┐  ┌─────────────┐   │                       │
│   │  │ ✅ 成功轨迹 │  │ ❌ 失败轨迹 │   │                       │
│   │  │  奖励动作   │  │  抑制动作   │   │                       │
│   │  └─────────────┘  └─────────────┘   │                       │
│   └───────────────────┬─────────────────┘                       │
│                       │                                         │
│   Phase 3: 自我进化 ──┘ ──▶ 超越人类示教!                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. 核心架构升级 (Architecture Upgrades)</h3>

<h4>1.1 5B VLM Backbone</h4>
<ul><li><strong>规模</strong>: 从 π0 的 3B 升级到 <strong>5B 参数</strong>。</li>
<li><strong>意义</strong>: 更大的模型意味着更强的语义理解能力，能够处理更复杂的长指令 (e.g., "把那个红色的、有点破损的盒子折叠好放进箱子里")。</li>
</ul>
<h4>1.2 Action Expert (动作专家)</h4>
<ul><li><strong>问题</strong>: 通用大模型通常"手笨"，难以处理穿针引线、精密装配等微米级操作。</li>
<li><strong>解决</strong>: π0.6 引入了一个专门的 <strong>Action Expert</strong> 模块。</li>
</ul>    - 这是一个独立于 VLM 主干的小型高频网络。
    - 专门负责将 VLM 的高层意图转化为高频、高精度的电机控制信号。
    - 类似于人类的小脑 (负责运动控制) 与大脑 (负责思考) 的分工。</p>

<h3>2. π*0.6 (Pi-Star): 强化学习的引入</h3>

<p>π0.6 的最强形态是 <strong>π*0.6</strong>，它是经过 RL 强化后的版本。</p>

<h4>2.1 Recap 算法 (The Recap Algorithm)</h4>
这是 π*0.6 的核心秘密武器。
<ul><li><strong>传统方法 (BC)</strong>: 模仿人类示教。人类怎么做，机器人就怎么做。上限是人类水平。</li>
<li><strong>Recap (Offline RL)</strong>:</li>
</ul>    - <strong>复盘 (Recap)</strong>: 模型不仅学习成功的轨迹，还学习<strong>失败</strong>的轨迹。
    - <strong>机制</strong>:
        1. 机器人尝试执行任务，收集数据 (包含成功和失败)。
        2. 算法分析失败案例，标记"哪一步走错了"。
        3. 在下一次训练中，抑制导致失败的动作概率，奖励导致成功的动作。
    - <strong>效果</strong>: 就像人类练球一样，通过不断的试错和复盘，最终超越教练 (人类示教者)。</p>

<h4>2.2 性能飞跃 (Performance Leap)</h4>
<ul><li><strong>吞吐量 (Throughput)</strong>: 在折叠衣物、制作咖啡等任务上，π*0.6 的操作速度是 π0.5 的 <strong>2倍</strong>。</li>
<li><strong>鲁棒性</strong>: 失败率降低了 <strong>50%</strong> 以上。</li>
<li><strong>连续工作</strong>: 展示了连续工作 24 小时无故障的能力 (Making Espresso all day)。</li>
</ul>
<h3>3. 训练范式的转变 (Paradigm Shift)</h3>

<p>从 <strong>Supervised Learning (监督学习)</strong> 转向 <strong>Data-Driven Self-Improvement (数据驱动的自我进化)</strong>。</p>

<p><li> <strong>Phase 1: Pre-training</strong>: 使用海量互联网数据 + 机器人数据，训练出一个 Base Model (π0.6)。</li>
<li> <strong>Phase 2: Interaction</strong>: 机器人在仿真或真机中大量运行，收集经验。</li>
<li> <strong>Phase 3: Post-training (Recap)</strong>: 使用 Offline RL 算法在收集的数据上进行强化学习，得到 π*0.6。</li></p>

<h3>4. 总结：Pi 系列进化史</h3>

<table>
<thead><tr>
<th>模型</th>
<th>核心关键词</th>
<th>解决的问题</th>
<th>类似人类阶段</th>
</tr></thead>
<tbody>
<tr>
<td><strong>π0</strong></td>
<td>Flow Matching</td>
<td>怎么动？(基础控制)</td>
<td>婴儿学步</td>
</tr>
<tr>
<td><strong>π0.5</strong></td>
<td>Open-World</td>
<td>怎么适应？(环境泛化)</td>
<td>幼儿探索世界</td>
</tr>
<tr>
<td><strong>π0.6</strong></td>
<td>Action Expert</td>
<td>怎么做细？(精细操作)</td>
<td>技工学徒</td>
</tr>
<tr>
<td><strong>π*0.6</strong></td>
<td><strong>Recap (RL)</strong></td>
<td><strong>怎么做得更好？(自我进化)</strong></td>
<td><strong>行业大师</strong></td>
</tr>
</tbody></table>

<blockquote><strong>面试 Tip</strong>: 重点强调 <strong>Recap 算法</strong> 和 <strong>Offline RL</strong>。这是目前 Embodied AI 领域从"能用"走向"好用"的关键技术路径。</blockquote>

<hr></p>

<hr></p>

<h2>第27章 Galaxea G0</h2>

<blockquote>[!IMPORTANT]</blockquote>
<blockquote><strong>Galaxea G0</strong> 是<strong>星海图智能（Galaxea）</strong> 开发的<strong>双系统 VLA 框架</strong>，通过分离<strong>规划（VLM）</strong>和<strong>执行（VLA）</strong>，实现了更强的泛化能力和长时域任务处理能力。</blockquote>

<h3>1. 概述</h3>
Galaxea G0 是一个创新的双系统架构，旨在解决传统单一 VLA 模型在<strong>长时域移动操作</strong>（long-horizon mobile manipulation）任务中的泛化瓶颈。</p>

<ul><li>  <strong>开发者</strong>: 星海图智能（Galaxea）</li>
<li>  <strong>论文</strong>: <a href="https://arxiv.org/abs/2509.00576">Galaxea Open-World Dataset and G0 Dual-System VLA Model (arXiv:2509.00576)</a></li>
<li>  <strong>核心创新</strong>: <strong>双系统架构</strong> - 将高层规划和低层执行解耦</li>
<li>  <strong>数据集</strong>: Galaxea Open-World Dataset（500+ 小时，50 个真实场景）</li>
</ul>
<h3>2. 核心创新：双系统架构</h3>

<h4>2.1. 为什么需要双系统？</h4>
传统的单一 VLA 模型面临以下挑战：
<ul><li>  <strong>长时域任务</strong>: 需要同时处理高层规划（"先去客厅，再去厨房"）和低层控制（"抓取杯子"）。</li>
<li>  <strong>泛化瓶颈</strong>: 单一模型难以同时优化语义理解和精细操作。</li>
<li>  <strong>数据效率</strong>: 需要大量端到端数据，但长时域数据收集成本极高。</li>
</ul>
<h4>2.2. G0 的解决方案：分层解耦</h4>
Galaxea G0 采用<strong>两个独立但协作的模型</strong>：</p>

<pre><code class="">用户指令 → [ G0-VLM ] → 子任务序列 → [ G0-VLA ] → 机器人动作
          (规划大脑)                    (执行小脑)
</code></pre>

<h4>2.2.1. G0-VLM (Vision-Language Model)</h4>
<ul><li>  <strong>职责</strong>: 多模态规划，高层推理</li>
<li>  <strong>输入</strong>: 视觉观察 + 用户指令（如 "把客厅的杯子拿到厨房"）</li>
<li>  <strong>输出</strong>: 子任务序列（如 <code>["导航到客厅", "抓取杯子", "导航到厨房", "放下杯子"]</code>）</li>
<li>  <strong>优势</strong>: 保留 VLM 的强大语义理解和常识推理能力</li>
</ul>
<h4>2.2.2. G0-VLA (Vision-Language-Action Model)</h4>
<ul><li>  <strong>职责</strong>: 细粒度执行，低层控制</li>
<li>  <strong>输入</strong>: 当前子任务 + 视觉观察</li>
<li>  <strong>输出</strong>: 连续动作（关节角度、底盘速度）</li>
<li>  <strong>优势</strong>: 专注于精确的感知-动作映射，不被长时域任务分心</li>
</ul>
<h3>3. 训练策略：三阶段课程学习</h3>

<p>Galaxea G0 采用<strong>三阶段渐进式训练</strong>：</p>

<h4>阶段 1: 跨具身预训练 (Cross-Embodiment Pre-training)</h4>
<ul><li>  <strong>数据</strong>: Open X-Embodiment 等大规模多样化数据</li>
<li>  <strong>目标</strong>: 学习通用的世界知识和动作先验</li>
<li>  <strong>效果</strong>: 模型获得广泛的任务理解能力</li>
</ul>
<h4>阶段 2: 单具身预训练 (Single-Embodiment Pre-training)</h4>
<ul><li>  <strong>数据</strong>: <strong>Galaxea Open-World Dataset</strong>（同一机器人 R1 Lite，500+ 小时）</li>
<li>  <strong>目标</strong>: 适配特定机器人的感知-动作能力</li>
<li>  <strong>关键</strong>: 这是 G0 性能提升的<strong>核心阶段</strong></li>
</ul>
<h4>阶段 3: 任务后训练 (Task-Specific Post-training)</h4>
<ul><li>  <strong>数据</strong>: 高质量的特定任务演示</li>
<li>  <strong>目标</strong>: 精调复杂技能（如精细抓取、灵巧操作）</li>
<li>  <strong>效果</strong>: 最终性能优化</li>
</ul>
<h3>4. Galaxea Open-World Dataset</h3>

<p>这是 G0 的<strong>关键数据资产</strong>：</p>

<h4>4.1. 数据规模</h4>
<ul><li>  <strong>时长</strong>: 500+ 小时</li>
<li>  <strong>场景</strong>: 50 个真实环境（住宅、厨房、零售、办公室）</li>
<li>  <strong>任务</strong>: 150+ 种任务（移动、抓取、放置、导航）</li>
<li>  <strong>具身</strong>: 统一使用 Galaxea R1 Lite 机器人</li>
</ul>
<h4>4.2. 数据特点</h4>
<ul><li>  <strong>真实世界</strong>: 非模拟，真实的杂乱环境</li>
<li>  <strong>子任务标注</strong>: 精确的层级标注（任务 → 子任务 → 动作）</li>
<li>  <strong>语言富集</strong>: 每个子任务都有自然语言描述</li>
<li>  <strong>开源</strong>: 完全公开，可用于研究</li>
</ul>
<h4>4.3. 与其他数据集的对比</h4>
<table>
<thead><tr>
<th>数据集</th>
<th>时长</th>
<th>具身一致性</th>
<th>移动操作</th>
<th>子任务标注</th>
</tr></thead>
<tbody>
<tr>
<td><strong>Galaxea Open-World</strong></td>
<td><strong>500+ hr</strong></td>
<td><strong>单一机器人</strong></td>
<td><strong>✓</strong></td>
<td><strong>✓</strong></td>
</tr>
<tr>
<td>Open X-Embodiment</td>
<td>1000+ hr</td>
<td>多机器人</td>
<td>部分</td>
<td>✗</td>
</tr>
<tr>
<td>BridgeData</td>
<td>100+ hr</td>
<td>单一机器人</td>
<td>✗</td>
<td>✗</td>
</tr>
</tbody></table>

<h3>5. 性能表现</h3>

<p>G0 模型在多个基准测试中表现出色：</p>

<h4>5.1. 桌面操作 (Tabletop Manipulation)</h4>
<ul><li>  <strong>成功率</strong>: 显著优于单一 VLA 基线</li>
<li>  <strong>泛化</strong>: 对新物体和新场景泛化能力强</li>
</ul>
<h4>5.2. 少样本学习 (Few-Shot Learning)</h4>
<ul><li>  <strong>优势</strong>: 单具身预训练阶段使模型能快速适应新任务</li>
</ul>
<h4>5.3. 长时域移动操作 (Long-Horizon Mobile Manipulation)</h4>
<ul><li>  <strong>核心优势</strong>: 双系统架构在多步骤任务中表现突出</li>
<li>  <strong>示例</strong>: "从卧室拿遥控器到客厅，然后去厨房拿杯子" - 成功率 85%+</li>
</ul>
<h3>6. 双系统 vs 单系统</h3>
<table>
<thead><tr>
<th>特性</th>
<th><strong>双系统 (G0)</strong></th>
<th>单系统 (传统 VLA)</th>
</tr></thead>
<tbody>
<tr>
<td><strong>长时域任务</strong></td>
<td><strong>强（分层规划）</strong></td>
<td>弱（端到端困难）</td>
</tr>
<tr>
<td><strong>泛化能力</strong></td>
<td><strong>强（VLM 保留）</strong></td>
<td>中（易过拟合）</td>
</tr>
<tr>
<td><strong>数据效率</strong></td>
<td><strong>高（分阶段训练）</strong></td>
<td>低（需大量端到端数据）</td>
</tr>
<tr>
<td><strong>推理速度</strong></td>
<td>稍慢（两阶段）</td>
<td>快（一次推理）</td>
</tr>
<tr>
<td><strong>可解释性</strong></td>
<td><strong>强（子任务可见）</strong></td>
<td>弱（黑盒）</td>
</tr>
</tbody></table>

<h3>7. 面试要点</h3>
<ul><li>  <strong>双系统架构</strong>: 记住 G0-VLM（规划）+ G0-VLA（执行）的分工。</li>
<li>  <strong>三阶段训练</strong>: 跨具身 → 单具身（关键）→ 任务后训练。</li>
<li>  <strong>Galaxea Open-World Dataset</strong>: 500+ 小时，50 场景，统一具身，子任务标注。</li>
<li>  <strong>核心优势</strong>: 长时域移动操作，泛化能力强。</li>
<li>  <strong>与其他模型对比</strong>: </li>
</ul>    - vs Pi0: Pi0 是单一模型，G0 是双系统。
    - vs WALL-OSS: WALL-OSS 强调 Uni-CoT，G0 强调分层解耦。</p>

<h3>8. 参考资源</h3>
<ul><li>  <strong>论文</strong>: <a href="https://arxiv.org/abs/2509.00576">arXiv:2509.00576</a></li>
<li>  <strong>数据集</strong>: <a href="https://huggingface.co/datasets/Galaxea/Open-World-Dataset">Hugging Face - Galaxea Open-World Dataset</a></li>
<li>  <strong>GitHub</strong>: <a href="https://github.com/Galaxea/G0">Galaxea G0</a></li>
<li>  <strong>官网</strong>: <a href="https://galaxea-ai.com/">星海图智能</a></li>
</ul>

<hr></p>

<h2>第28章 WALL-OSS</h2>

<blockquote>[!IMPORTANT]</blockquote>
<blockquote><strong>WALL-OSS</strong> (World-Action-Language-Learning – Open Source System，世界-动作-语言-学习 开源系统) 是 <strong>X Square Robot (自变量机器人)</strong> 的重要开源贡献，旨在弥合静态 VLM 与动态物理交互之间的鸿沟。它被设计为具身 AI 的 "Linux 时刻"。</blockquote>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    WALL-OSS 架构 (Uni-CoT)                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   输入: 📷 图像 + 📝 "清洁桌子"                                  │
│              │                                                  │
│              ▼                                                  │
│   ┌─────────────────────────────────────────┐                  │
│   │       Qwen2.5 VLMoE Backbone            │                  │
│   │         (专家混合架构)                   │                  │
│   └─────────────────┬───────────────────────┘                  │
│                     │                                          │
│                     ▼                                          │
│   ┌─────────────────────────────────────────┐                  │
│   │           Uni-CoT 推理链                 │                  │
│   │  ┌─────────────────────────────────┐    │                  │
│   │  │ 1. 高层推理: 理解"清洁桌子"      │    │                  │
│   │  │         ↓                       │    │                  │
│   │  │ 2. 子任务分解: 找海绵→抓取→擦拭  │    │                  │
│   │  │         ↓                       │    │                  │
│   │  │ 3. 细粒度动作: (x,y,z) 轨迹      │    │                  │
│   │  └─────────────────────────────────┘    │                  │
│   └─────────────────┬───────────────────────┘                  │
│                     │                                          │
│         ┌───────────┴───────────┐                              │
│         │                       │                              │
│         ▼                       ▼                              │
│   ┌───────────┐          ┌───────────┐                         │
│   │ 离散头    │          │ 连续头    │                         │
│   │(FAST Token)│          │(Flow Match)│                        │
│   │  高层决策  │          │  平滑轨迹  │                         │
│   └─────┬─────┘          └─────┬─────┘                         │
│         │                      │                               │
│         └──────────┬───────────┘                               │
│                    ▼                                           │
│              🦾 双头动作输出                                     │
│                                                                 │
│   创新: 统一推理链 (Uni-CoT) = 大脑规划 + 小脑控制               │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h3>1. 概述</h3>
WALL-OSS 是一个端到端的具身基础模型，集成了<strong>视觉、语言和动作 (VLA)</strong>。与传统的将感知、规划和控制分离的流水线不同，WALL-OSS 将这些统一到一个可微分系统中。</p>

<ul><li>  <strong>开发者</strong>: X Square Robot (自变量机器人)</li>
<li>  <strong>发布日期</strong>: 2025 年</li>
<li>  <strong>核心目标</strong>: 使机器人能够理解世界 (World)、推理任务 (Language) 并执行复杂动作 (Action)，以统一的方式。</li>
<li>  <strong>开源</strong>: <a href="https://github.com/X-Square-Robot/wall-x">GitHub</a> | <a href="https://huggingface.co/X-Square-Robot">Hugging Face</a></li>
</ul>
<h3>2. 核心创新：Uni-CoT</h3>
WALL-OSS 的突出特点是<strong>统一跨层思维链 (Unified Cross-Level Chain-of-Thought, Uni-CoT)</strong>。</p>

<p>传统 LLM 中的 CoT (思维链) 专注于语义推理。WALL-OSS 将其扩展到物理领域：
<li> <strong>高层推理</strong>: 理解用户意图（例如："清洁桌子"）。</li>
<li> <strong>子任务分解</strong>: 将其分解（例如："找到海绵"、"抓取海绵"、"擦拭表面"）。</li>
<li> <strong>细粒度动作合成</strong>: 生成精确的关节角度和轨迹（例如："将手臂移动到 (x,y,z)，速度为 v"）。</li></p>

<p><strong>为什么重要</strong>: 这将 "大脑"（推理）和 "小脑"（控制）统一到一个连续链中，减少了计划与物理现实不匹配的 "模态解耦" 问题。</p>

<h3>3. 架构深度解析</h3>
WALL-OSS 采用<strong>紧密耦合的多模态架构</strong>，使用<strong>专家混合 (Mixture-of-Experts, MoE)</strong> 设计。</p>

<h4>3.1. 双输出头</h4>
为了处理语义规划（离散）和运动控制（连续）的不同性质，WALL-OSS 使用两个专门的头：
<ul><li>  <strong>离散动作头</strong>: 用于高层决策和 token 生成。</li>
<li>  <strong>连续动作头（流匹配）</strong>: 用于高频、平滑的运动控制。使用<strong>流匹配 (Flow Matching)</strong> 扩散技术生成精确的轨迹。</li>
</ul>
<h4>3.2. 任务路由 FFN 与共享注意力</h4>
模型使用共享注意力机制处理多模态输入（视觉 + 文本），但根据任务阶段（推理 vs. 执行）将信息路由到不同的前馈网络 (FFN)。这允许模型专业化，同时不丢失全局上下文。</p>

<h3>4. 训练策略</h3>
训练过程是一个<strong>两阶段流水线</strong>，旨在模仿人类学习：</p>

<p><li> <strong>启发阶段 (Inspiration Stage，对齐)</strong>:</li>
    -   重点: 将语义指令与离散动作先验对齐。
    -   目标: 确保机器人在空间和语义上 "知道要做什么"。
    -   数据: 包含离散动作 token 的大规模 VLA 数据集。</p>

<p><li> <strong>整合阶段 (Integration Stage，流匹配)</strong>:</li>
    -   重点: 针对高频连续控制进行微调。
    -   技术: 使用<strong>流匹配 (Flow Matching)</strong>（比标准扩散更高效的替代方案）生成平滑、物理上可行的轨迹。
    -   目标: 确保机器人 "知道如何平滑移动"。</p>

<h3>5. 数据策略：Wall-80k</h3>
X Square Robot 发布了 <strong>Wall-80k</strong>，一个对训练 WALL-OSS 至关重要的高质量数据集。
<ul><li>  <strong>规模</strong>: 80,000+ 条轨迹。</li>
<li>  <strong>格式</strong>: 与 <strong>LeRobot</strong>（Hugging Face 标准）兼容。</li>
<li>  <strong>组成</strong>: 真实世界遥操作数据和高质量模拟数据的混合。</li>
<li>  <strong>增强</strong>: 使用生成视频技术增强训练数据，提高对新环境的泛化能力。</li>
</ul>
<h3>6. 性能与对比</h3>
<table>
<thead><tr>
<th>特性</th>
<th>WALL-OSS</th>
<th>RT-2 (Google)</th>
<th>OpenVLA</th>
</tr></thead>
<tbody>
<tr>
<td><strong>架构</strong></td>
<td>MoE + 流匹配</td>
<td>VLM + Token 化动作</td>
<td>VLM + L1 回归</td>
</tr>
<tr>
<td><strong>推理</strong></td>
<td><strong>Uni-CoT (强)</strong></td>
<td>CoT (仅语义)</td>
<td>标准</td>
</tr>
<tr>
<td><strong>控制</strong></td>
<td><strong>连续（平滑）</strong></td>
<td>离散 Token（抖动）</td>
<td>连续</td>
</tr>
<tr>
<td><strong>开源</strong></td>
<td><strong>是（全栈）</strong></td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td><strong>数据</strong></td>
<td>Wall-80k（公开）</td>
<td>专有</td>
<td>Open X-Embodiment</td>
</tr>
</tbody></table>

<h3>7. 面试要点</h3>
<ul><li>  <strong>Uni-CoT 是关键</strong>: 记住 "统一跨层思维链"。它连接了高层规划和低层控制。</li>
<li>  <strong>流匹配控制</strong>: 使用流匹配（而不仅仅是简单的扩散或回归）生成平滑动作。</li>
<li>  <strong>MoE 架构</strong>: 使用专家混合处理视觉、语言和动作处理的不同需求。</li>
<li>  <strong>数据为中心</strong>: 强调 Wall-80k 数据集和与 LeRobot 生态系统的兼容性。</li>
</ul>

<hr></p>

<h1>第七部分：评估与推理</h1>

<hr></p>

<h2>第29章 Chain-of-Thought 推理</h2>

<blockquote><strong>核心概念</strong>: 思维链 (Chain-of-Thought, CoT) 是一种让大模型在给出最终答案前，先生成<strong>中间推理步骤</strong>的技术。在 VLA 领域，CoT 使机器人能够进行复杂任务规划和可解释的决策。</blockquote>

<h3>1. 为什么 VLA 需要 CoT? (Why CoT for VLA?)</h3>

<h4>1.1 传统 VLA 的局限</h4>

<p>传统 VLA 模型是<strong>端到端</strong>的：
<pre><code class="math">\text{Observation} + \text{Instruction} \xrightarrow{\text{VLA}} \text{Action}
</code></pre>

<p><strong>问题</strong>:
<ul><li><strong>黑箱决策</strong>: 无法理解机器人为什么这样做</li>
<li><strong>长序列任务困难</strong>: 复杂任务需要多步规划</li>
<li><strong>错误难以诊断</strong>: 出错时不知道哪步推理有问题</li>
<li><strong>泛化能力弱</strong>: 缺乏显式的推理能力</li>
</ul>
<h4>1.2 CoT 的价值</h4>

<pre><code class="math">\text{Observation} + \text{Instruction} \xrightarrow{\text{VLA}} \underbrace{\text{Reasoning Steps}}_{\text{思维链}} \xrightarrow{} \text{Action}
</code></pre>

<ul><li><strong>可解释性</strong>: 可以追溯机器人的"思考过程"</li>
<li><strong>复杂任务分解</strong>: 自动将长任务拆解为子任务</li>
<li><strong>错误纠正</strong>: 可以在推理过程中发现并纠正错误</li>
<li><strong>泛化增强</strong>: 显式推理有助于处理新场景</li>
</ul>
<h4>1.3 VLA 中的 CoT 应用案例</h4>

<table>
<thead><tr>
<th>模型</th>
<th>CoT 类型</th>
<th>应用</th>
</tr></thead>
<tbody>
<tr>
<td><strong>RT-2</strong></td>
<td>语言 CoT</td>
<td>"first pick up the red cup, then place it..."</td>
</tr>
<tr>
<td><strong>WALL-OSS</strong></td>
<td>Uni-CoT</td>
<td>统一的视觉-语言-动作推理</td>
</tr>
<tr>
<td><strong>Galaxea G0</strong></td>
<td>分层 CoT</td>
<td>VLM 规划 + VLA 执行</td>
</tr>
<tr>
<td><strong>π0.5</strong></td>
<td>隐式 CoT</td>
<td>Latent Thought (隐空间推理)</td>
</tr>
</tbody></table>

<h3>2. CoT 的基本形式 (CoT Formulations)</h3>

<h4>2.1 显式语言 CoT (Explicit Language CoT)</h4>

<p>机器人用自然语言描述推理过程。</p>

<pre><code class="">输入:
<ul><li>图像: 厨房场景，桌上有苹果、香蕉、刀</li>
<li>指令: "帮我切一个水果"</li>
</ul>
CoT 输出:
"让我分析这个任务:
<li>首先，我需要识别桌上的水果 - 我看到了苹果和香蕉</li>
<li>苹果比较适合切片，我选择苹果</li>
<li>切水果需要刀，刀在桌子右侧</li>
<li>执行步骤:</li>
   - 步骤1: 移动到刀的位置，抓取刀
   - 步骤2: 移动到苹果位置
   - 步骤3: 放下刀，抓取苹果固定
   - 步骤4: 用刀切割苹果
开始执行步骤1..."</p>

<p>动作输出: [x, y, z, roll, pitch, yaw, gripper]
</code></pre>

<h4>2.2 结构化 CoT (Structured CoT)</h4>

<p>用结构化格式（JSON/XML）表示推理。</p>

<pre><code class="python">class StructuredCoT:
    def generate(self, obs, instruction):
        # 生成结构化推理
        cot = {
            "scene_understanding": {
                "objects": ["apple", "banana", "knife"],
                "spatial_relations": {
                    "knife": "right of table",
                    "apple": "center of table"
                }
            },
            "task_decomposition": [
                {"subtask": "grasp_knife", "target": "knife"},
                {"subtask": "grasp_apple", "target": "apple"},
                {"subtask": "cut", "object": "apple", "tool": "knife"}
            ],
            "current_step": 0,
            "reasoning": "Need to first get the knife before cutting"
        }
        return cot
</code></pre>

<h4>2.3 隐式 CoT (Implicit/Latent CoT)</h4>

<p>在隐空间中进行推理，不生成显式文本。</p>

<pre><code class="python">class LatentCoT(nn.Module):
    """π0.5 风格的隐式推理"""
    def __init__(self, vlm, action_decoder, num_thought_tokens=32):
        super().__init__()
        self.vlm = vlm
        self.action_decoder = action_decoder
        self.thought_tokens = nn.Parameter(torch.randn(num_thought_tokens, hidden_dim))
    
    def forward(self, obs, instruction):
        # 编码观测和指令
        context = self.vlm.encode(obs, instruction)
        
        # 在隐空间"思考" - 通过 Transformer 层迭代
        thoughts = self.thought_tokens.unsqueeze(0).expand(batch_size, -1, -1)
        
        for layer in self.reasoning_layers:
            # 思考 tokens 与 context 交互
            thoughts = layer(thoughts, context)  # Cross-Attention
        
        # 从思考结果解码动作
        actions = self.action_decoder(thoughts)
        return actions
</code></pre>

<h4>2.4 交错 CoT (Interleaved CoT)</h4>

<p>推理和动作生成交错进行。</p>

<pre><code class="">时刻 t0: [观测] → [推理: "我看到桌上有杯子"] → [动作: 伸手]
时刻 t1: [观测] → [推理: "手接近杯子了"] → [动作: 张开夹爪]
时刻 t2: [观测] → [推理: "夹爪对准杯子"] → [动作: 抓取]
...
</code></pre>

<pre><code class="python">class InterleavedCoT:
    def step(self, obs, history):
        # 基于历史上下文生成推理
        reasoning = self.vlm.generate_text(
            prompt=f"历史: {history}\n当前观测: {obs}\n下一步分析: "
        )
        
        # 基于推理生成动作
        action = self.action_head(
            self.encode(obs, reasoning)
        )
        
        # 更新历史
        history.append({"reasoning": reasoning, "action": action})
        
        return action, reasoning
</code></pre>

<h3>3. VLA 中的 CoT 架构 (CoT Architectures in VLA)</h3>

<h4>3.1 WALL-OSS 的 Uni-CoT</h4>
<blockquote><strong>延伸阅读</strong>: WALL-OSS 的 Unified CoT 架构对应 <code>theory/wall_oss.md</code> 中的 Uni-CoT + Dual Heads 设计。</blockquote>

<pre><code class="">┌─────────────────────────────────────────────────────────────┐
│                      Unified CoT                            │
│                                                             │
│   输入: [图像 Tokens] + [语言 Tokens]                        │
│              │                                              │
│              ▼                                              │
│      ┌───────────────────┐                                  │
│      │   VLM Backbone    │                                  │
│      │   (Qwen2.5-VL)    │                                  │
│      └─────────┬─────────┘                                  │
│                │                                            │
│                ▼                                            │
│      ┌───────────────────┐                                  │
│      │  思维链生成器      │◀── "First I need to..."         │
│      │  (CoT Generator)  │                                  │
│      └─────────┬─────────┘                                  │
│                │                                            │
│         ┌──────┴──────┐                                     │
│         │             │                                     │
│         ▼             ▼                                     │
│   ┌──────────┐  ┌──────────┐                               │
│   │ Flow Head│  │FAST Head │                               │
│   │ (精细)   │  │ (高效)   │                               │
│   └──────────┘  └──────────┘                               │
└─────────────────────────────────────────────────────────────┘
</code></pre>

<h4>3.2 Galaxea G0 的分层 CoT</h4>
<blockquote><strong>延伸阅读</strong>: Galaxea G0 的双系统结构与 <code>theory/galaxea_g0.md</code> 中的星海图模型详解一致。</blockquote>

<pre><code class="">┌─────────────────────────────────────────────────────────────┐
│                   G0-VLM (大脑)                              │
│   - 场景理解                                                 │
│   - 任务规划                                                 │
│   - 生成子任务序列                                           │
└──────────────────────────┬──────────────────────────────────┘
                           │ 子任务: "pick up cup"
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                   G0-VLA (小脑)                              │
│   - 接收子任务                                               │
│   - 生成具体动作                                             │
│   - 高频控制执行                                             │
└─────────────────────────────────────────────────────────────┘
</code></pre>

<pre><code class="python">class GalaxeaG0:
    def __init__(self):
        self.vlm = G0_VLM()  # 大脑: 规划
        self.vla = G0_VLA()  # 小脑: 执行
    
    def execute(self, obs, instruction):
        # 大脑: 生成任务规划 (CoT)
        plan = self.vlm.plan(obs, instruction)
        # plan = ["locate cup", "approach cup", "grasp cup", "lift cup"]
        
        # 逐步执行
        for subtask in plan:
            while not subtask.is_completed(obs):
                # 小脑: 生成具体动作
                action = self.vla.act(obs, subtask)
                obs = env.step(action)
        
        return "Task completed"
</code></pre>

<h3>4. CoT 训练方法 (Training CoT for VLA)</h3>

<h4>4.1 人工标注 CoT</h4>

<pre><code class="python">cot_dataset = [
    {
        "observation": "kitchen_scene_001.jpg",
        "instruction": "make a sandwich",
        "cot": [
            "I see bread, cheese, lettuce on the table",
            "To make a sandwich, I need to: 1) pick up bread 2) add cheese 3) add lettuce 4) close sandwich",
            "Starting with step 1: picking up bread"
        ],
        "action": [0.1, 0.2, 0.3, 0, 0, 0, 1]  # 抓面包动作
    },
    # ... more examples
]
</code></pre>

<h4>4.2 自动生成 CoT (使用 GPT-4V)</h4>

<pre><code class="python">def generate_cot_labels(image, instruction, action_sequence):
    """使用 GPT-4V 自动生成 CoT 标签"""
    prompt = f"""
    你是一个机器人任务分析专家。
    
    任务: {instruction}
    [图像已附加]
    
    机器人执行了以下动作序列: {action_sequence}
    
    请生成机器人的思维链，描述:
    1. 场景理解
    2. 任务分解
    3. 每步动作的原因
    
    格式:
    思考: [你的推理]
    """
    
    cot = gpt4v.generate(prompt, image)
    return cot
</code></pre>

<h4>4.3 CoT 蒸馏 (CoT Distillation)</h4>

<p>从大模型蒸馏 CoT 能力到小模型。</p>

<pre><code class="python">class CoTDistillation:
    def __init__(self, teacher_vlm, student_vla):
        self.teacher = teacher_vlm  # GPT-4V / Claude
        self.student = student_vla  # 目标部署模型
    
    def generate_training_data(self, obs, instruction):
        # 教师生成 CoT
        teacher_cot = self.teacher.generate_cot(obs, instruction)
        teacher_action = self.teacher.generate_action(obs, instruction, teacher_cot)
        
        return {
            "obs": obs,
            "instruction": instruction,
            "cot": teacher_cot,
            "action": teacher_action
        }
    
    def train_student(self, data):
        # 训练学生模型同时预测 CoT 和动作
        pred_cot, pred_action = self.student(data['obs'], data['instruction'])
        
        # CoT 损失 (语言建模)
        cot_loss = self.language_loss(pred_cot, data['cot'])
        
        # 动作损失
        action_loss = F.mse_loss(pred_action, data['action'])
        
        return cot_loss + action_loss
</code></pre>

<h3>5. CoT 的关键技术 (Key Techniques)</h3>

<h4>5.1 Self-Consistency (自一致性)</h4>

<p>生成多个 CoT，投票选择最一致的结果。</p>

<pre><code class="python">def self_consistency_cot(model, obs, instruction, num_samples=5):
    """生成多个 CoT，选择最一致的结果"""
    cots_and_actions = []
    
    for _ in range(num_samples):
        cot = model.generate_cot(obs, instruction, temperature=0.7)
        action = model.generate_action(obs, instruction, cot)
        cots_and_actions.append((cot, action))
    
    # 动作聚类
    actions = [a for _, a in cots_and_actions]
    clusters = cluster_actions(actions, threshold=0.1)
    
    # 选择最大聚类的代表
    largest_cluster = max(clusters, key=len)
    best_action = np.mean(largest_cluster, axis=0)
    
    return best_action
</code></pre>

<h4>5.2 Tree-of-Thoughts (思维树)</h4>

<p>探索多个推理分支，选择最优路径。</p>

<pre><code class="python">class TreeOfThoughts:
    def __init__(self, model, branching_factor=3, depth=3):
        self.model = model
        self.branching_factor = branching_factor
        self.depth = depth
    
    def search(self, obs, instruction):
        root = ThoughtNode(obs, instruction, thought="", score=0)
        
        # BFS/DFS 搜索思维树
        frontier = [root]
        
        for d in range(self.depth):
            new_frontier = []
            for node in frontier:
                # 扩展: 生成多个可能的下一步思考
                children = self.expand(node)
                
                # 评估: 对每个思考打分
                for child in children:
                    child.score = self.evaluate(child)
                
                new_frontier.extend(children)
            
            # 剪枝: 保留 top-k
            frontier = sorted(new_frontier, key=lambda n: n.score, reverse=True)
            frontier = frontier[:self.branching_factor]
        
        # 返回最佳路径
        best_node = max(frontier, key=lambda n: n.score)
        return best_node.get_action()
    
    def expand(self, node):
        """生成多个候选思考"""
        prompt = f"当前思考: {node.thought}\n可能的下一步: "
        thoughts = self.model.generate_multiple(prompt, n=self.branching_factor)
        return [ThoughtNode(node.obs, node.instruction, t) for t in thoughts]
    
    def evaluate(self, node):
        """评估思考的质量"""
        prompt = f"思考: {node.thought}\n这个思考对于完成'{node.instruction}'有多大帮助? (0-10分)"
        score = self.model.generate(prompt)
        return float(score)
</code></pre>

<h4>5.3 ReAct (Reasoning + Acting)</h4>

<p>交替进行推理和行动。</p>

<pre><code class="python">class ReAct:
    """Reasoning and Acting 交替执行"""
    
    def execute(self, env, instruction, max_steps=20):
        obs = env.reset()
        history = []
        
        for step in range(max_steps):
            # Thought: 推理当前应该做什么
            thought = self.reason(obs, instruction, history)
            print(f"Thought: {thought}")
            
            # Action: 基于推理执行动作
            action = self.act(obs, thought)
            print(f"Action: {action}")
            
            # Observation: 获取新观测
            obs, reward, done, info = env.step(action)
            print(f"Observation: {info['description']}")
            
            history.append({
                "thought": thought,
                "action": action,
                "observation": info['description']
            })
            
            if done:
                break
        
        return history
    
    def reason(self, obs, instruction, history):
        prompt = f"""
        任务: {instruction}
        历史: {history}
        当前观测: {obs}
        
        思考: 下一步我应该做什么?
        """
        return self.model.generate(prompt)
    
    def act(self, obs, thought):
        prompt = f"""
        当前思考: {thought}
        当前观测: {obs}
        
        执行动作 (格式: [x, y, z, roll, pitch, yaw, gripper]):
        """
        action_str = self.model.generate(prompt)
        return parse_action(action_str)
</code></pre>

<h3>6. CoT 的评估 (Evaluating CoT)</h3>

<h4>6.1 推理质量评估</h4>

<pre><code class="python">def evaluate_cot_quality(cot, ground_truth_steps):
    """评估 CoT 推理质量"""
    metrics = {}
    
    # 1. 步骤覆盖率: CoT 是否包含所有必要步骤
    covered = sum(1 for gt in ground_truth_steps if gt in cot)
    metrics['step_coverage'] = covered / len(ground_truth_steps)
    
    # 2. 逻辑一致性: 步骤之间是否有逻辑矛盾
    metrics['logical_consistency'] = check_consistency(cot)
    
    # 3. 可执行性: 推理是否能转化为有效动作
    metrics['executability'] = check_executability(cot)
    
    return metrics
</code></pre>

<h4>6.2 任务成功率对比</h4>

<table>
<thead><tr>
<th>方法</th>
<th>CALVIN 成功率</th>
<th>长序列任务</th>
<th>新场景泛化</th>
</tr></thead>
<tbody>
<tr>
<td><strong>无 CoT</strong></td>
<td>65%</td>
<td>35%</td>
<td>40%</td>
</tr>
<tr>
<td><strong>语言 CoT</strong></td>
<td>72%</td>
<td>52%</td>
<td>55%</td>
</tr>
<tr>
<td><strong>结构化 CoT</strong></td>
<td>75%</td>
<td>58%</td>
<td>60%</td>
</tr>
<tr>
<td><strong>分层 CoT (G0)</strong></td>
<td><strong>78%</strong></td>
<td><strong>65%</strong></td>
<td><strong>62%</strong></td>
</tr>
</tbody></table>

<h3>7. 面试高频问题 (Q&A)</h3>

<p><strong>Q1: CoT 会增加推理延迟，如何解决?</strong></p>

<p>A:
<ul><li><strong>隐式 CoT</strong>: 在隐空间推理，不生成文本 (π0.5)</li>
<li><strong>蒸馏</strong>: 将 CoT 能力蒸馏到无 CoT 的小模型</li>
<li><strong>缓存</strong>: 对常见场景预计算 CoT</li>
<li><strong>并行</strong>: 推理和低层控制并行执行</li>
</ul>
<strong>Q2: 显式 CoT 和隐式 CoT 哪个更好?</strong></p>

<p>A:
<ul><li><strong>显式 CoT</strong>: 可解释性强，便于调试，但速度慢</li>
<li><strong>隐式 CoT</strong>: 速度快，但黑箱</li>
<li><strong>选择</strong>: 研究/调试用显式，部署用隐式</li>
</ul>
<strong>Q3: 如何让 VLA 学会 CoT?</strong></p>

<p>A:
<ul><li><strong>方法 1</strong>: 人工标注 CoT 数据 (昂贵但质量高)</li>
<li><strong>方法 2</strong>: 使用 GPT-4V 自动生成 CoT 标签</li>
<li><strong>方法 3</strong>: 从有 CoT 的大模型蒸馏到小模型</li>
<li><strong>方法 4</strong>: RL 强化学习 CoT 策略</li>
</ul>
<strong>Q4: CoT 的"幻觉"问题如何处理?</strong></p>

<p>A:
<ul><li><strong>Grounding</strong>: 强制 CoT 与视觉观测对齐</li>
<li><strong>Self-Consistency</strong>: 多次采样投票</li>
<li><strong>验证器</strong>: 训练一个模型检验 CoT 合理性</li>
<li><strong>人机协作</strong>: 关键步骤让人类确认</li>
</ul>
<strong>Q5: WALL-OSS 的 Uni-CoT 有什么特别之处?</strong></p>

<p>A:
<ul><li><strong>统一</strong>: 视觉、语言、动作在同一个 CoT 框架下推理</li>
<li><strong>双头</strong>: Flow (精细) + FAST (高效) 双输出</li>
<li><strong>可解释</strong>: 可以输出推理过程供人审查</li>
<li><strong>泛化</strong>: CoT 帮助处理未见过的任务</li>
</ul>
<h3>8. 代码示例：简单的 CoT VLA</h3>

<pre><code class="python">class CoTVLA(nn.Module):
    def __init__(self, vlm_backbone, action_dim=7):
        super().__init__()
        self.vlm = vlm_backbone
        self.cot_head = nn.Linear(vlm.hidden_size, vlm.vocab_size)
        self.action_head = nn.Linear(vlm.hidden_size, action_dim)
    
    def forward(self, obs, instruction, generate_cot=True):
        # 编码观测和指令
        context = self.vlm.encode(obs, instruction)
        
        if generate_cot:
            # 生成思维链
            cot_tokens = []
            for _ in range(max_cot_length):
                logits = self.cot_head(context[:, -1, :])
                next_token = logits.argmax(-1)
                cot_tokens.append(next_token)
                
                # 将新 token 加入上下文
                token_emb = self.vlm.embed(next_token)
                context = torch.cat([context, token_emb.unsqueeze(1)], dim=1)
                
                if next_token == eos_token:
                    break
            
            cot_text = self.vlm.decode(cot_tokens)
        else:
            cot_text = None
        
        # 基于 CoT 增强的上下文生成动作
        action = self.action_head(context[:, -1, :])
        
        return action, cot_text</p>

<p>model = CoTVLA(vlm_backbone)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)</p>

<p>for batch in dataloader:
    pred_action, pred_cot = model(batch['obs'], batch['instruction'])
    
    # CoT 语言损失
    cot_loss = F.cross_entropy(pred_cot, batch['cot_labels'])
    
    # 动作损失
    action_loss = F.mse_loss(pred_action, batch['action'])
    
    loss = cot_loss + action_loss
    loss.backward()
    optimizer.step()
</code></pre>

<h3>9. 参考资源 (References)</h3>

<ul><li><strong>CoT Prompting</strong>: <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning</a></li>
<li><strong>ReAct</strong>: <a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting</a></li>
<li><strong>Tree of Thoughts</strong>: <a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with LLMs</a></li>
<li><strong>WALL-OSS</strong>: <a href="https://github.com/xsquare-robot">X Square Robot Official</a></li>
<li><strong>RT-2 CoT</strong>: <a href="https://arxiv.org/abs/2307.15818">RT-2: Vision-Language-Action Models</a></li>
</ul>
<hr></p>

<hr></p>

<h2>第30章 评估方法论</h2>

<p>在 VLA 领域，"怎么算做好了"是一个极其复杂的问题。与 CV/NLP 不同，机器人没有静态的 Test Set，必须在动态环境中评估。本章深入探讨评估的数学定义、基准细节及实战协议。</p>

<h3>1. 核心评估指标 (Core Metrics)</h3>

<h4>1.1. Success Rate (SR) - 成功率</h4>
最直观但也最粗糙的指标。
<pre><code class="math">SR = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(\text{task}_i \text{ completed})
</code></pre>
<ul><li><strong>定义</strong>: $N$ 次尝试中成功的次数。</li>
<li><strong>置信区间 (Confidence Interval)</strong>: 由于 $N$ 通常较小 (真机实验昂贵)，SR 的方差很大。建议使用 <strong>Wald Interval</strong> 或 <strong>Wilson Score Interval</strong> 报告误差范围。</li>
</ul>  $$ \hat{p} \pm z \sqrt{\frac{\hat{p}(1-\hat{p})}{N}} $$
  (当 $N=20, \hat{p}=0.5$ 时，误差范围高达 $\pm 22\%$！所以真机评估至少要跑 50 次以上才具备统计意义。)</p>

<h4>1.2. Mean Steps to Success (MSS) - 平均成功步数</h4>
衡量效率的指标。
<ul><li><strong>定义</strong>: 在成功的 Episodes 中，平均消耗的时间步数。</li>
<li><strong>意义</strong>: 两个模型 SR 都是 100%，但模型 A 用了 5 秒，模型 B 用了 10 秒，显然 A 更好 (更流畅，犹豫更少)。</li>
</ul>
<h4>1.3. Intervention Rate (IR) - 干预率</h4>
适用于长程任务 (Long-horizon) 或自动驾驶。
<pre><code class="math">IR = \frac{\text{Total Interventions}}{\text{Total Operation Time (hours)}} \quad \text{or} \quad \frac{\text{Interventions}}{\text{Total Steps}}
</code></pre>
<ul><li><strong>定义</strong>: 人类专家为了防止灾难性后果 (碰撞、掉落) 而接管机器人的频率。</li>
<li><strong>MPI (Miles Per Intervention)</strong>: 自动驾驶常用，VLA 中对应 <strong>Steps Per Intervention (SPI)</strong>。</li>
</ul>
<h4>1.4. Executable Rate (ER) - 可执行率</h4>
衡量模型的<strong>安全性</strong>和<strong>运动学一致性</strong>。
<ul><li><strong>定义</strong>: 模型生成的 Action 能够被底层控制器 (IK Solver / Impedance Controller) 执行的比例。</li>
<li><strong>常见失败</strong>:</li>
</ul>    - <strong>IK No Solution</strong>: 目标点超出工作空间。
    - <strong>Self-Collision</strong>: 机械臂自碰撞。
    - <strong>Singularity</strong>: 奇异点。</p>

<hr></p>

<h3>2. 仿真基准深度解析 (Simulation Benchmarks)</h3>

<h4>2.1. CALVIN (Computer Vision and Language for Interaction)</h4>
<blockquote><strong>核心能力</strong>: <strong>Long-horizon Language Following</strong> (长序列语义跟随)。</blockquote>

<ul><li><strong>任务</strong>: 连续完成 5 个指令 (e.g., "Open drawer" -> "Pick up block" -> "Place in drawer" -> "Turn on light" -> "Close drawer")。</li>
<li><strong>评估方式</strong>: <strong>Chain Success Rate</strong>。</li>
</ul>    - 1-step SR: 完成第 1 个任务的概率。
    - 5-step SR: 连续完成 5 个任务的概率 (极难，SOTA 通常 < 10%)。
<ul><li><strong>意义</strong>: 测试 VLA 模型的<strong>状态保持能力</strong> (Stateful) 和<strong>上下文理解能力</strong>。</li>
</ul>
<h4>2.2. SIMPLER (Sim-to-Real Evaluation)</h4>
<blockquote><strong>核心能力</strong>: <strong>Sim-to-Real Correlation</strong> (仿真与真机的相关性)。</blockquote>

<ul><li><strong>痛点</strong>: 以前我们在仿真里跑分高，真机一塌糊涂。</li>
<li><strong>创新</strong>: SIMPLER 使用真实视频作为纹理，并精细调节物理参数，使得仿真中的排名 (Rank) 与真机排名高度正相关 (Pearson Correlation > 0.8)。</li>
<li><strong>用途</strong>: 在上真机前，先用 SIMPLER 筛选 Checkpoint，节省昂贵的真机测试时间。</li>
</ul>
<h4>2.3. ManiSkill 2/3</h4>
<blockquote><strong>核心能力</strong>: <strong>Generalizable Manipulation</strong> (泛化抓取)。</blockquote>

<ul><li><strong>引擎</strong>: SAPIEN (PhysX)。</li>
<li><strong>特点</strong>: 包含 2000+ 种不同的物体 (PartNet Mobility)，每个物体都有不同的拓扑结构 (不同的门把手、不同的水龙头)。</li>
<li><strong>评估</strong>: 测试模型在 <strong>Unseen Object Instances</strong> 上的 Zero-shot 成功率。</li>
</ul>
<hr></p>

<h3>3. 真机评估协议 (Real World Protocols)</h3>

<p>真机评估是“玄学”的重灾区。为了保证公平，必须遵循严格的协议。</p>

<h4>3.1. 变量控制 (Variable Control)</h4>
<ul><li><strong>物体位置</strong>:</li>
</ul>    - <strong>Fixed</strong>: 每次都放在完全相同的位置 (测试记忆能力)。
    - <strong>Randomized</strong>: 在 $10cm \times 10cm$ 的区域内随机放置 (测试泛化能力)。
<ul><li><strong>干扰项 (Distractors)</strong>:</li>
</ul>    - 场景中是否包含与任务无关的物体？(测试抗干扰能力)。
<ul><li><strong>光照</strong>:</li>
</ul>    - 固定光源 vs 自然光变化。</p>

<h4>3.2. A/B Testing</h4>
在真机上对比模型 A 和 B 时，必须交替进行 (Interleaved)，以消除环境随时间变化 (e.g., 电机发热、光照变化) 的影响。
<ul><li><strong>错误做法</strong>: 上午测模型 A (50次)，下午测模型 B (50次)。</li>
<li><strong>正确做法</strong>: A, B, A, B, ... 交替测试。</li>
</ul>
<h4>3.3. Reset-Free Evaluation</h4>
<ul><li><strong>定义</strong>: 机器人完成任务后，自动执行“复位”动作，或者下一个任务的初始状态就是上一个任务的结束状态。</li>
<li><strong>意义</strong>: 实现 24/7 无人值守的自动化评估 (Scale Up Evaluation)。</li>
</ul>
<hr></p>

<h3>4. 模型选择策略 (Checkpoint Selection)</h3>

<p>在 VLA 训练中，Loss 不代表一切。</p>

<h4>4.1. 为什么 Loss 失效？</h4>
<ul><li><strong>多模态分布</strong>: 比如面对障碍物，向左走和向右走都是对的。MSE Loss 会让模型走中间 (撞墙)，Loss 可能不降反升，但策略其实变好了 (学会了多模态)。</li>
<li><strong>过拟合</strong>: 模型可能记住了训练集里的特定噪声。</li>
</ul>
<h4>4.2. EMA (Exponential Moving Average)</h4>
<ul><li><strong>策略</strong>: 维护一份模型权重的滑动平均版本。</li>
</ul>  $$ \theta_{EMA} = \alpha \theta_{EMA} + (1-\alpha) \theta_{current} $$
<ul><li><strong>作用</strong>: 极大地稳定了评估时的表现，平滑了训练过程中的震荡。<strong>所有 SOTA 模型 (RT-2, Octo, Pi0) 评估时用的都是 EMA 权重，而不是当前权重</strong>。</li>
</ul>
<h4>4.3. 最佳实践</h4>
<li>每 5000 Steps 保存一个 Checkpoint。</li>
<li>使用 SIMPLER 或 CALVIN 进行并行评估。</li>
<li>选取仿真 SR 最高的 Top-3 Checkpoint。</li>
<li>在真机上对这 Top-3 进行小样本 (N=10) 快速筛选。</li>
<li>选定最佳模型进行大规模 (N=50) 测试。</li></p>

<hr></p>

<h3>5. Evaluation Pipeline 构建 (Building Evaluation Pipeline)</h3>

<h4>5.1 整体架构</h4>

<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                   Evaluation Pipeline 架构                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌───────────┐    ┌───────────┐    ┌───────────┐    ┌────────┐│
│   │   Data    │───▶│   Model   │───▶│  Metrics  │───▶│ Report ││
│   │  Loader   │    │ Inference │    │ Compute   │    │ &amp; Log  ││
│   └───────────┘    └───────────┘    └───────────┘    └────────┘│
│        │                │                │               │      │
│        ▼                ▼                ▼               ▼      │
│   RLDS/HDF5        Batch/Stream     SR/MSS/IR      W&amp;B/TB      │
│   CALVIN/SIMPLER   Multi-ckpt       Confidence     Artifacts   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>5.2 关键组件</h4>

<pre><code class="python">class EvaluationPipeline:
    """VLA 评估流水线"""
    
    def __init__(self, config):
        # 1. 数据管理
        self.test_set = load_test_set(config.benchmark)  # CALVIN, SIMPLER, etc.
        
        # 2. 模型加载
        self.model = load_model(config.checkpoint)
        
        # 3. 指标计算器
        self.metrics = {
            'success_rate': SuccessRateMetric(),
            'mean_steps': MeanStepsMetric(),
            'intervention_rate': InterventionRateMetric(),
        }
        
        # 4. 日志记录
        self.logger = WandBLogger(project=config.project)
    
    def run_episode(self, task):
        """运行单个 Episode"""
        obs = self.env.reset(task)
        done = False
        steps = 0
        
        while not done and steps &lt; self.max_steps:
            action = self.model.predict(obs)
            obs, reward, done, info = self.env.step(action)
            steps += 1
        
        return {
            'success': info['success'],
            'steps': steps,
            'trajectory': info['trajectory']
        }
    
    def evaluate(self, num_episodes=50):
        """完整评估流程"""
        results = []
        
        for i in range(num_episodes):
            task = self.test_set.sample()
            result = self.run_episode(task)
            results.append(result)
            
            # 实时日志
            self.logger.log_episode(i, result)
        
        # 计算指标
        metrics = self.compute_metrics(results)
        
        # 置信区间
        metrics['sr_ci'] = self.wilson_interval(
            metrics['success_rate'], num_episodes
        )
        
        # 保存结果
        self.logger.log_summary(metrics)
        self.save_artifacts(results)
        
        return metrics
    
    def wilson_interval(self, p, n, z=1.96):
        """Wilson Score Interval for SR"""
        denominator = 1 + z**2 / n
        center = (p + z<em>*2 / (2</em>n)) / denominator
        spread = z <em> np.sqrt((p</em>(1-p) + z<em>*2/(4</em>n)) / n) / denominator
        return (center - spread, center + spread)
</code></pre>

<h4>5.3 CI/CD 集成</h4>

<pre><code class="yaml">name: Model Evaluation</p>

<p>on:
  push:
    paths:
      - 'checkpoints/**'</p>

<p>jobs:
  evaluate:
    runs-on: self-hosted-gpu
    steps:
      - name: Run CALVIN Evaluation
        run: |
          python eval.py \
            --benchmark calvin \
            --checkpoint ${{ github.sha }} \
            --num_episodes 100
      
      - name: Upload Results
        uses: wandb/upload-artifact@v1
        with:
          name: eval-results
          path: results/
</code></pre>

<h4>5.4 失败案例分析</h4>

<pre><code class="python">class FailureAnalyzer:
    """自动分析失败原因"""
    
    def analyze(self, failed_episodes):
        categories = {
            'perception': [],      # 视觉识别错误
            'planning': [],        # 规划路径错误
            'execution': [],       # 执行精度不足
            'ik_failure': [],      # 逆运动学无解
            'collision': [],       # 碰撞
        }
        
        for ep in failed_episodes:
            reason = self.classify_failure(ep)
            categories[reason].append(ep)
        
        # 可视化
        self.plot_failure_distribution(categories)
        
        # 生成报告
        return self.generate_report(categories)
</code></pre>

<hr></p>

<h3>6. 面试高频考点</h3>

<p><strong>Q: 具体讲讲怎么构建 Evaluation Pipeline 的？</strong>
A: 核心组件包括：
<li><strong>数据管理</strong>: 标准化测试集 (CALVIN/SIMPLER)，版本控制，确保可复现</li>
<li><strong>推理服务</strong>: 支持多 Checkpoint 并行评估，Batch/Streaming 两种模式</li>
<li><strong>指标计算</strong>: 自动化 SR/MSS/IR 计算，Wilson Interval 置信区间</li>
<li><strong>可视化</strong>: 失败案例分析，Attention 可视化，轨迹回放</li>
<li><strong>CI/CD 集成</strong>: 每次训练自动触发评估，结果上传 W&B</li></p>

<p><strong>Q: 什么是 "Cherry-picking"？如何避免？</strong>
A: Cherry-picking 指只展示成功的视频片段。避免方法是报告严格定义的 Success Rate，并公开所有尝试的原始视频 (Uncut Videos) 或日志。</p>

<p><strong>Q: 为什么 Sim-to-Real 评估很难？</strong>
A: 因为 Reality Gap。仿真里的成功不代表真机成功。SIMPLER 等工作试图缩小这个 Gap，但目前仍未完全解决。最可靠的评估依然是真机。</p>

<p><strong>Q: 如何评估模型的泛化性 (Generalization)？</strong>
A: 定义三个级别的泛化：
L1: <strong>Interpolation</strong> (训练分布内，新位置/新角度)。
L2: <strong>Visual Extrapolation</strong> (未见过的物体颜色/纹理/背景)。
L3: <strong>Semantic Extrapolation</strong> (未见过的物体类别/新指令)。
面试时要明确指出模型达到了哪一级别的泛化。</p>

<hr></p>

<hr></p>

<h2>第31章 知识隔离</h2>

<blockquote>[!IMPORTANT]</blockquote>
<blockquote><strong>Knowledge Insulation</strong> (知识绝缘) 是 <strong>Physical Intelligence</strong> 在 Pi0 模型中提出的一种训练技术，通过<strong>梯度隔离</strong>防止 VLM 在学习机器人控制时发生<strong>灾难性遗忘 (Catastrophic Forgetting)</strong>。</blockquote>

<h3>1. 核心问题：灾难性遗忘</h3>
当我们将预训练的 <strong>Vision-Language Model (VLM)</strong> 微调为 <strong>Vision-Language-Action (VLA)</strong> 模型时，会遇到一个致命问题：</p>

<h4>1.1. 什么是灾难性遗忘？</h4>
<ul><li>  <strong>定义</strong>: 神经网络在学习新任务时，会<strong>覆盖</strong>之前学到的知识。</li>
<li>  <strong>在 VLA 中的表现</strong>: </li>
</ul>    -   VLM 原本拥有强大的<strong>语义理解</strong>（从互联网规模数据学到）。
    -   当我们添加<strong>连续动作专家</strong>（Action Expert）进行机器人控制训练时，VLM 会<strong>忘记</strong>其原有的语言和视觉知识。
    -   结果：机器人学会了动作，但<strong>丢失了泛化能力</strong>和<strong>指令理解能力</strong>。</p>

<h4>1.2. 为什么会发生？</h4>
<ul><li>  <strong>分布不匹配</strong>: 机器人数据（如 10Hz 的关节角度）与 VLM 预训练数据（互联网图文）分布差异巨大。</li>
<li>  <strong>新参数的干扰</strong>: 新增的<strong>连续动作头</strong>（未经训练）的梯度会<strong>污染</strong> VLM 主干的参数。</li>
<li>  <strong>训练目标冲突</strong>: VLM 原本优化语言 token，现在要同时优化连续动作，导致混乱。</li>
</ul>
<h3>2. Knowledge Insulation 的解决方案</h3>

<h4>2.1. 核心思想：梯度隔离</h4>
<strong>关键操作</strong>: 在训练时<strong>阻止</strong>连续动作专家的梯度回传到 VLM 主干。</p>

<pre><code class="python">vlm_features = vlm_backbone(image, text)  # VLM 提取特征</p>

<p>discrete_actions = vlm_head(vlm_features)
loss_discrete = cross_entropy(discrete_actions, discrete_targets)</p>

<p>vlm_features_detached = vlm_features.detach()  # 🔑 关键！阻止梯度回传
continuous_actions = action_expert(vlm_features_detached)
loss_continuous = mse(continuous_actions, continuous_targets)</p>

<p>loss_discrete.backward()  # ✅ VLM 被更新
</code></pre>

<h4>2.2. 双轨训练</h4>
Knowledge Insulation 采用<strong>双轨并行</strong>训练策略：</p>

<table>
<thead><tr>
<th>分支</th>
<th>输入</th>
<th>输出</th>
<th>梯度去向</th>
<th>目的</th>
</tr></thead>
<tbody>
<tr>
<td><strong>离散动作分支</strong></td>
<td>VLM 特征</td>
<td>离散 Token（如 FAST）</td>
<td><strong>→ VLM 主干</strong></td>
<td>保持 VLM 语义能力</td>
</tr>
<tr>
<td><strong>连续动作分支</strong></td>
<td>VLM 特征（detached）</td>
<td>连续动作（关节角度）</td>
<td><strong>→ 仅动作专家</strong></td>
<td>学习精确控制</td>
</tr>
</tbody></table>

<h3>3. 为什么这样有效？</h3>

<h4>3.1. 保护 VLM 的语义知识</h4>
<ul><li>  <strong>VLM 只看到离散 token</strong>: 类似于它预训练时的语言 token，<strong>分布对齐</strong>。</li>
<li>  <strong>避免未训练参数的污染</strong>: 连续动作专家初始时是随机的，其梯度会破坏 VLM，隔离后就安全了。</li>
</ul>
<h4>3.2. 连续动作专家独立学习</h4>
<ul><li>  <strong>专家专注</strong>: 动作专家只需学习从 VLM 特征到连续动作的映射，不用担心破坏 VLM。</li>
<li>  <strong>高效收敛</strong>: 可以使用<strong>流匹配 (Flow Matching)</strong> 或<strong>扩散 (Diffusion)</strong> 等复杂技术，训练更快。</li>
</ul>
<h4>3.3. 推理时无缝切换</h4>
<ul><li>  <strong>训练时</strong>: VLM 学离散，专家学连续。</li>
<li>  <strong>推理时</strong>: VLM 提取特征 → 动作专家生成连续动作 → 机器人执行。</li>
</ul>
<h3>4. 实验效果对比</h3>
<table>
<thead><tr>
<th>指标</th>
<th><strong>无 Knowledge Insulation</strong></th>
<th><strong>有 Knowledge Insulation</strong></th>
</tr></thead>
<tbody>
<tr>
<td><strong>训练速度</strong></td>
<td>慢（VLM 被破坏后需重新学习）</td>
<td><strong>快（VLM 稳定，专家快速收敛）</strong></td>
</tr>
<tr>
<td><strong>语义理解</strong></td>
<td>差（灾难性遗忘）</td>
<td><strong>强（保留 VLM 能力）</strong></td>
</tr>
<tr>
<td><strong>新任务泛化</strong></td>
<td>弱（过拟合机器人数据）</td>
<td><strong>强（利用 VLM 的网络知识）</strong></td>
</tr>
<tr>
<td><strong>指令跟随</strong></td>
<td>退化（特别是多语言）</td>
<td><strong>保持（零样本多语言）</strong></td>
</tr>
</tbody></table>

<h3>5. 与其他技术的结合</h3>
Knowledge Insulation 通常与以下技术配合使用：</p>

<ul><li>  <strong>FAST Tokenizer</strong>: 提供高效的离散动作 token（DCT + BPE）。</li>
<li>  <strong>Flow Matching</strong>: 连续动作专家使用流匹配生成平滑轨迹。</li>
<li>  <strong>Co-Training</strong>: 同时训练 VLM 分支（语言理解）和动作分支（物理控制）。</li>
<li>  <strong>LoRA</strong>: 使用低秩适应进一步减少对 VLM 的修改。</li>
</ul>
<h3>6. 面试要点</h3>
<ul><li>  <strong>核心</strong>: 记住 "梯度隔离 (Gradient Isolation)" 和 ".detach()" 操作。</li>
<li>  <strong>问题</strong>: 灾难性遗忘 - VLM 学机器人控制时会忘记语义知识。</li>
<li>  <strong>解决</strong>: 双轨训练 - VLM 学离散 token，动作专家学连续控制，梯度不回传。</li>
<li>  <strong>效果</strong>: 保护 VLM 知识，加速训练，提升泛化。</li>
<li>  <strong>来源</strong>: Physical Intelligence 的 Pi0 模型首次系统性应用这一技术。</li>
</ul>
<h3>7. 延伸：持续学习 (Continual Learning)</h3>
Knowledge Insulation 是<strong>持续学习</strong>领域的一个应用案例：
<ul><li>  <strong>持续学习目标</strong>: 机器人能不断学习新技能，而不忘记旧技能。</li>
<li>  <strong>Knowledge Insulation 的贡献</strong>: 在 VLA 适配阶段就防止遗忘，为后续持续学习打好基础。</li>
<li>  <strong>其他技术</strong>: EWC (弹性权重巩固)、Memory Replay（经验回放）、Progressive Networks（渐进网络）。</li>
</ul>
<h3>8. 参考资源</h3>
<ul><li>  <strong>Pi0 Technical Report</strong>: <a href="https://physicalintelligence.company/">Physical Intelligence</a></li>
<li>  <strong>相关论文</strong>: VLM2VLA, ReVLA (视觉灾难性遗忘恢复)</li>
<li>  <strong>代码示例</strong>: 查看 Pi0 的 GitHub（如开源）中的梯度隔离实现</li>
</ul>

<hr></p>

<h1>附录</h1>

<hr></p>

<h2>附录A 数据格式与处理</h2>

<p>在 VLA 模型的训练中，数据是核心壁垒。本章介绍机器人学习中通用的数据格式和处理策略。</p>

<h3>1. 主流数据格式对比 (Mainstream Data Formats)</h3>

<p>在 VLA 领域，数据格式的选择直接影响训练效率和生态兼容性。目前主要有三种主流格式：</p>

<h4>1.1. RLDS (Robotics Language-Image Datasets)</h4>
<ul><li><strong>生态位</strong>: <strong>Google / Open X-Embodiment 标准</strong>。</li>
<li><strong>底层</strong>: 基于 <code>TensorFlow Datasets (TFDS)</code> 和 <code>ProtoBuf</code>。</li>
<li><strong>物理格式</strong>: <strong><code>.tfrecord</code></strong> 文件。</li>
</ul>    - 这是一种基于行 (Row-based) 的二进制序列化格式，将数据序列化为 Protocol Buffers 消息。
<ul><li><strong>特点</strong>:</li>
</ul>    - <strong>序列化</strong>: 适合大规模分布式读取，Google TPU 友好。
    - <strong>标准化</strong>: 强制定义了 <code>observation</code>, <code>action</code>, <code>language</code> 的标准接口。
    - <strong>流式读取</strong>: 支持云端存储 (GCS) 的流式训练，无需下载整个数据集。
<ul><li><strong>适用场景</strong>: 使用 TPU 训练，或基于 RT-1/RT-2/Octo 架构开发时。</li>
</ul>
<h4>1.2. LeRobot Dataset (Hugging Face)</h4>
<ul><li><strong>生态位</strong>: <strong>PyTorch / Open Source 社区新标准</strong>。</li>
<li><strong>底层</strong>: 基于 <code>Parquet</code> (列式存储) 和 <code>Hugging Face Datasets</code> (Apache Arrow)。</li>
<li><strong>物理格式</strong>: <strong><code>.parquet</code></strong> 文件。</li>
</ul>    - 这是一种基于列 (Column-based) 的存储格式，压缩率极高，读取特定列（如只读 Action 不读 Image）非常快。
<ul><li><strong>特点</strong>:</li>
</ul>    - <strong>可视化</strong>: 在 Hugging Face 网页端可直接预览视频和元数据。
    - <strong>轻量级</strong>: 不依赖 TensorFlow，安装简单 (<code>pip install lerobot</code>)。
    - <strong>PyTorch 原生</strong>: 数据加载器直接输出 PyTorch Tensors。
<ul><li><strong>适用场景</strong>: 使用 GPU 训练，基于 OpenVLA/ACT/Diffusion Policy 开发新项目时。</li>
</ul>
<h4>1.3. HDF5 / Robomimic</h4>
<ul><li><strong>生态位</strong>: <strong>传统科研 / 仿真数据标准</strong>。</li>
<li><strong>底层</strong>: <code>HDF5</code> (Hierarchical Data Format)。</li>
<li><strong>物理格式</strong>: <strong><code>.hdf5</code></strong> 或 <strong><code>.h5</code></strong> 文件。</li>
</ul>    - 类似于一个"文件系统"，内部可以像文件夹一样组织数据 (Groups/Datasets)。
<ul><li><strong>特点</strong>:</li>
</ul>    - <strong>单文件</strong>: 整个数据集通常是一个巨大的二进制文件。
    - <strong>随机访问</strong>: 支持高效的随机索引读取 (Random Access)。
    - <strong>结构灵活</strong>: 类似于文件系统的层级结构。
<ul><li><strong>缺点</strong>: 不适合超大规模数据集 (TB 级别)，难以流式读取。</li>
<li><strong>适用场景</strong>: 仿真环境 (MuJoCo) 数据收集，小规模真机实验。</li>
</ul>
<h4>📊 格式对比表</h4>

<table>
<thead><tr>
<th>特性</th>
<th>RLDS</th>
<th>LeRobot</th>
<th>HDF5</th>
</tr></thead>
<tbody>
<tr>
<td><strong>背书机构</strong></td>
<td>Google DeepMind</td>
<td>Hugging Face</td>
<td>Stanford (Robomimic)</td>
</tr>
<tr>
<td><strong>核心依赖</strong></td>
<td>TensorFlow</td>
<td>PyTorch / Arrow</td>
<td>h5py</td>
</tr>
<tr>
<td><strong>存储格式</strong></td>
<td>TFRecord (序列化)</td>
<td>Parquet (列式)</td>
<td>HDF5 (层级)</td>
</tr>
<tr>
<td><strong>流式读取</strong></td>
<td>⭐⭐⭐ (原生支持)</td>
<td>⭐⭐ (支持)</td>
<td>⭐ (困难)</td>
</tr>
<tr>
<td><strong>生态兼容</strong></td>
<td>Open X-Embodiment</td>
<td>Transformers / Hub</td>
<td>Simulators</td>
</tr>
<tr>
<td><strong>推荐指数</strong></td>
<td>⭐⭐⭐ (大规模/TPU)</td>
<td>⭐⭐⭐ (新项目首选)</td>
<td>⭐⭐ (科研/仿真)</td>
</tr>
</tbody></table>

<hr></p>

<h3>2. 代码示例：如何加载数据</h3>

<h4>2.1. Loading RLDS (TensorFlow)</h4>
<pre><code class="python">import tensorflow_datasets as tfds</p>

<p>ds = tfds.load('fractal20220817_data', split='train')</p>

<p>for episode in ds.take(1):
    steps = episode['steps']
    for step in steps:
        image = step['observation']['image']
        action = step['action']
        # 需要手动转换为 PyTorch Tensor 如果不用 TF
</code></pre>

<h4>2.2. Loading LeRobot (PyTorch)</h4>
<pre><code class="python">from lerobot.common.datasets.lerobot_dataset import LeRobotDataset</p>

<p>dataset = LeRobotDataset("lerobot/pusht")</p>

<p>item = dataset[0]
image = item['observation.image']  # 自动归一化并转为 Tensor (C, H, W)
action = item['action']
print(f"Action shape: {action.shape}")
</code></pre>

<h3>3. PyTorch 完整训练流程 (PyTorch Training Pipeline)</h3>

<p>在 PyTorch 中训练 VLA 模型，数据流通常遵循以下模式：<code>Dataset</code> -> <code>DataLoader</code> -> <code>Model</code>。</p>

<h4>3.1. 核心组件</h4>
<li> <strong>Dataset</strong>: 负责读取磁盘上的数据 (RLDS/Parquet)，并进行预处理 (Resize, Normalize)。</li>
<li> <strong>Processor/Transform</strong>: 处理多模态数据。</li>
    -   <strong>Image</strong>: <code>Resize((224, 224))</code>, <code>Normalize(mean, std)</code>.
    -   <strong>Text</strong>: Tokenizer (如 Llama Tokenizer) 将指令转为 Input IDs.
    -   <strong>Action</strong>: 归一化到 [-1, 1].
<li> <strong>DataLoader</strong>: 将多个样本打包成 Batch。需要自定义 <code>collate_fn</code> 来处理变长序列 (Padding)。</li></p>

<h4>3.2. 代码实战 (Pseudo-code)</h4>

<pre><code class="python">import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoProcessor</p>

<p>class VLADataset(Dataset):
    def __init__(self, data_path, processor):
        self.data = load_data(data_path) # e.g., LeRobotDataset
        self.processor = processor</p>

<p>    def __getitem__(self, idx):
        item = self.data[idx]
        
        # 1. 获取原始数据
        image = item['observation.image'] # (C, H, W)
        text = item['language_instruction'] # "Pick up the apple"
        action = item['action'] # (Time, Action_Dim)
        
        # 2. 多模态预处理 (关键步骤!)
        # VLA 模型通常需要同时输入图像和文本
        inputs = self.processor(
            text=text, 
            images=image, 
            return_tensors="pt", 
            padding="max_length",
            truncation=True
        )
        
        # 3. 返回字典
        return {
            "input_ids": inputs.input_ids.squeeze(),
            "pixel_values": inputs.pixel_values.squeeze(),
            "labels": action # 动作作为监督信号
        }</p>

<p>dataset = VLADataset(path, processor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
model = OpenVLAModel.from_pretrained("openvla/openvla-7b")</p>

<p>for batch in dataloader:
    # 将数据送入 GPU
    input_ids = batch["input_ids"].cuda()
    pixel_values = batch["pixel_values"].cuda()
    actions = batch["labels"].cuda()
    
    # 前向传播
    # VLA 模型通常计算 Action MSE Loss 或 Cross-Entropy Loss (如果是 Tokenized Action)
    loss = model(
        input_ids=input_ids, 
        pixel_values=pixel_values, 
        labels=actions
    ).loss
    
    loss.backward()
    optimizer.step()
</code></pre>

<h4>3.3. 常见坑点 (Pitfalls)</h4>
<ul><li>  <strong>数据类型</strong>: 确保 Action 是 <code>float32</code> (对于 Diffusion/Regression) 或 <code>long</code> (对于 Tokenization)。</li>
<li>  <strong>图像通道</strong>: PyTorch 默认是 <code>(C, H, W)</code>，而有些读取库 (如 OpenCV/PIL) 可能是 <code>(H, W, C)</code>，务必检查 <code>permute</code>。</li>
<li>  <strong>归一化</strong>: 动作必须使用<strong>统计数据 (Statistics)</strong> 进行归一化 (e.g., min-max 或 mean-std)。<strong>推理时必须使用相同的统计数据反归一化</strong>。</li>
</ul>
<hr></p>

<h3>4. 数据加权与平衡 (Data Weighting & Balancing)</h3>
在训练通用 VLA 模型时，通常会混合多种数据集。不同数据集的质量、规模和难度差异巨大，直接混合训练效果往往不佳。</p>

<h4>常见策略</h4>
<li><strong>按数据集规模加权</strong>:</li>
    - 简单的按比例采样，但这会导致大规模数据集 (通常是简单的重复任务) 主导训练，模型学不到复杂任务。
<li><strong>按任务难度加权</strong>:</li>
    - 给包含复杂操作 (e.g., 使用工具, 长序列) 的数据集更高的权重。
<li><strong>成功率过滤 (Success Filtering)</strong>:</li>
    - 仅使用 <code>is_terminal=True</code> 且 <code>reward=1</code> 的成功轨迹进行 BC (Behavior Cloning) 训练。
    - 对于失败轨迹，可以用于对比学习 (Contrastive Learning) 或作为负样本。
<h4>4.4. 联合训练 (Co-training)</h4>
为了防止灾难性遗忘并保持通用泛化能力，VLA 训练通常会混合互联网数据。
<blockquote>详见独立章节：<strong><a href="./co_training.md">联合训练详解 (Co-training)</a></strong></blockquote>

<h3>4. 数据收集工具链 (Data Collection Tools)</h3>

<p>高质量的数据源于高效的收集工具。</p>

<h4>4.1. 遥操作 (Teleoperation)</h4>
<ul><li><strong>VR 头显 (Vision Pro / Quest 3)</strong>:</li>
</ul>    - <strong>优势</strong>: 沉浸感强，能收集 6-DoF 姿态，适合灵巧手操作。
    - <strong>方案</strong>: ALOHA (VR版), AnyTeleop。
<ul><li><strong>主从臂 (Leader-Follower Arms)</strong>:</li>
</ul>    - <strong>优势</strong>: 力反馈真实，操作精度极高。
    - <strong>方案</strong>: ALOHA (使用 WidowX 作为主臂), GELLO (低成本 3D 打印主臂)。
<ul><li><strong>手柄/3D 鼠标</strong>:</li>
</ul>    - <strong>优势</strong>: 成本低，易获取。
    - <strong>劣势</strong>: 难以控制高自由度 (如灵巧手)。</p>

<h4>4.2. 自动化收集 (Autonomous Collection)</h4>
<ul><li><strong>Scripted Policy</strong>: 在仿真或简单场景中，用硬编码脚本生成数据。</li>
<li><strong>Self-Replay</strong>: 机器人回放成功的轨迹，并添加噪声进行数据增强。</li>
</ul>
<h3>5. 动作空间对齐 (Action Space Alignment)</h3>
不同机器人的动作空间不同 (e.g., 7-DoF 机械臂 vs 14-DoF 双臂 vs 四足)。</p>

<ul><li><strong>归一化 (Normalization)</strong>: 将所有动作维度归一化到 [-1, 1] 或 [0, 1]。</li>
<li><strong>Proprioception Padding</strong>: 对于自由度较少的机器人，用 0 填充剩余维度。</li>
<li><strong>相对控制 vs 绝对控制</strong>:</li>
</ul>    - <strong>Delta Action</strong>: 预测当前状态的增量 (dx, dy, dz)。泛化性更好。
    - <strong>Absolute Action</strong>: 预测绝对坐标。精度更高，但依赖标定。
    - <strong>趋势</strong>: VLA 模型通常偏向于使用 <strong>Delta Action (End-effector velocity/pose delta)</strong>。</p>

<h3>7. 面试高频考点</h3>
<li> <strong>数据格式</strong>: RLDS 和 LeRobot 格式有什么区别？为什么 PyTorch 用户现在倾向于 LeRobot？(答: LeRobot 去除了 TF 依赖，原生支持 PyTorch，且基于 Parquet 存储效率高)</li>
<li> <strong>数据流</strong>: 在 VLA 训练中，Processor 的作用是什么？(答: 同时处理图像归一化和文本 Tokenization，确保多模态对齐)</li>
<li> <strong>数据平衡</strong>: 如果我有 1000 条简单的 Pick-Place 数据和 100 条复杂的 Assembly 数据，应该怎么训练？(答: 重采样 Assembly 数据，提高其在 Batch 中的比例)</li>
<li> <strong>Action Space</strong>: 为什么要用 Delta Action？(答: 减少对绝对坐标的依赖，更容易迁移到不同位置或不同机器人)</li>
<li> <strong>数据收集</strong>: 相比于 VR 遥操作，主从臂 (Leader-Follower) 有什么优缺点？(答: 主从臂有力反馈，精度高，但成本高且不仅限于异构机器人映射)</li>
<li> <strong>Co-training</strong>: 为什么在训练 VLA 时要混合互联网 VQA 数据？(答: 防止灾难性遗忘，保持 VLM Backbone 的通用语义理解和泛化能力)</li></p>

<hr></p>

<hr></p>

<h2>附录B 文献综述</h2>

<p>本章节对 VLA 领域的核心文献进行<strong>深度技术归纳</strong>，适合面试前快速复习模型细节。</p>

<h3>1. Diffusion Policy (Chi et al., RSS 2023)</h3>
<blockquote><strong>论文</strong>: <a href="https://arxiv.org/abs/2303.04137">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</a></blockquote>

<ul><li><strong>核心问题</strong>: 解决传统 MSE 回归在多模态分布 (Multimodal Distribution) 下的平均值问题 (即"撞墙"问题)。</li>
<li><strong>核心技术</strong>: <strong>DDPM (Denoising Diffusion Probabilistic Models)</strong>。将动作生成建模为从高斯噪声中逐步去噪的过程。</li>
<li><strong>Backbone</strong>:</li>
</ul>    - <strong>CNN-based</strong>: 1D Temporal CNN (类似 U-Net)，适合短时序。
    - <strong>Transformer-based</strong>: DiT (Diffusion Transformer)，适合长时序。
<ul><li><strong>Action Space</strong>: <strong>连续空间 (Continuous)</strong>。无离散化误差，精度极高。</li>
<li><strong>Inference</strong>: 迭代去噪。原始 DDPM 需 100 步，使用 <strong>DDIM</strong> 可加速至 10-15 步。</li>
<li><strong>Deep Dive</strong>:</li>
</ul>    - <strong>EBM 视角</strong>: Diffusion 实际上是在学习能量地貌 (Energy Landscape)，相比 MSE 的单峰平均，它能捕捉多模态分布 (Multimodal Distribution)。
    - <strong>Conditioning</strong>: 通过 <strong>FiLM</strong> 层将语言/图像特征注入 U-Net。
<ul><li><strong>Key Contribution</strong>: 首次将生成式 AI (Generative AI) 引入机器人控制，完美解决了多解问题，并在高精度任务 (如穿针) 上表现卓越。</li>
</ul>
<h3>2. RT-2 (Google DeepMind, 2023)</h3>
<blockquote><strong>论文</strong>: <a href="https://arxiv.org/abs/2307.15818">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a></blockquote>

<ul><li><strong>核心问题</strong>: 如何让机器人拥有互联网级别的语义理解能力 (泛化到未见过的物体/指令)。</li>
<li><strong>核心技术</strong>: <strong>VLA (Vision-Language-Action)</strong> = VLM + Action Tokens。</li>
<li><strong>Backbone</strong>: <strong>PaLI-X (55B)</strong> 或 <strong>PaLM-E (12B)</strong>。</li>
<li><strong>Action Tokenization</strong>:</li>
</ul>    - <strong>Uniform Discretization</strong>: 将动作维度归一化并切分为 <strong>256 个 Bins</strong>。
    - <strong>Text Mapping</strong>: 将这些 Bins 映射为特殊的文本 Token (如 "1", "128")，与自然语言共享词表。
<ul><li><strong>Training</strong>: <strong>Co-fine-tuning</strong> (混合微调)。同时训练互联网 VQA 数据 (保持语义) 和机器人操作数据 (学习控制)。</li>
<li><strong>Key Contribution</strong>: 涌现出 <strong>Semantic Reasoning</strong> (语义推理) 能力。例如听到 "pick up the extinct animal" 能抓起恐龙玩具，尽管训练数据里只有 "pick up dinosaur"。</li>
</ul>
<h3>3. OpenVLA (Stanford, 2024)</h3>
<blockquote><strong>论文</strong>: <a href="https://arxiv.org/abs/2406.09246">OpenVLA: An Open-Source Vision-Language-Action Model</a></blockquote>

<ul><li><strong>核心问题</strong>: 复现 RT-2 的能力，但完全开源且高效。</li>
<li><strong>核心技术</strong>: <strong>Parameter-Efficient Fine-Tuning (LoRA)</strong>。</li>
<li><strong>Backbone</strong>:</li>
</ul>    - <strong>Language</strong>: <strong>Llama 2 7B</strong>。
    - <strong>Vision</strong>: <strong>SigLIP</strong> (比 CLIP 更强的视觉编码器)。
    - <strong>Projector</strong>: 2-layer MLP (将视觉 Embedding 映射到语言空间)。
<ul><li><strong>Action Output</strong>:</li>
</ul>    - 不同于 RT-2 直接输出文本，OpenVLA 使用专门的 <strong>Action Head</strong> (Linear Layer) 预测去离散化的动作 Token。
    - 依然是 <strong>256-bin Discretization</strong>。
<ul><li><strong>Optimization</strong>: 支持 <strong>4-bit Quantization (QLoRA)</strong>，使得 7B 模型可以在消费级显卡 (如 RTX 3090/4090) 上运行。</li>
<li><strong>Key Contribution</strong>: 提供了第一个性能接近闭源 SOTA 的开源 VLA 模型，并构建了完整的开源训练/部署生态。</li>
</ul>
<h3>4. Pi0 (Physical Intelligence, 2024)</h3>
<blockquote><strong>论文</strong>: <a href="https://www.physicalintelligence.company/blog/pi0">π0: A Generalist Robot Foundation Model</a></blockquote>

<ul><li><strong>核心问题</strong>: 解决 VLM 推理速度慢、难以进行高频 (50Hz) 连续控制的问题。</li>
<li><strong>核心技术</strong>: <strong>Flow Matching (流匹配)</strong>。</li>
<li><strong>Backbone</strong>: <strong>PaliGemma 3B</strong> (Google 的轻量级 VLM)。</li>
<li><strong>Action Space</strong>: <strong>连续空间 (Continuous)</strong>。</li>
</ul>    - 不同于 RT-2/OpenVLA 的离散 Token，Pi0 输出连续动作，避免了量化误差。
<ul><li><strong>Inference</strong>: 使用 ODE Solver (常微分方程求解器)。相比 Diffusion 的随机游走，Flow Matching 走直线，<strong>1-10 步</strong>即可生成高质量动作。</li>
<li><strong>Deep Dive</strong>:</li>
</ul>    - <strong>OT-CFM</strong>: 基于 Optimal Transport 构造直线路径 (Wasserstein Geodesic)。
    - <strong>ODE Solver</strong>: 训练时学习向量场，推理时使用 <strong>Euler</strong> (极速) 或 <strong>Heun</strong> (高精) 求解。
<ul><li><strong>Key Contribution</strong>: 结合了 VLM 的语义理解和 Flow Matching 的高频精细控制，实现了"大脑"与"小脑"的统一。</li>
</ul>
<h3>5. Pi0.5 (Physical Intelligence, 2025)</h3>
<blockquote><strong>核心定位</strong>: <strong>Open-World Explorer (开放世界探险家)</strong></blockquote>

<ul><li><strong>核心问题</strong>: 解决机器人无法在从未见过的环境 (Open World) 中泛化的问题。</li>
<li><strong>核心技术</strong>: <strong>Unified Model with Hierarchical Inference</strong>。</li>
<li><strong>架构创新</strong>:</li>
</ul>    - <strong>Latent Thought</strong>: 模型内部生成隐式的高层语义子任务 (Semantic Subtask)，再解码为底层动作。
    - <strong>Hybrid Architecture</strong>: 训练时使用 <strong>FAST Tokenizer</strong> (离散) 加速，推理时使用 <strong>Flow Matching</strong> (连续) 微调。
<ul><li><strong>Data Strategy</strong>: <strong>Co-training</strong>。混合 Robot Data (高质量) + Internet Videos (世界模型) + Simulation Data (长序列逻辑)。</li>
<li><strong>Key Contribution</strong>: 实现了跨形态 (Cross-Embodiment) 的 Zero-shot 迁移，并显著提升了长序列任务的成功率。</li>
</ul>
<h3>6. Pi0.6 (Physical Intelligence, 2025)</h3>
<blockquote><strong>核心定位</strong>: <strong>Self-Improving Master (自我进化大师)</strong></blockquote>

<ul><li><strong>核心问题</strong>: 如何超越人类示教的上限，实现极致的熟练度 (Proficiency)。</li>
<li><strong>核心技术</strong>: <strong>Recap Algorithm (Offline RL)</strong>。</li>
<li><strong>架构升级</strong>:</li>
</ul>    - <strong>5B Backbone</strong>: 更强的语义理解。
    - <strong>Action Expert</strong>: 独立的高频动作生成模块 (小脑)，专门负责精细操作。
<ul><li><strong>Recap 机制</strong>:</li>
</ul>    - 学习失败轨迹 (Failure Cases)，通过 Offline RL 抑制错误动作，奖励成功动作。
    - 实现了 <strong>Data-Driven Self-Improvement</strong>。
<ul><li><strong>Key Contribution</strong>: 证明了机器人可以通过自我复盘 (Recap) 在操作速度和鲁棒性上超越人类专家。</li>
</ul>
<h3>8. FAST (Physical Intelligence, 2025)</h3>
<blockquote><strong>论文</strong>: <a href="https://arxiv.org/abs/2501.09747">FAST: Efficient Action Tokenization for VLA Models (arXiv:2501.09747)</a></blockquote>

<ul><li><strong>核心问题</strong>: 传统动作 token 化方法（简单分桶）无法处理高频、灵巧的机器人操作。</li>
<li><strong>核心技术</strong>: <strong>Frequency-space Action Sequence Tokenization (DCT + BPE)</strong>。</li>
<li><strong>工作原理</strong>:</li>
</ul>    - <strong>DCT (离散余弦变换)</strong>: 将时域动作序列转换到频域，只保留低频系数（压缩比 2.5:1）。
    - <strong>BPE (字节对编码)</strong>: 类似 GPT，将常见 DCT 系数组合合并为单个 token（压缩比 2.3:1）。
<ul><li><strong>效果</strong>: 一个 10 步动作序列从 70 个 token 压缩为 <strong>2-3 个 token</strong>。</li>
<li><strong>FAST+</strong>: 在 100 万+真实机器人数据上预训练的通用 tokenizer，跨平台泛化。</li>
<li><strong>Key Contribution</strong>: 使 OpenVLA 训练速度提升 <strong>5 倍</strong>，同时保持高频动作精度。</li>
</ul>
<h3>9. Galaxea G0 (星海图智能, 2024)</h3>
<blockquote><strong>论文</strong>: <a href="https://arxiv.org/abs/2509.00576">Galaxea Open-World Dataset and G0 Dual-System VLA Model (arXiv:2509.00576)</a></blockquote>

<ul><li><strong>核心问题</strong>: 单一 VLA 模型难以同时处理长时域任务的高层规划和低层控制。</li>
<li><strong>核心技术</strong>: <strong>Dual-System Architecture (双系统架构)</strong>。</li>
<li><strong>架构设计</strong>:</li>
</ul>    - <strong>G0-VLM</strong>: 负责多模态规划和高层推理（大脑）。
    - <strong>G0-VLA</strong>: 负责细粒度执行和低层控制（小脑）。
<ul><li><strong>训练策略</strong>: <strong>三阶段课程学习</strong></li>
</ul>    1. 跨具身预训练（学习通用世界知识）
    2. 单具身预训练（适配特定机器人）← 核心阶段
    3. 任务后训练（精调复杂技能）
<ul><li><strong>Galaxea Open-World Dataset</strong>: 500+ 小时，50 个真实场景，统一具身（R1 Lite），精确子任务标注。</li>
<li><strong>Key Contribution</strong>: 在长时域移动操作任务上表现突出，泛化能力强，可解释性高（子任务可见）。</li>
</ul>
<h3>10. Knowledge Insulation (Physical Intelligence, 2024)</h3>
<blockquote><strong>技术</strong>: Pi0 的梯度隔离训练方法</blockquote>

<ul><li><strong>核心问题</strong>: VLA 微调时，新增的连续动作专家会破坏 VLM 的预训练语义知识（灾难性遗忘）。</li>
<li><strong>核心技术</strong>: <strong>Gradient Isolation (梯度隔离)</strong>。</li>
<li><strong>工作原理</strong>:</li>
</ul>    - <strong>VLM 分支</strong>: 学习离散动作 token（保持语义理解）。
    - <strong>动作专家分支</strong>: 学习连续动作（使用 <code>.detach()</code> 阻止梯度回传到 VLM）。
<ul><li><strong>效果</strong>: VLM 的语义知识被"绝缘"保护，同时动作专家独立学习连续控制。</li>
<li><strong>Key Contribution</strong>: 防止灾难性遗忘，加速训练，提升泛化能力，为持续学习打好基础。</li>
</ul>
<h3>总结对比表 (Summary Table)</h3>

<p>| 特性 | Diffusion Policy | RT-2 | OpenVLA | Pi0 | WALL-OSS | Galaxea G0 | FAST |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| <strong>核心机制</strong> | Denoising | Token Prediction | Token + LoRA | Flow Matching | <strong>Uni-CoT + Dual Heads</strong> | <strong>Dual-System</strong> | <strong>DCT + BPE</strong> |\n| <strong>动作空间</strong> | 连续 | 离散 (256) | 离散 (256) | 连续 | 连续 + 离散 | 连续 | <strong>离散 (极度压缩)</strong> |\n| <strong>Backbone</strong> | CNN/ViT | PaLI-X (55B) | Llama 2 (7B) | PaliGemma (3B) | VLM | <strong>VLM + VLA</strong> | <strong>Tokenizer</strong> |\n| <strong>推理速度</strong> | 慢 (100 步) | 极慢 | 中等 | 快 (1-10 步) | 快/精 | 稍慢 (两阶段) | <strong>极快 (5x)</strong> |\n| <strong>语义能力</strong> | 弱 | 极强 | 强 | 强 | <strong>强 (CoT)</strong> | <strong>强 (分离 VLM)</strong> | N/A |\n| <strong>适用场景</strong> | 精细操作 | 高层规划 | 通用操作 | 通用控制 | 长序列推理 | <strong>长时域移动操作</strong> | <strong>高频 Token 化</strong> |\n| <strong>核心优势</strong> | 多模态分布 | 语义涌现 | 开源生态 | 高效推理 | <strong>统一思维链</strong> | <strong>分层解耦</strong> | <strong>压缩效率</strong> |</p>

<hr></p>

<hr></p>

<h2>附录C ASCII 图表速查</h2>

<h3>ASCII Diagram Cheat Sheet</h3>

<p>集中提炼 <code>theory/</code> 中所有关键的 ASCII 结构图，便于记忆核心架构、流程与对比。</p>

<h4>1. Quantization Flow</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    量化流程 (Quantization Flow)                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   FP32 权重                    INT8/INT4 权重                   │
│   ┌─────────┐                  ┌─────────┐                      │
│   │ 0.0234  │                  │   3     │                      │
│   │ 0.1567  │   ──────────▶    │   20    │                      │
│   │-0.0891  │    量化 (Q)      │  -11    │                      │
│   │ 0.2103  │                  │   27    │                      │
│   └─────────┘                  └─────────┘                      │
│       ▲                             │                           │
│       │                             │                           │
│       └─────────────────────────────┘                           │
│              反量化 (DeQ)                                        │
│                                                                 │
│   存储: 32-bit ────────▶ 4-bit (8x 压缩)                        │
│   精度: 高 ────────────▶ 低 (有损失)                            │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>2. Symmetric vs Asymmetric Quantization</h4>
<pre><code class="">对称量化 (Symmetric)              非对称量化 (Asymmetric)
        Z = 0                           Z ≠ 0</p>

<p>    -127 ◀──────▶ +127              0 ◀──────────▶ 255
      │     0     │                 │      Z       │
      ├─────┼─────┤                 ├──────┼───────┤
      │     │     │                 │      │       │
   ───┴─────┴─────┴───           ──┴──────┴───────┴──
     min   0    max               min    0       max
      └─────┬─────┘                └───────┬───────┘
            │                              │
       数据分布对称                    数据分布偏移
     (适合权重 weights)           (适合激活值 activations)
</code></pre>

<h4>3. LoRA Architecture</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    LoRA 架构示意图                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│         输入 x                                                  │
│           │                                                     │
│     ┌─────┴─────┐                                               │
│     │           │                                               │
│     ▼           ▼                                               │
│  ┌──────┐   ┌──────┐                                            │
│  │  W₀  │   │  A   │  ← 低秩矩阵 (r &lt;&lt; d)                       │
│  │      │   │ r×k  │    可训练                                  │
│  │ d×k  │   └──┬───┘                                            │
│  │      │      │                                                │
│  │ 冻结 │      ▼                                                │
│  └──┬───┘   ┌──────┐                                            │
│     │       │  B   │  ← 低秩矩阵                                │
│     │       │ d×r  │    可训练                                  │
│     │       └──┬───┘                                            │
│     │          │                                                │
│     │    ΔW = B·A                                               │
│     │          │                                                │
│     └────┬─────┘                                                │
│          │  W = W₀ + α·BA                                       │
│          ▼                                                      │
│        输出 h                                                   │
│                                                                 │
│  参数量: d×k (冻结) + r×(d+k) (训练) ≈ 0.1% ~ 1%                │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>4. Co-training Mix</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                   Co-training 数据混合策略                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────┐       ┌──────────────────┐               │
│  │   机器人数据      │       │   互联网数据      │               │
│  │   (Robot Data)   │       │   (Web Data)     │               │
│  │                  │       │                  │               │
│  │  📷 + 🎯 + 🦾    │       │  📷 + 📝         │               │
│  │  图像 指令 动作   │       │  图像 文本        │               │
│  └────────┬─────────┘       └────────┬─────────┘               │
│           │                          │                          │
│           │    混合比例 1:1          │                          │
│           └──────────┬───────────────┘                          │
│                      ▼                                          │
│           ┌──────────────────────┐                              │
│           │      VLA 模型        │                              │
│           │   ┌──────┬──────┐   │                              │
│           │   │Action│ Text │   │                              │
│           │   │ Head │ Head │   │                              │
│           │   └──┬───┴──┬───┘   │                              │
│           └──────┼──────┼───────┘                              │
│                  │      │                                       │
│           ┌──────┴──┐ ┌─┴──────┐                               │
│           │ Action  │ │  Text  │                               │
│           │  Loss   │ │  Loss  │                               │
│           │ (机器人)│ │ (互联网)│                               │
│           └─────────┘ └────────┘                               │
│                                                                 │
│   效果: 保持语义能力 ✓  学习动作控制 ✓  防止灾难性遗忘 ✓         │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>5. Action Generation Matrix</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│              三种动作生成范式对比 (Action Generation)            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. 离散化 Tokenization (RT-1, RT-2)                            │
│     ┌───────────────────────────────┐                           │
│     │ 连续动作 ──▶ 离散 Bins ──▶ Token 预测   │                 │
│     └───────────────────────────────┘                           │
│     ⚡ 快速 (1步)  |  📉 精度损失  |  ✅ 多模态                  │
│                                                                 │
│  2. 扩散策略 Diffusion (Octo)                                   │
│     ┌───────────────────────────────┐                           │
│     │ 噪声 ════▶ 去噪 ════▶...═══▶ 动作 │                         │
│     └───────────────────────────────┘                           │
│     🐢 慢 (50-100步)  |  🎯 高精度  |  ✅ 多模态                │
│                                                                 │
│  3. 流匹配 Flow Matching (π0)                                   │
│     ┌───────────────────────────────┐                           │
│     │ 噪声 ════════════════════▶ 动作 │                          │
│     └───────────────────────────────┘                           │
│     ⚡ 极快 (1-10步)  |  🎯 高精度  |  ✅ 多模态                  │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>6. π0.5 Unified Flow</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    π0.5 统一架构 (Unified Model)                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   输入: 📷 图像 + 📝 语言指令                                    │
│              │                                                  │
│              ▼                                                  │
│   ┌──────────────────────────────┐                              │
│   │          VLM Backbone         │                              │
│   └─────────────────┬──────────────┘                             │
│                     ▼                                           │
│   ┌────────────────────┬─────────────────────┐                  │
│   │  Latent Thought     │                      │                 │
│   │  (subtask planner)  │                      │                 │
│   └─────────┬──────────┘                      │                 │
│             ▼                                 ▼                 │
│      ┌────────────┐                     ┌────────────┐           │
│      │ FAST Token  │                     │ Flow Match  │           │
│      │ (Pretrain)  │                     │  (Inference)│           │
│      └────────────┘                     └────────────┘           │
│                │                                 │              │
│                └──────────────┬──────────────────┘              │
│                               ▼                                 │
│                      🦾 连续动作输出 (50Hz)                       │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>7. π0.6 + Recap Flow</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                π0.6 / π*0.6 架构与训练流程                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                     ┌──────────────────┐                        │
│                     │  5B VLM Backbone │                        │
│                     └────────┬─────────┘                        │
│                              ▼                                  │
│                     ┌──────────────────┐                        │
│                     │  Action Expert   │                        │
│                     └────────┬─────────┘                        │
│                              ▼                                  │
│                         🦾 精细动作                               │
│                                                                 │
│   Recap Training Pipeline:                                         │
│   Phase1: Collect successes/failures                              │
│              │                                                   │
│           Analyze trajectories                                    │
│              │                                                   │
│   Phase2: Recap Offline RL (reward positives, penalize negatives)│
│              │                                                   │
│           Update policy                                           │
│              │                                                   │
│   Phase3: Self-improvement → surpass human demonstrations         │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>8. VLA Architecture Evolution</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    VLA 架构演进路线图                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   RT-1 (2022)              RT-2 (2023)              π0 (2024)   │
│   ┌─────────┐              ┌─────────┐              ┌─────────┐ │
│   │EffNet   │              │ PaLI-X  │              │ Gemma   │ │
│   │   +     │   ────▶      │  55B    │   ────▶      │  +Flow  │ │
│   │Tokenize │              │ VLM+Act │              │ Matching│ │
│   └─────────┘              └─────────┘              └─────────┘ │
│     小模型                   大VLM                    高效推理   │
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                    通用 VLA 架构模板                     │   │
│   │                                                         │   │
│   │    📷 Image          📝 Language                        │   │
│   │       │                  │                              │   │
│   │       ▼                  ▼                              │   │
│   │  ┌─────────┐       ┌───────────┐                        │   │
│   │  │ Vision  │       │ Language  │                        │   │
│   │  │ Encoder │       │ Encoder   │                        │   │
│   │  └────┬────┘       └─────┬─────┘                        │   │
│   │       │                  │                              │   │
│   │       └────────┬─────────┘                              │   │
│   │                │ Fusion                                 │   │
│   │                ▼                                        │   │
│   │       ┌─────────────────┐                               │   │
│   │       │  Transformer    │                               │   │
│   │       │    Backbone     │                               │   │
│   │       └────────┬────────┘                               │   │
│   │                │                                        │   │
│   │                ▼                                        │   │
│   │       ┌─────────────────┐                               │   │
│   │       │   Action Head   │                               │   │
│   │       │ Token/Diff/Flow │                               │   │
│   │       └────────┬────────┘                               │   │
│   │                │                                        │   │
│   │                ▼                                        │   │
│   │          🦾 Robot Action                                │   │
│   └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>9. Flash Attention vs Standard</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│              Flash Attention vs 标准 Attention                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   标准 Attention (内存瓶颈)           Flash Attention (分块)    │
│                                                                 │
│   ┌───────────────────┐               ┌───────────────────┐     │
│   │      Q · K^T      │               │   Q₁·K₁  Q₁·K₂   │     │
│   │   ┌─────────────┐ │               │  ┌─────┐┌─────┐  │     │
│   │   │             │ │   ────▶       │  │ 块1 ││ 块2 │  │     │
│   │   │   N × N     │ │   分块        │  └─────┘└─────┘  │     │
│   │   │  (巨大!)    │ │               │  ┌─────┐┌─────┐  │     │
│   │   │             │ │               │  │ 块3 ││ 块4 │  │     │
│   │   └─────────────┘ │               │  └─────┘└─────┘  │     │
│   └───────────────────┘               └───────────────────┘     │
│   内存: O(N²) ❌                       内存: O(N) ✅             │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                        内存层级优化                              │
│                                                                 │
│   ┌────────────────┐     ┌────────────────┐                     │
│   │      HBM       │     │     SRAM       │                     │
│   │   (显存)       │     │   (L2 Cache)   │                     │
│   │   24GB+        │ ◀─▶ │    ~20MB       │                     │
│   │   慢 1TB/s     │     │   快 19TB/s    │                     │
│   └────────────────┘     └────────────────┘                     │
│          │                      │                               │
│          │   标准: 多次读写      │   Flash: 一次加载             │
│          │   Q,K ──▶ S ──▶ P    │   全部在 SRAM 计算            │
│          │   每步写回 HBM       │   只写最终结果                 │
│   结果: 2-4x 加速, 显存 O(N²) → O(N)                            │
└─────────────────────────────────────────────────────────────────┘
</code></pre>

<h4>10. WALL-OSS Uni-CoT</h4>
<pre><code class="">┌─────────────────────────────────────────────────────────────────┐
│                    WALL-OSS 架构 (Uni-CoT)                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   输入: 📷 图像 + 📝 指令                                          │
│              │                                                  │
│              ▼                                                  │
│   ┌─────────────────────────────────────────┐                  │
│   │       Qwen2.5 VLMoE Backbone            │                  │
│   └─────────────────┬───────────────────────┘                  │
│                     │                                          │
│                     ▼                                          │
│   ┌─────────────────────────────────────────┐                  │
│   │           Uni-CoT 推理链                 │                  │
│   │  ┌─────────────────────────────────┐    │                  │
│   │  │ 1. 高层推理: 理解任务               │    │                  │
│   │  │ 2. 子任务分解: 找海绵→抓取→擦拭     │    │                  │
│   │  │ 3. 细粒度动作: (x,y,z) 轨迹         │    │                  │
│   └─────────────────┬───────────────────────┘                  │
│                     │                                          │
│         ┌───────────┴───────────┐                              │
│         ▼                       ▼                              │
│   ┌───────────┐          ┌───────────┐                         │
│   │ 离散头    │          │ 连续头    │                         │
│   │(FAST Token)│          │(Flow Match)│                        │
│   └─────┬─────┘          └─────┬─────┘                         │
│         │                      │                               │
│         └──────────┬───────────┘                               │
│                    ▼                                           │
│              🦾 双头动作输出                                     │
└─────────────────────────────────────────────────────────────────┘
</code></pre>


<hr>
<p style="text-align: center; color: #666; font-size: 12px;">
    VLA 面试手册 | <a href="https://github.com/sou350121/vla-interview-handbook">GitHub</a>
</p>
</body>
</html>
