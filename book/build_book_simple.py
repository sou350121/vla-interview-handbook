#!/usr/bin/env python3
"""
VLA Interview Handbook - Simple Book Builder
ä½¿ç”¨çº¯ Python ç”Ÿæˆåˆå¹¶çš„ Markdownï¼ˆæ— éœ€ pandocï¼‰

PDF è½¬æ¢å¯ä»¥ä½¿ç”¨:
1. åœ¨çº¿å·¥å…·: https://md2pdf.netlify.app/
2. VS Code æ’ä»¶: Markdown PDF
3. Typora å¯¼å‡º
4. grip + æµè§ˆå™¨æ‰“å°
"""

import os
from pathlib import Path
from datetime import datetime

# ç« èŠ‚é¡ºåºå®šä¹‰
CHAPTERS = [
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€æ¶æ„
    ("part1", "# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€æ¶æ„"),
    ("ç¬¬1ç«  Transformer vs CNN", "transformer_vs_cnn.md"),
    ("ç¬¬2ç«  Flash Attention ä¸æ¨ç†ä¼˜åŒ–", "flash_attention.md"),
    ("ç¬¬3ç«  å¤šæ¨¡æ€æ¨¡å‹åŸºç¡€", "multimodal_models.md"),
    ("ç¬¬4ç«  VLA æ¶æ„æ€»è§ˆ", "vla_arch.md"),
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šç­–ç•¥ç”Ÿæˆä¸åŠ¨ä½œè¡¨ç¤º
    ("part2", "# ç¬¬äºŒéƒ¨åˆ†ï¼šç­–ç•¥ç”Ÿæˆä¸åŠ¨ä½œè¡¨ç¤º"),
    ("ç¬¬5ç«  åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•", "action_representations.md"),
    ("ç¬¬6ç«  Diffusion Policy", "diffusion_policy.md"),
    ("ç¬¬7ç«  Action Chunking Transformer (ACT)", "act.md"),
    ("ç¬¬8ç«  Flow Matching", "pi0_flow_matching.md"),
    ("ç¬¬9ç«  FAST åŠ¨ä½œåºåˆ—ç¼–ç ", "fast.md"),
    
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šè®­ç»ƒæŠ€æœ¯ä¸ä¼˜åŒ–
    ("part3", "# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè®­ç»ƒæŠ€æœ¯ä¸ä¼˜åŒ–"),
    ("ç¬¬10ç«  å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT/LoRA)", "peft_lora.md"),
    ("ç¬¬11ç«  å¼ºåŒ–å­¦ä¹ åŸºç¡€ä¸ RLHF", "reinforcement_learning.md"),
    ("ç¬¬12ç«  çŸ¥è¯†è’¸é¦", "knowledge_distillation.md"),
    ("ç¬¬13ç«  è‡ªç›‘ç£å­¦ä¹ ", "self_supervised_learning.md"),
    ("ç¬¬14ç«  è¿ç§»å­¦ä¹ ä¸ Co-training", "transfer_learning.md"),
    ("ç¬¬14ç« é™„ Co-training è¯¦è§£", "co_training.md"),
    ("ç¬¬15ç«  é‡åŒ–æŠ€æœ¯", "quantization_theory.md"),
    
    # ç¬¬å››éƒ¨åˆ†ï¼šæ„ŸçŸ¥ä¸ç©ºé—´æ™ºèƒ½
    ("part4", "# ç¬¬å››éƒ¨åˆ†ï¼šæ„ŸçŸ¥ä¸ç©ºé—´æ™ºèƒ½"),
    ("ç¬¬16ç«  ç©ºé—´æ•°å­¦åŸºç¡€", "spatial_math.md"),
    ("ç¬¬17ç«  æœºå™¨äººæ§åˆ¶æ–¹æ³•", "robot_control.md"),
    ("ç¬¬18ç«  æ„ŸçŸ¥æŠ€æœ¯", "perception_techniques.md"),
    ("ç¬¬19ç«  ç‚¹äº‘ä¸ SLAM", "pointcloud_slam.md"),
    ("ç¬¬20ç«  çŠ¶æ€ä¼°è®¡", "state_estimation.md"),
    
    # ç¬¬äº”éƒ¨åˆ†ï¼šæŠ“å–ä¸è¿åŠ¨è§„åˆ’
    ("part5", "# ç¬¬äº”éƒ¨åˆ†ï¼šæŠ“å–ä¸è¿åŠ¨è§„åˆ’"),
    ("ç¬¬21ç«  æŠ“å–ç®—æ³•", "grasp_algorithms.md"),
    ("ç¬¬22ç«  è¿åŠ¨è§„åˆ’", "motion_planning.md"),
    ("ç¬¬23ç«  è§¦è§‰ VLA", "tactile_vla.md"),
    
    # ç¬¬å…­éƒ¨åˆ†ï¼šå‰æ²¿æ¨¡å‹è§£æ
    ("part6", "# ç¬¬å…­éƒ¨åˆ†ï¼šå‰æ²¿æ¨¡å‹è§£æ"),
    ("ç¬¬24ç«  RDT (Robotics Diffusion Transformer)", "rdt.md"),
    ("ç¬¬25ç«  Ï€0.5 è§£æ", "pi0_5_dissection.md"),
    ("ç¬¬26ç«  Ï€0.6 è§£æ", "pi0_6_dissection.md"),
    ("ç¬¬27ç«  Galaxea G0", "galaxea_g0.md"),
    ("ç¬¬28ç«  WALL-OSS", "wall_oss.md"),
    
    # ç¬¬ä¸ƒéƒ¨åˆ†ï¼šè¯„ä¼°ä¸æ¨ç†
    ("part7", "# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šè¯„ä¼°ä¸æ¨ç†"),
    ("ç¬¬29ç«  Chain-of-Thought æ¨ç†", "chain_of_thought.md"),
    ("ç¬¬30ç«  è¯„ä¼°æ–¹æ³•è®º", "evaluation.md"),
    ("ç¬¬31ç«  çŸ¥è¯†éš”ç¦»", "knowledge_insulation.md"),
    
    # é™„å½•
    ("appendix", "# é™„å½•"),
    ("é™„å½•A æ•°æ®æ ¼å¼ä¸å¤„ç†", "data.md"),
    ("é™„å½•B æ–‡çŒ®ç»¼è¿°", "literature_review.md"),
    ("é™„å½•C ASCII å›¾è¡¨é€ŸæŸ¥", "ascii_cheatsheet.md"),
]


def generate_toc():
    """ç”Ÿæˆç›®å½•"""
    toc = ["# ç›®å½•\n"]
    chapter_num = 0
    
    for title, filename in CHAPTERS:
        if title.startswith("part") or title.startswith("appendix"):
            toc.append(f"\n{filename}\n")
        elif title.startswith("é™„å½•"):
            toc.append(f"- {title}")
        else:
            chapter_num += 1
            toc.append(f"- {title}")
    
    return "\n".join(toc) + "\n"


def clean_content(content: str) -> str:
    """æ¸…ç†å†…å®¹"""
    lines = []
    for line in content.split('\n'):
        # ç§»é™¤è¿”å›é“¾æ¥
        if '[â† Back to' in line or '[â† è¿”å›' in line:
            continue
        lines.append(line)
    return '\n'.join(lines)


def main():
    script_dir = Path(__file__).resolve().parent
    theory_dir = script_dir.parent / "theory"
    output_dir = script_dir / "output"
    output_dir.mkdir(exist_ok=True)
    
    # ä¹¦ç±å¤´éƒ¨
    book = f"""# VLA é¢è¯•æ‰‹å†Œï¼šä»ç†è®ºåˆ°å®è·µ

> **Vision-Language-Action å®Œå…¨æŒ‡å—**
>
> ç”Ÿæˆæ—¥æœŸ: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")}
>
> åœ¨çº¿ç‰ˆæœ¬: https://github.com/sou350121/vla-interview-handbook

---

{generate_toc()}

---

"""
    
    # å¤„ç†æ¯ä¸ªç« èŠ‚
    for title, filename in CHAPTERS:
        if title.startswith("part") or title.startswith("appendix"):
            book += f"\n---\n\n{filename}\n\n"
            continue
            
        filepath = theory_dir / filename
        if not filepath.exists():
            print(f"âš ï¸  è·³è¿‡: {filename}")
            continue
        
        print(f"ğŸ“– {title}")
        content = filepath.read_text(encoding='utf-8')
        content = clean_content(content)
        
        # æ·»åŠ ç« èŠ‚åˆ†éš”
        book += f"\n---\n\n## {title}\n\n"
        
        # è°ƒæ•´æ ‡é¢˜çº§åˆ«
        for line in content.split('\n'):
            if line.startswith('# '):
                # è·³è¿‡åŸæ ‡é¢˜ï¼Œå·²ç»ç”¨ç« èŠ‚æ ‡é¢˜æ›¿æ¢
                continue
            elif line.startswith('## '):
                book += '###' + line[2:] + '\n'
            elif line.startswith('### '):
                book += '####' + line[3:] + '\n'
            else:
                book += line + '\n'
    
    # å†™å…¥æ–‡ä»¶
    output_path = output_dir / "VLA_Interview_Handbook_Full.md"
    output_path.write_text(book, encoding='utf-8')
    
    print(f"\nâœ… ç”Ÿæˆå®Œæˆ: {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.1f} KB")
    print(f"   æ€»å­—æ•°çº¦: {len(book)} å­—ç¬¦")
    print("\nğŸ“„ è½¬æ¢ä¸º PDF çš„æ–¹æ³•:")
    print("   1. åœ¨çº¿: https://md2pdf.netlify.app/")
    print("   2. VS Code: å®‰è£… 'Markdown PDF' æ’ä»¶ï¼Œå³é”®å¯¼å‡º")
    print("   3. Typora: æ–‡ä»¶ â†’ å¯¼å‡º â†’ PDF")
    print("   4. grip: pip install grip && grip --export output.html")


if __name__ == "__main__":
    main()


